{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto EAF - Notebook MonolÃ­tico\n",
    "\n",
    "## Electric Arc Furnace - PredicciÃ³n de Temperatura y ComposiciÃ³n QuÃ­mica\n",
    "\n",
    "Este notebook contiene la migraciÃ³n completa del proyecto modular EAF a un formato monolÃ­tico.\n",
    "\n",
    "---\n",
    "\n",
    "## PARTE 1: ConfiguraciÃ³n del Entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importaciones y ConfiguraciÃ³n de Logging\n",
    "\n",
    "Importamos todas las librerÃ­as necesarias para:\n",
    "- **ManipulaciÃ³n de datos**: pandas, numpy\n",
    "- **Sistema de archivos**: os, shutil, pathlib, json\n",
    "- **Logging y warnings**: para trazabilidad y control de mensajes\n",
    "- **VisualizaciÃ³n**: matplotlib, seaborn\n",
    "- **Machine Learning**: sklearn (mÃ©tricas, modelos, split), xgboost\n",
    "- **Persistencia**: joblib para guardar/cargar modelos\n",
    "- **Datos**: kagglehub para descarga de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTACIONES PRINCIPALES\n",
    "# =============================================================================\n",
    "\n",
    "# ManipulaciÃ³n de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sistema de archivos y utilidades\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Logging y warnings\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# VisualizaciÃ³n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Persistencia de modelos\n",
    "import joblib\n",
    "\n",
    "# Descarga de datos\n",
    "import kagglehub\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURACIÃ“N DE LOGGING\n",
    "# =============================================================================\n",
    "\n",
    "# Configurar logging para que imprima en la salida del notebook con formato de tiempo\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    force=True  # Forzar reconfiguraciÃ³n si ya existe\n",
    ")\n",
    "\n",
    "# Crear logger principal del notebook\n",
    "logger = logging.getLogger('EAF_Notebook')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Suprimir warnings innecesarios para limpieza de output\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# ConfiguraciÃ³n de visualizaciÃ³n\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# ConfiguraciÃ³n de pandas para mejor visualizaciÃ³n\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "logger.info(\"Importaciones completadas exitosamente\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ConfiguraciÃ³n de Directorios\n",
    "\n",
    "Definimos la estructura de carpetas del proyecto y las creamos automÃ¡ticamente si no existen:\n",
    "- `data/raw`: Datos crudos descargados de Kaggle\n",
    "- `data/processed`: Datos procesados listos para entrenamiento\n",
    "- `models`: Modelos de temperatura entrenados\n",
    "- `models/chemical_results`: Modelos quÃ­micos y sus mÃ©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURACIÃ“N DE DIRECTORIOS\n",
    "# =============================================================================\n",
    "\n",
    "# Definir raÃ­z del proyecto usando pathlib\n",
    "# Usamos Path.cwd() para notebooks, asumiendo que se ejecuta desde la raÃ­z del proyecto\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Si el notebook estÃ¡ en una subcarpeta, ajustar:\n",
    "# PROJECT_ROOT = Path.cwd().parent  # Descomentar si es necesario\n",
    "\n",
    "# Definir estructura de directorios\n",
    "DIRECTORIES = {\n",
    "    'DATA_RAW': PROJECT_ROOT / 'data' / 'raw',\n",
    "    'DATA_PROCESSED': PROJECT_ROOT / 'data' / 'processed',\n",
    "    'MODELS': PROJECT_ROOT / 'models',\n",
    "    'CHEMICAL_RESULTS': PROJECT_ROOT / 'models' / 'chemical_results'\n",
    "}\n",
    "\n",
    "# Crear directorios si no existen\n",
    "for dir_name, dir_path in DIRECTORIES.items():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Directorio verificado/creado: {dir_name} -> {dir_path}\")\n",
    "\n",
    "# Asignar a variables individuales para fÃ¡cil acceso\n",
    "DATA_RAW = DIRECTORIES['DATA_RAW']\n",
    "DATA_PROCESSED = DIRECTORIES['DATA_PROCESSED']\n",
    "MODELS_DIR = DIRECTORIES['MODELS']\n",
    "CHEMICAL_RESULTS_DIR = DIRECTORIES['CHEMICAL_RESULTS']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ESTRUCTURA DE DIRECTORIOS DEL PROYECTO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"PROJECT_ROOT:      {PROJECT_ROOT}\")\n",
    "print(f\"DATA_RAW:          {DATA_RAW}\")\n",
    "print(f\"DATA_PROCESSED:    {DATA_PROCESSED}\")\n",
    "print(f\"MODELS_DIR:        {MODELS_DIR}\")\n",
    "print(f\"CHEMICAL_RESULTS:  {CHEMICAL_RESULTS_DIR}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Constantes del Negocio (CRÃTICO)\n",
    "\n",
    "Estas constantes definen la lÃ³gica de negocio del proyecto EAF:\n",
    "- **INPUT_FEATURES**: Variables de entrada del proceso (gases, carbono, materiales aÃ±adidos)\n",
    "- **CHEMICAL_TARGETS**: Targets quÃ­micos a predecir (composiciÃ³n final del acero)\n",
    "- **CHEMICAL_COLUMNS**: Columnas que requieren conversiÃ³n de coma a punto decimal\n",
    "- **CHEMICAL_SPECS**: Rangos de especificaciÃ³n para control de calidad\n",
    "- **DEFAULT_HYPERPARAMS**: HiperparÃ¡metros por defecto para los modelos\n",
    "- **MODEL_DISPLAY_NAMES**: Mapeo de nombres de modelos para UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONSTANTES DEL NEGOCIO - PROYECTO EAF\n",
    "# =============================================================================\n",
    "# Migradas exactamente desde src/config.py y dashboard/config.py\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FEATURES DE INPUT\n",
    "# Variables de entrada disponibles en el dataset para predicciÃ³n\n",
    "# -----------------------------------------------------------------------------\n",
    "INPUT_FEATURES = [\n",
    "    'total_o2_lance',           # OxÃ­geno total inyectado por lanza\n",
    "    'total_gas_lance',          # Gas total inyectado por lanza\n",
    "    'total_injected_carbon',    # Carbono total inyectado\n",
    "    'valc',                     # Valor inicial de Carbono\n",
    "    'valsi',                    # Valor inicial de Silicio\n",
    "    'valmn',                    # Valor inicial de Manganeso\n",
    "    'valp',                     # Valor inicial de FÃ³sforo\n",
    "    'vals',                     # Valor inicial de Azufre\n",
    "    'valcu',                    # Valor inicial de Cobre\n",
    "    'valcr',                    # Valor inicial de Cromo\n",
    "    'valmo',                    # Valor inicial de Molibdeno\n",
    "    'valni',                    # Valor inicial de NÃ­quel\n",
    "    'added_mat_140107',         # Material aÃ±adido: Cal viva\n",
    "    'added_mat_202007',         # Material aÃ±adido: Mineral de hierro\n",
    "    'added_mat_202008',         # Material aÃ±adido: Dolomita\n",
    "    'added_mat_202039',         # Material aÃ±adido: Coque\n",
    "    'added_mat_202063',         # Material aÃ±adido: Fluorita\n",
    "    'added_mat_203068',         # Material aÃ±adido: FeSi\n",
    "    'added_mat_203085',         # Material aÃ±adido: FeMn\n",
    "    'added_mat_205069',         # Material aÃ±adido: Grafito\n",
    "    'added_mat_360258',         # Material aÃ±adido: Chatarra especial\n",
    "    'added_mat_705043'          # Material aÃ±adido: Otros aditivos\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TARGETS QUÃMICOS\n",
    "# Valores FINALES de composiciÃ³n quÃ­mica a predecir\n",
    "# -----------------------------------------------------------------------------\n",
    "CHEMICAL_TARGETS = [\n",
    "    'target_valc',              # Carbono final\n",
    "    'target_valmn',             # Manganeso final\n",
    "    'target_valsi',             # Silicio final\n",
    "    'target_valp',              # FÃ³sforo final\n",
    "    'target_vals',              # Azufre final\n",
    "    'target_valcu',             # Cobre final\n",
    "    'target_valcr',             # Cromo final\n",
    "    'target_valmo',             # Molibdeno final\n",
    "    'target_valni'              # NÃ­quel final\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TARGETS DE TEMPERATURA\n",
    "# Columnas que deben excluirse de features cuando se predice temperatura\n",
    "# -----------------------------------------------------------------------------\n",
    "TEMPERATURE_TARGETS = ['target_temperature']\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# COLUMNAS A EXCLUIR COMO FEATURES\n",
    "# IDs, targets y otras columnas que no deben usarse como variables predictoras\n",
    "# -----------------------------------------------------------------------------\n",
    "EXCLUDE_FROM_FEATURES = [\n",
    "    'heatid',                   # Identificador Ãºnico de colada\n",
    "    'target_temperature',       # Target de temperatura\n",
    "    'target_valc',              # Target quÃ­mico: Carbono\n",
    "    'target_valmn',             # Target quÃ­mico: Manganeso\n",
    "    'target_valsi',             # Target quÃ­mico: Silicio\n",
    "    'target_valp',              # Target quÃ­mico: FÃ³sforo\n",
    "    'target_vals',              # Target quÃ­mico: Azufre\n",
    "    'target_valcu',             # Target quÃ­mico: Cobre\n",
    "    'target_valcr',             # Target quÃ­mico: Cromo\n",
    "    'target_valmo',             # Target quÃ­mico: Molibdeno\n",
    "    'target_valni'              # Target quÃ­mico: NÃ­quel\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# COLUMNAS QUÃMICAS\n",
    "# Columnas que usan coma como separador decimal en el CSV original\n",
    "# Requieren conversiÃ³n a punto decimal durante la carga\n",
    "# -----------------------------------------------------------------------------\n",
    "CHEMICAL_COLUMNS = [\n",
    "    'valc',                     # Carbono inicial\n",
    "    'valsi',                    # Silicio inicial\n",
    "    'valmn',                    # Manganeso inicial\n",
    "    'valp',                     # FÃ³sforo inicial\n",
    "    'vals',                     # Azufre inicial\n",
    "    'valcu',                    # Cobre inicial\n",
    "    'valcr',                    # Cromo inicial\n",
    "    'valmo',                    # Molibdeno inicial\n",
    "    'valni'                     # NÃ­quel inicial\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ESPECIFICACIONES QUÃMICAS\n",
    "# Rangos de especificaciÃ³n (min, max) para valores FINALES - Control de calidad\n",
    "# -----------------------------------------------------------------------------\n",
    "CHEMICAL_SPECS = {\n",
    "    'target_valc':  (0.05, 0.50),    # Carbono: 0.05% - 0.50%\n",
    "    'target_valmn': (0.30, 1.50),    # Manganeso: 0.30% - 1.50%\n",
    "    'target_valsi': (0.10, 0.60),    # Silicio: 0.10% - 0.60%\n",
    "    'target_valp':  (0.001, 0.025),  # FÃ³sforo: 0.001% - 0.025%\n",
    "    'target_vals':  (0.001, 0.025),  # Azufre: 0.001% - 0.025%\n",
    "    'target_valcu': (0.001, 0.030),  # Cobre: 0.001% - 0.030%\n",
    "    'target_valcr': (0.001, 0.030),  # Cromo: 0.001% - 0.030%\n",
    "    'target_valmo': (0.001, 0.010),  # Molibdeno: 0.001% - 0.010%\n",
    "    'target_valni': (0.001, 0.030)   # NÃ­quel: 0.001% - 0.030%\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# RANGOS DE TEMPERATURA\n",
    "# Rangos Ã³ptimos para indicador de calidad tÃ©rmica\n",
    "# -----------------------------------------------------------------------------\n",
    "TEMPERATURE_RANGES = {\n",
    "    'optimal_min': 1580,            # Temperatura mÃ­nima Ã³ptima (Â°C)\n",
    "    'optimal_max': 1650             # Temperatura mÃ¡xima Ã³ptima (Â°C)\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MODELOS DISPONIBLES\n",
    "# Lista de modelos soportados para entrenamiento\n",
    "# -----------------------------------------------------------------------------\n",
    "AVAILABLE_MODELS = [\n",
    "    'xgboost',                      # XGBoost Regressor\n",
    "    'random_forest',                # Random Forest Regressor\n",
    "    'linear'                        # Linear Regression\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# NOMBRES DE MODELOS PARA DISPLAY\n",
    "# Mapeo de identificadores internos a nombres para UI/reportes\n",
    "# -----------------------------------------------------------------------------\n",
    "MODEL_DISPLAY_NAMES = {\n",
    "    'xgboost': 'XGBoost Regressor',\n",
    "    'random_forest': 'Random Forest Regressor',\n",
    "    'linear': 'Linear Regression'\n",
    "}\n",
    "\n",
    "# Mapeo inverso para UI -> identificador interno\n",
    "UI_MODEL_NAMES = {\n",
    "    'Linear Regression': 'linear',\n",
    "    'Random Forest Regressor': 'random_forest',\n",
    "    'XGBoost Regressor': 'xgboost'\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# HIPERPARÃMETROS POR DEFECTO\n",
    "# ConfiguraciÃ³n inicial para entrenamiento de modelos\n",
    "# -----------------------------------------------------------------------------\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    'n_estimators': 100,            # NÃºmero de Ã¡rboles (RF/XGBoost)\n",
    "    'max_depth': 6,                 # Profundidad mÃ¡xima de Ã¡rboles\n",
    "    'learning_rate': 0.1,           # Tasa de aprendizaje (XGBoost)\n",
    "    'test_size': 0.2,               # ProporciÃ³n de datos para test\n",
    "    'random_state': 42              # Semilla para reproducibilidad\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DATASETS DISPONIBLES PARA EDA\n",
    "# Archivos de datos procesados por tipo de anÃ¡lisis\n",
    "# -----------------------------------------------------------------------------\n",
    "EDA_DATASETS = {\n",
    "    'Temperatura': 'dataset_final_temp.csv',\n",
    "    'Quimica': 'dataset_final_chemical.csv'\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIGURACIÃ“N DE SERVICIOS (opcional)\n",
    "# URL del servicio BentoML para despliegue\n",
    "# -----------------------------------------------------------------------------\n",
    "BENTOML_URL = os.getenv('BENTOML_URL', 'http://localhost:3000')\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFICACIÃ“N DE CONSTANTES\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CONSTANTES DEL NEGOCIO CARGADAS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"INPUT_FEATURES:     {len(INPUT_FEATURES)} variables\")\n",
    "print(f\"CHEMICAL_TARGETS:   {len(CHEMICAL_TARGETS)} targets\")\n",
    "print(f\"CHEMICAL_COLUMNS:   {len(CHEMICAL_COLUMNS)} columnas\")\n",
    "print(f\"CHEMICAL_SPECS:     {len(CHEMICAL_SPECS)} especificaciones\")\n",
    "print(f\"AVAILABLE_MODELS:   {len(AVAILABLE_MODELS)} modelos\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\n--- INPUT_FEATURES (completo) ---\")\n",
    "for i, feat in enumerate(INPUT_FEATURES, 1):\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "print(\"\\n--- CHEMICAL_TARGETS (completo) ---\")\n",
    "for i, target in enumerate(CHEMICAL_TARGETS, 1):\n",
    "    print(f\"  {i}. {target}\")\n",
    "\n",
    "print(\"\\n--- CHEMICAL_SPECS (rangos) ---\")\n",
    "for target, (min_val, max_val) in CHEMICAL_SPECS.items():\n",
    "    print(f\"  {target}: [{min_val:.3f}, {max_val:.3f}]\")\n",
    "\n",
    "logger.info(\"Constantes del negocio cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fin de PARTE 1\n",
    "\n",
    "El entorno estÃ¡ configurado y listo para las siguientes fases:\n",
    "- **PARTE 2**: Descarga y carga de datos\n",
    "- **PARTE 3**: Preprocesamiento y feature engineering\n",
    "- **PARTE 4**: Entrenamiento de modelos\n",
    "- **PARTE 5**: EvaluaciÃ³n y visualizaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## PARTE 2: Ingesta de Datos\n\nEn esta secciÃ³n implementamos la descarga automÃ¡tica del dataset desde Kaggle.\n\nEl dataset **\"Industrial Data from the Arc Furnace\"** contiene 11 archivos CSV con informaciÃ³n del proceso de fundiciÃ³n:\n- Datos del transformador y temperatura\n- Mediciones quÃ­micas iniciales y finales\n- Materiales cargados e inyectados\n- Datos del horno de cuchara (Ladle Furnace)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 2.1 ConfiguraciÃ³n de Descarga y Archivos Esperados\n\nDefinimos los archivos que componen el dataset completo y la referencia al dataset de Kaggle.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CONFIGURACIÃ“N DE DESCARGA DE DATOS\n# =============================================================================\n\n# Dataset de Kaggle (fuente oficial de los datos)\nKAGGLE_DATASET = \"yuriykatser/industrial-data-from-the-arc-furnace\"\n\n# -----------------------------------------------------------------------------\n# ARCHIVOS ESPERADOS DEL DATASET\n# Lista completa de los 11 archivos CSV que componen el dataset\n# -----------------------------------------------------------------------------\nARCHIVOS_ESPERADOS = [\n    \"eaf_transformer.csv\",              # Datos del transformador del horno\n    \"basket_charged.csv\",               # Cestas de chatarra cargadas\n    \"eaf_temp.csv\",                      # Mediciones de temperatura\n    \"eaf_final_chemical_measurements.csv\",  # ComposiciÃ³n quÃ­mica final\n    \"eaf_added_materials.csv\",           # Materiales aÃ±adidos al horno\n    \"inj_mat.csv\",                       # Materiales inyectados\n    \"eaf_gaslance_mat.csv\",              # Gases inyectados por lanza\n    \"lf_initial_chemical_measurements.csv\",  # QuÃ­mica inicial (horno cuchara)\n    \"ladle_tapping.csv\",                 # Datos de colada\n    \"lf_added_materials.csv\",            # Materiales aÃ±adidos en LF\n    \"ferro.csv\"                          # Ferroaleaciones utilizadas\n]\n\nprint(f\"Dataset de Kaggle: {KAGGLE_DATASET}\")\nprint(f\"Archivos esperados: {len(ARCHIVOS_ESPERADOS)}\")\nprint(\"\\n--- Lista de archivos ---\")\nfor i, archivo in enumerate(ARCHIVOS_ESPERADOS, 1):\n    print(f\"  {i:2d}. {archivo}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.2 FunciÃ³n de Descarga de Datos\n\nImplementamos `download_data()` con la siguiente lÃ³gica:\n1. Verifica si todos los archivos ya existen en `data/raw`\n2. Si existen y `force=False`, no descarga (evita trabajo innecesario)\n3. Si faltan archivos, descarga desde Kaggle usando `kagglehub`\n4. Copia los archivos al directorio del proyecto\n5. Reporta el estado de cada archivo con su tamaÃ±o",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIÃ“N DE DESCARGA DE DATOS\n# =============================================================================\n\ndef download_data(force: bool = False) -> Path:\n    \"\"\"\n    Descarga los datos de Kaggle y los copia al directorio data/raw/.\n    \n    Args:\n        force: Si es True, sobreescribe los archivos existentes aunque ya existan.\n               Si es False (default), solo descarga si faltan archivos.\n    \n    Returns:\n        Path al directorio data/raw/ con los datos descargados.\n    \n    Raises:\n        ConnectionError: Si hay problemas de conexiÃ³n con Kaggle.\n        PermissionError: Si no hay permisos para escribir en el directorio.\n        Exception: Para otros errores inesperados.\n    \n    Example:\n        >>> raw_path = download_data()  # Descarga solo si es necesario\n        >>> raw_path = download_data(force=True)  # Forzar re-descarga\n    \"\"\"\n    try:\n        # Asegurar que el directorio existe\n        DATA_RAW.mkdir(parents=True, exist_ok=True)\n        \n        # ---------------------------------------------------------------------\n        # PASO 1: Verificar archivos existentes\n        # ---------------------------------------------------------------------\n        archivos_existentes = [\n            f for f in ARCHIVOS_ESPERADOS \n            if (DATA_RAW / f).exists()\n        ]\n        archivos_faltantes = [\n            f for f in ARCHIVOS_ESPERADOS \n            if not (DATA_RAW / f).exists()\n        ]\n        \n        logger.info(f\"Archivos existentes: {len(archivos_existentes)}/{len(ARCHIVOS_ESPERADOS)}\")\n        \n        # Si todos existen y no se fuerza, terminar temprano\n        if len(archivos_existentes) == len(ARCHIVOS_ESPERADOS) and not force:\n            logger.info(f\"Todos los datos ya existen en {DATA_RAW}\")\n            logger.info(\"Usa force=True para volver a descargar.\")\n            print(f\"\\n{'='*60}\")\n            print(\"DATOS YA DISPONIBLES - No se requiere descarga\")\n            print(f\"{'='*60}\")\n            print(f\"Directorio: {DATA_RAW}\")\n            print(f\"Archivos: {len(archivos_existentes)}\")\n            return DATA_RAW\n        \n        # Mostrar archivos faltantes si los hay\n        if archivos_faltantes and not force:\n            logger.info(f\"Archivos faltantes ({len(archivos_faltantes)}):\")\n            for archivo in archivos_faltantes:\n                logger.info(f\"  - {archivo}\")\n        \n        # ---------------------------------------------------------------------\n        # PASO 2: Descargar desde Kaggle\n        # ---------------------------------------------------------------------\n        print(f\"\\n{'='*60}\")\n        print(\"INICIANDO DESCARGA DESDE KAGGLE\")\n        print(f\"{'='*60}\")\n        logger.info(f\"Descargando dataset: {KAGGLE_DATASET}\")\n        logger.info(\"Esto puede tardar unos minutos dependiendo de la conexiÃ³n...\")\n        \n        # Descargar usando kagglehub\n        kaggle_path = kagglehub.dataset_download(KAGGLE_DATASET)\n        kaggle_path = Path(kaggle_path)\n        \n        logger.info(f\"Dataset descargado en cachÃ©: {kaggle_path}\")\n        \n        # ---------------------------------------------------------------------\n        # PASO 3: Copiar archivos al directorio del proyecto\n        # ---------------------------------------------------------------------\n        logger.info(f\"Copiando archivos a: {DATA_RAW}\")\n        print(\"-\" * 60)\n        \n        archivos_copiados = 0\n        archivos_no_encontrados = []\n        total_size_mb = 0\n        \n        for archivo in ARCHIVOS_ESPERADOS:\n            origen = kaggle_path / archivo\n            destino = DATA_RAW / archivo\n            \n            if origen.exists():\n                # Copiar archivo preservando metadatos\n                shutil.copy2(origen, destino)\n                \n                # Calcular tamaÃ±o\n                size_mb = destino.stat().st_size / (1024 * 1024)\n                total_size_mb += size_mb\n                \n                logger.info(f\"  âœ“ {archivo} ({size_mb:.2f} MB)\")\n                archivos_copiados += 1\n            else:\n                archivos_no_encontrados.append(archivo)\n                logger.warning(f\"  âœ— {archivo} (NO ENCONTRADO en origen)\")\n        \n        # ---------------------------------------------------------------------\n        # PASO 4: Resumen final\n        # ---------------------------------------------------------------------\n        print(\"-\" * 60)\n        print(f\"\\n{'='*60}\")\n        print(\"RESUMEN DE DESCARGA\")\n        print(f\"{'='*60}\")\n        print(f\"Archivos copiados:    {archivos_copiados}/{len(ARCHIVOS_ESPERADOS)}\")\n        print(f\"TamaÃ±o total:         {total_size_mb:.2f} MB\")\n        print(f\"Destino:              {DATA_RAW}\")\n        \n        if archivos_no_encontrados:\n            logger.warning(f\"Archivos no encontrados: {archivos_no_encontrados}\")\n            print(f\"âš ï¸  Archivos faltantes: {len(archivos_no_encontrados)}\")\n        else:\n            print(\"âœ… Todos los archivos descargados correctamente\")\n        \n        print(f\"{'='*60}\")\n        \n        logger.info(\"Descarga completada exitosamente\")\n        return DATA_RAW\n        \n    except ConnectionError as e:\n        logger.error(f\"Error de conexiÃ³n con Kaggle: {e}\")\n        print(\"âŒ Error: No se pudo conectar con Kaggle.\")\n        print(\"   Verifica tu conexiÃ³n a internet y credenciales de Kaggle.\")\n        raise\n        \n    except PermissionError as e:\n        logger.error(f\"Error de permisos: {e}\")\n        print(f\"âŒ Error: No hay permisos para escribir en {DATA_RAW}\")\n        raise\n        \n    except Exception as e:\n        logger.error(f\"Error inesperado durante la descarga: {e}\")\n        print(f\"âŒ Error inesperado: {e}\")\n        raise\n\n\ndef verificar_datos() -> bool:\n    \"\"\"\n    Verifica que todos los archivos necesarios existan en data/raw.\n    \n    Returns:\n        True si todos los archivos existen, False en caso contrario.\n    \n    Example:\n        >>> if verificar_datos():\n        ...     print(\"Datos listos para procesar\")\n    \"\"\"\n    if not DATA_RAW.exists():\n        logger.warning(f\"El directorio {DATA_RAW} no existe.\")\n        print(\"âš ï¸  El directorio de datos no existe.\")\n        print(\"   Ejecuta download_data() para descargar los datos.\")\n        return False\n    \n    archivos_faltantes = []\n    archivos_ok = []\n    \n    for archivo in ARCHIVOS_ESPERADOS:\n        ruta = DATA_RAW / archivo\n        if ruta.exists():\n            size_mb = ruta.stat().st_size / (1024 * 1024)\n            archivos_ok.append((archivo, size_mb))\n        else:\n            archivos_faltantes.append(archivo)\n    \n    print(f\"\\n{'='*60}\")\n    print(\"VERIFICACIÃ“N DE DATOS\")\n    print(f\"{'='*60}\")\n    \n    if archivos_ok:\n        print(f\"\\nâœ… Archivos disponibles ({len(archivos_ok)}):\")\n        for archivo, size in archivos_ok:\n            print(f\"   - {archivo} ({size:.2f} MB)\")\n    \n    if archivos_faltantes:\n        print(f\"\\nâŒ Archivos faltantes ({len(archivos_faltantes)}):\")\n        for archivo in archivos_faltantes:\n            print(f\"   - {archivo}\")\n        logger.warning(f\"Faltan {len(archivos_faltantes)} archivos\")\n        return False\n    \n    total_size = sum(size for _, size in archivos_ok)\n    print(f\"\\nðŸ“Š Total: {len(archivos_ok)} archivos, {total_size:.2f} MB\")\n    print(f\"{'='*60}\")\n    \n    logger.info(f\"VerificaciÃ³n OK: {len(ARCHIVOS_ESPERADOS)} archivos disponibles\")\n    return True\n\n\nprint(\"âœ… Funciones de descarga definidas: download_data(), verificar_datos()\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.3 EjecuciÃ³n de Descarga\n\nEjecutamos la funciÃ³n de descarga. Si los datos ya existen, no se descargarÃ¡ nada.\nPara forzar una nueva descarga, usa `download_data(force=True)`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# EJECUCIÃ“N DE DESCARGA DE DATOS\n# =============================================================================\n\n# Descargar datos (solo si es necesario)\nraw_data_path = download_data(force=False)\n\n# Verificar que todos los archivos estÃ¡n disponibles\ndatos_ok = verificar_datos()\n\nif datos_ok:\n    print(\"\\nâœ… Datos listos para la siguiente fase (Preprocesamiento)\")\nelse:\n    print(\"\\nâš ï¸  Ejecuta download_data(force=True) si hay problemas\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Fin de PARTE 2\n\nLa ingesta de datos estÃ¡ completa:\n- **11 archivos CSV** descargados desde Kaggle\n- Datos almacenados en `data/raw/`\n- VerificaciÃ³n automÃ¡tica de integridad\n\nSiguiente fase:\n- **PARTE 3**: Preprocesamiento y Feature Engineering",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## PARTE 3: Feature Engineering y CreaciÃ³n del Dataset Maestro\n\nEsta es la parte mÃ¡s crÃ­tica del proyecto. Transformamos los **11 archivos CSV** de series temporales en **2 datasets tabulares** listos para Machine Learning.\n\n**Pipeline de transformaciÃ³n:**\n1. **Carga estandarizada**: Normalizar columnas y convertir formatos europeos\n2. **AgregaciÃ³n temporal**: Convertir series de tiempo a valores por colada (heatid)\n3. **Pivotado de materiales**: Crear columnas por tipo de material aÃ±adido\n4. **ExtracciÃ³n de targets**: Obtener temperatura y composiciÃ³n quÃ­mica final\n5. **FusiÃ³n**: Combinar todas las fuentes en un dataset maestro\n6. **Limpieza**: Rellenar nulos tÃ©cnicos y eliminar columnas innecesarias",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1 FunciÃ³n de Carga Estandarizada\n\nEsta funciÃ³n es fundamental: carga CSVs y convierte automÃ¡ticamente formatos numÃ©ricos europeos (coma decimal) a formato estÃ¡ndar (punto decimal).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIÃ“N DE CARGA ESTANDARIZADA\n# =============================================================================\n\ndef load_standardized(filepath: Path) -> pd.DataFrame:\n    \"\"\"\n    Carga un CSV y estandariza los nombres de columnas.\n    \n    IMPORTANTE: Detecta y convierte automÃ¡ticamente formatos numÃ©ricos europeos\n    donde se usa coma como separador decimal (ej: \"12,5\" -> 12.5).\n    \n    Args:\n        filepath: Ruta al archivo CSV (Path o string)\n    \n    Returns:\n        DataFrame con:\n        - Columnas en minÃºsculas y sin espacios\n        - Valores numÃ©ricos con formato decimal estÃ¡ndar (punto)\n    \n    Example:\n        >>> df = load_standardized(DATA_RAW / \"eaf_temp.csv\")\n        >>> df.columns  # ['heatid', 'temp', 'datetime', ...]\n    \"\"\"\n    # Cargar CSV\n    df = pd.read_csv(filepath, low_memory=False)\n    \n    # Estandarizar nombres de columnas: minÃºsculas y sin espacios\n    df.columns = df.columns.str.lower().str.strip()\n    \n    # ---------------------------------------------------------------------\n    # CONVERSIÃ“N DE COMAS DECIMALES (formato europeo -> americano)\n    # Detecta columnas tipo object que contienen patrones como \"12,5\" o \"-3,14\"\n    # ---------------------------------------------------------------------\n    for col in df.select_dtypes(include=['object']).columns:\n        # PatrÃ³n: nÃºmero opcional negativo, dÃ­gitos, coma, dÃ­gitos\n        # Ejemplos vÃ¡lidos: \"12,5\", \"-3,14\", \"0,001\"\n        if df[col].astype(str).str.match(r'^-?\\d+,\\d+$').any():\n            df[col] = df[col].astype(str).str.replace(',', '.', regex=False)\n            logger.debug(f\"  Convertida coma decimal en columna: {col}\")\n    \n    return df\n\n\n# Test de la funciÃ³n\nprint(\"âœ… FunciÃ³n load_standardized() definida\")\nprint(\"\\nCaracterÃ­sticas:\")\nprint(\"  - Convierte nombres de columnas a minÃºsculas\")\nprint(\"  - Elimina espacios en nombres de columnas\")\nprint(\"  - Detecta y convierte comas decimales europeas a puntos\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.2 Funciones de AgregaciÃ³n de Series Temporales\n\nLos datos originales son series temporales (mÃºltiples registros por colada). Estas funciones agregan los datos para obtener **un valor por colada (heatid)**.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIONES DE AGREGACIÃ“N DE SERIES TEMPORALES\n# =============================================================================\n\ndef aggregate_gas_data(df_gas: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Agrega los datos de gas lance por colada.\n    \n    Obtiene el ÃšLTIMO valor registrado temporalmente (por revtime) de cada colada.\n    Esto representa el estado final de O2 y gas inyectados.\n    \n    Args:\n        df_gas: DataFrame con datos de eaf_gaslance_mat.csv\n    \n    Returns:\n        DataFrame indexado por heatid con columnas:\n        - total_o2_lance: Ãšltimo valor de O2 inyectado\n        - total_gas_lance: Ãšltimo valor de gas inyectado\n    \"\"\"\n    df = df_gas.copy()\n    \n    # Convertir columnas a numÃ©rico\n    cols_gas = ['o2_amount', 'gas_amount']\n    for col in cols_gas:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    \n    # Convertir tiempo (formato original: \"2016-01-01 18:31:46,003\")\n    # Nota: La coma en milisegundos debe convertirse a punto\n    df['revtime'] = pd.to_datetime(\n        df['revtime'].astype(str).str.replace(',', '.', regex=False),\n        format='%Y-%m-%d %H:%M:%S.%f',\n        errors='coerce'\n    )\n    \n    # Ordenar por tiempo y obtener el ÃšLTIMO registro por colada\n    df = df.sort_values('revtime')\n    grp_gas = df.groupby('heatid').last()[cols_gas].rename(columns={\n        'o2_amount': 'total_o2_lance',\n        'gas_amount': 'total_gas_lance'\n    })\n    \n    return grp_gas\n\n\ndef aggregate_injection_data(df_inj: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Agrega los datos de inyecciones de carbÃ³n por colada.\n    \n    Obtiene el ÃšLTIMO valor registrado temporalmente (por revtime) de cada colada.\n    \n    Args:\n        df_inj: DataFrame con datos de inj_mat.csv\n    \n    Returns:\n        DataFrame indexado por heatid con columna:\n        - total_injected_carbon: Ãšltimo valor de carbÃ³n inyectado\n    \"\"\"\n    df = df_inj.copy()\n    \n    # Convertir a numÃ©rico\n    df['inj_amount_carbon'] = pd.to_numeric(df['inj_amount_carbon'], errors='coerce')\n    \n    # Convertir tiempo (formato: \"2016-01-01 18:31:46,003\")\n    df['revtime'] = pd.to_datetime(\n        df['revtime'].astype(str).str.replace(',', '.', regex=False),\n        format='%Y-%m-%d %H:%M:%S.%f',\n        errors='coerce'\n    )\n    \n    # Ordenar por tiempo y obtener el ÃšLTIMO registro por colada\n    df = df.sort_values('revtime')\n    grp_inj = df.groupby('heatid').last()[['inj_amount_carbon']].rename(\n        columns={'inj_amount_carbon': 'total_injected_carbon'}\n    )\n    \n    return grp_inj\n\n\ndef aggregate_transformer_data(df_transformer: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Agrega los datos del transformador por colada.\n    \n    Calcula la ENERGÃA TOTAL consumida: MW * DuraciÃ³n (en minutos)\n    y la duraciÃ³n total del proceso.\n    \n    Args:\n        df_transformer: DataFrame con datos de eaf_transformer.csv\n    \n    Returns:\n        DataFrame indexado por heatid con columnas:\n        - total_energy: Suma de (MW * duraciÃ³n) para toda la colada\n        - total_duration: DuraciÃ³n total en minutos\n    \"\"\"\n    df = df_transformer.copy()\n    \n    # FunciÃ³n para parsear DURATION de formato \"MM: SS\" a minutos decimales\n    def parse_duration(duration_str):\n        \"\"\"Convierte 'MM: SS' a minutos decimales.\"\"\"\n        try:\n            duration_str = str(duration_str).strip()\n            parts = duration_str.split(':')\n            if len(parts) == 2:\n                minutes = float(parts[0].strip())\n                seconds = float(parts[1].strip())\n                return minutes + seconds / 60.0\n            return 0.0\n        except (ValueError, AttributeError):\n            return 0.0\n    \n    # Aplicar conversiÃ³n de duraciÃ³n\n    df['duration_minutes'] = df['duration'].apply(parse_duration)\n    \n    # Convertir MW a numÃ©rico\n    df['mw'] = pd.to_numeric(df['mw'], errors='coerce').fillna(0)\n    \n    # Calcular energÃ­a = MW * duraciÃ³n (en minutos)\n    df['energy'] = df['mw'] * df['duration_minutes']\n    \n    # Agregar por colada: SUMA de energÃ­a y duraciÃ³n\n    grp_transformer = df.groupby('heatid').agg({\n        'energy': 'sum',\n        'duration_minutes': 'sum'\n    }).rename(columns={\n        'energy': 'total_energy',\n        'duration_minutes': 'total_duration'\n    })\n    \n    return grp_transformer\n\n\ndef aggregate_charged_amount(df_ladle: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Agrega la cantidad total de material cargado por colada.\n    \n    Args:\n        df_ladle: DataFrame con datos de ladle_tapping.csv\n    \n    Returns:\n        DataFrame indexado por heatid con columna:\n        - total_charged_amount: Suma total de carga\n    \"\"\"\n    df = df_ladle.copy()\n    \n    # Convertir a numÃ©rico\n    df['charge_amount'] = pd.to_numeric(df['charge_amount'], errors='coerce').fillna(0)\n    \n    # Agregar: SUMA por colada\n    grp_charged = df.groupby('heatid').agg({\n        'charge_amount': 'sum'\n    }).rename(columns={'charge_amount': 'total_charged_amount'})\n    \n    return grp_charged\n\n\nprint(\"âœ… Funciones de agregaciÃ³n definidas:\")\nprint(\"  - aggregate_gas_data(): O2 y gas inyectado (Ãºltimo valor)\")\nprint(\"  - aggregate_injection_data(): CarbÃ³n inyectado (Ãºltimo valor)\")\nprint(\"  - aggregate_transformer_data(): EnergÃ­a total (MW * tiempo)\")\nprint(\"  - aggregate_charged_amount(): Carga total (suma)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.3 FunciÃ³n de Pivotado de Materiales\n\nEsta funciÃ³n transforma la tabla de materiales aÃ±adidos en columnas individuales.\nSelecciona los **top N materiales mÃ¡s frecuentes** y crea una columna por cada uno (`added_mat_XXXXXX`).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIÃ“N DE PIVOTADO DE MATERIALES\n# =============================================================================\n\ndef pivot_materials(df_ladle: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:\n    \"\"\"\n    Pivota los materiales agregados, seleccionando los top_n mÃ¡s frecuentes.\n    \n    Transforma una tabla con mÃºltiples filas por colada (una por material)\n    en una tabla con UNA fila por colada y UNA columna por material.\n    \n    Args:\n        df_ladle: DataFrame con datos de ladle_tapping.csv\n                  Debe contener columnas: heatid, mat_code, charge_amount\n        top_n: NÃºmero de materiales mÃ¡s frecuentes a incluir (default: 10)\n    \n    Returns:\n        DataFrame pivotado indexado por heatid con columnas:\n        - added_mat_XXXXXX: Cantidad de material con cÃ³digo XXXXXX\n        \n    Example:\n        >>> pivot = pivot_materials(df_ladle, top_n=10)\n        >>> pivot.columns\n        Index(['added_mat_140107', 'added_mat_202007', ...])\n    \"\"\"\n    df = df_ladle.copy()\n    \n    # Convertir cantidad a numÃ©rico\n    df['charge_amount'] = pd.to_numeric(df['charge_amount'], errors='coerce')\n    \n    # Seleccionar los TOP N materiales por frecuencia de uso\n    top_materials = df['mat_code'].value_counts().head(top_n).index\n    logger.info(f\"Top {top_n} materiales seleccionados: {list(top_materials)}\")\n    \n    # Filtrar solo los materiales mÃ¡s frecuentes\n    df_filtered = df[df['mat_code'].isin(top_materials)]\n    \n    # Crear pivot table\n    # - index: heatid (una fila por colada)\n    # - columns: mat_code (una columna por material)\n    # - values: charge_amount (suma de cantidades)\n    # - fill_value: 0 (si no se usÃ³ el material, es 0)\n    pivot_ladle = df_filtered.pivot_table(\n        index='heatid',\n        columns='mat_code',\n        values='charge_amount',\n        aggfunc='sum',\n        fill_value=0\n    ).add_prefix('added_mat_')\n    \n    logger.info(f\"Pivot de materiales: {pivot_ladle.shape[0]} coladas, {pivot_ladle.shape[1]} materiales\")\n    \n    return pivot_ladle\n\n\ndef get_datetime_range(df_ladle: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Extrae el rango de fechas (inicio y fin) de cada colada.\n    \n    Ãštil para anÃ¡lisis temporal y filtrado por perÃ­odo.\n    \n    Args:\n        df_ladle: DataFrame con datos de ladle_tapping.csv\n    \n    Returns:\n        DataFrame con columnas:\n        - heatid: Identificador de colada\n        - fecha_inicio: Primera fecha registrada\n        - fecha_fin: Ãšltima fecha registrada\n    \"\"\"\n    df = df_ladle.copy()\n    \n    # Convertir a datetime\n    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n    \n    # Agregar: mÃ­nimo y mÃ¡ximo por colada\n    datetime_range = df.groupby('heatid').agg({\n        'datetime': ['min', 'max']\n    })\n    datetime_range.columns = ['fecha_inicio', 'fecha_fin']\n    datetime_range = datetime_range.reset_index()\n    \n    return datetime_range\n\n\nprint(\"âœ… Funciones de pivotado definidas:\")\nprint(\"  - pivot_materials(): Crea columnas por material (top N)\")\nprint(\"  - get_datetime_range(): Extrae rango de fechas por colada\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.4 Funciones de ExtracciÃ³n de Targets\n\nEstas funciones extraen las variables objetivo (targets) que queremos predecir:\n- **Temperatura final**: La Ãºltima mediciÃ³n de temperatura antes del vaciado\n- **ComposiciÃ³n quÃ­mica final**: Valores de C, Mn, Si, P, S, Cu, Cr, Mo, Ni",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIONES DE EXTRACCIÃ“N DE TARGETS\n# =============================================================================\n\ndef get_final_temperature(df_temp: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Obtiene la temperatura final (al vaciado) de cada colada.\n    \n    Toma la ÃšLTIMA mediciÃ³n de temperatura registrada temporalmente,\n    que corresponde al momento del vaciado del horno.\n    \n    Args:\n        df_temp: DataFrame con datos de eaf_temp.csv\n    \n    Returns:\n        DataFrame con columnas:\n        - heatid: Identificador de colada\n        - target_temperature: Temperatura final en Â°C\n    \"\"\"\n    df = df_temp.copy()\n    \n    # Detectar columnas automÃ¡ticamente\n    cols_temp = [c for c in df.columns if 'temp' in c and 'time' not in c]\n    cols_time = [c for c in df.columns if 'time' in c or 'date' in c]\n    \n    col_temp_name = cols_temp[0] if cols_temp else 'temp'\n    col_time_name = cols_time[0] if cols_time else 'datetime'\n    \n    logger.info(f\"Columna de temperatura detectada: {col_temp_name}\")\n    logger.info(f\"Columna de tiempo detectada: {col_time_name}\")\n    \n    # Limpiar tipos\n    df[col_temp_name] = pd.to_numeric(df[col_temp_name], errors='coerce')\n    df[col_time_name] = pd.to_datetime(df[col_time_name], errors='coerce')\n    \n    # Obtener la ÃšLTIMA mediciÃ³n (temperatura al vaciado)\n    # Ordenar por tiempo y tomar el Ãºltimo registro de cada colada\n    df_target = df.sort_values(col_time_name).groupby('heatid').tail(1)\n    \n    # Seleccionar solo ID y temperatura\n    df_target = df_target[['heatid', col_temp_name]].rename(\n        columns={col_temp_name: 'target_temperature'}\n    )\n    \n    logger.info(f\"Temperaturas extraÃ­das: {len(df_target)} coladas\")\n    \n    return df_target\n\n\ndef get_final_chemical_composition(df_chem_final: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Obtiene la composiciÃ³n quÃ­mica final de cada colada.\n    \n    Extrae los valores finales de los elementos quÃ­micos relevantes\n    para el control de calidad del acero.\n    \n    Args:\n        df_chem_final: DataFrame con datos de eaf_final_chemical_measurements.csv\n    \n    Returns:\n        DataFrame con columnas:\n        - heatid: Identificador de colada\n        - target_valc: Carbono final (%)\n        - target_valmn: Manganeso final (%)\n        - target_valsi: Silicio final (%)\n        - target_valp: FÃ³sforo final (%)\n        - target_vals: Azufre final (%)\n        - target_valcu: Cobre final (%)\n        - target_valcr: Cromo final (%)\n        - target_valmo: Molibdeno final (%)\n        - target_valni: NÃ­quel final (%)\n    \"\"\"\n    # Elementos quÃ­micos a extraer como targets (lista completa)\n    chemical_elements = [\n        'valc', 'valmn', 'valsi', 'valp', 'vals',\n        'valcu', 'valcr', 'valmo', 'valni'\n    ]\n    \n    # Verificar quÃ© columnas existen realmente en el archivo\n    available_elements = [col for col in chemical_elements if col in df_chem_final.columns]\n    \n    if not available_elements:\n        logger.warning(\"No se encontraron columnas de elementos quÃ­micos en el archivo\")\n        return pd.DataFrame()\n    \n    logger.info(f\"Elementos quÃ­micos disponibles: {available_elements}\")\n    \n    # Seleccionar heatid y elementos quÃ­micos\n    cols_to_select = ['heatid'] + available_elements\n    df_targets = df_chem_final[cols_to_select].copy()\n    \n    # Convertir a numÃ©rico (maneja tanto puntos como comas)\n    for col in available_elements:\n        df_targets[col] = pd.to_numeric(df_targets[col], errors='coerce')\n    \n    # Renombrar con prefijo target_\n    rename_dict = {col: f'target_{col}' for col in available_elements}\n    df_targets = df_targets.rename(columns=rename_dict)\n    \n    # Eliminar duplicados por heatid (tomar el Ãºltimo registro)\n    df_targets = df_targets.drop_duplicates(subset=['heatid'], keep='last')\n    \n    logger.info(f\"Targets quÃ­micos extraÃ­dos: {len(df_targets)} coladas, {len(available_elements)} elementos\")\n    \n    return df_targets\n\n\nprint(\"âœ… Funciones de targets definidas:\")\nprint(\"  - get_final_temperature(): Ãšltima temperatura por colada\")\nprint(\"  - get_final_chemical_composition(): ComposiciÃ³n quÃ­mica final (9 elementos)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.5 ConstrucciÃ³n del Dataset Maestro\n\nEstas funciones combinan todas las fuentes de datos en un Ãºnico dataset maestro, realizando los merges necesarios y la limpieza final.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CONSTRUCCIÃ“N DEL DATASET MAESTRO\n# =============================================================================\n\ndef build_master_dataset(raw_data_dir: Path) -> pd.DataFrame:\n    \"\"\"\n    Construye el dataset maestro combinando todas las fuentes de datos.\n    \n    Este es el NÃšCLEO del feature engineering. Carga todos los archivos,\n    aplica las agregaciones y fusiona todo en un Ãºnico DataFrame.\n    \n    Args:\n        raw_data_dir: Ruta al directorio con los datos raw\n    \n    Returns:\n        DataFrame maestro con todas las features de input (sin targets)\n        \n    Pipeline:\n        1. Cargar archivos CSV estandarizados\n        2. Agregar series temporales (gas, inyecciÃ³n, transformador, carga)\n        3. Pivotar materiales\n        4. Extraer rango de fechas\n        5. Fusionar todo por heatid\n        6. Rellenar nulos tÃ©cnicos con 0\n    \"\"\"\n    logger.info(\"=\" * 50)\n    logger.info(\"CONSTRUYENDO DATASET MAESTRO\")\n    logger.info(\"=\" * 50)\n    \n    # -------------------------------------------------------------------------\n    # PASO 1: Cargar archivos necesarios\n    # -------------------------------------------------------------------------\n    logger.info(\"Cargando archivos...\")\n    \n    df_gas = load_standardized(raw_data_dir / \"eaf_gaslance_mat.csv\")\n    logger.info(f\"  - eaf_gaslance_mat.csv: {df_gas.shape}\")\n    \n    df_inj = load_standardized(raw_data_dir / \"inj_mat.csv\")\n    logger.info(f\"  - inj_mat.csv: {df_inj.shape}\")\n    \n    df_ladle = load_standardized(raw_data_dir / \"ladle_tapping.csv\")\n    logger.info(f\"  - ladle_tapping.csv: {df_ladle.shape}\")\n    \n    df_chem_initial = load_standardized(raw_data_dir / \"lf_initial_chemical_measurements.csv\")\n    logger.info(f\"  - lf_initial_chemical_measurements.csv: {df_chem_initial.shape}\")\n    \n    df_transformer = load_standardized(raw_data_dir / \"eaf_transformer.csv\")\n    logger.info(f\"  - eaf_transformer.csv: {df_transformer.shape}\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 2: Agregar series temporales\n    # -------------------------------------------------------------------------\n    logger.info(\"\\nAgregando series temporales...\")\n    \n    grp_gas = aggregate_gas_data(df_gas)\n    logger.info(f\"  - Gases: {grp_gas.shape[0]} coladas\")\n    \n    grp_inj = aggregate_injection_data(df_inj)\n    logger.info(f\"  - Inyecciones: {grp_inj.shape[0]} coladas\")\n    \n    grp_transformer = aggregate_transformer_data(df_transformer)\n    logger.info(f\"  - Transformador: {grp_transformer.shape[0]} coladas\")\n    \n    grp_charged = aggregate_charged_amount(df_ladle)\n    logger.info(f\"  - Carga: {grp_charged.shape[0]} coladas\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 3: Pivotar materiales\n    # -------------------------------------------------------------------------\n    logger.info(\"\\nPivotando materiales...\")\n    pivot_ladle = pivot_materials(df_ladle, top_n=10)\n    \n    # -------------------------------------------------------------------------\n    # PASO 4: Extraer rango de fechas\n    # -------------------------------------------------------------------------\n    logger.info(\"\\nExtrayendo rango de fechas...\")\n    datetime_range = get_datetime_range(df_ladle)\n    \n    # -------------------------------------------------------------------------\n    # PASO 5: Fusionar dataset maestro\n    # -------------------------------------------------------------------------\n    logger.info(\"\\nFusionando dataset maestro...\")\n    \n    # Dataset base: mediciones quÃ­micas iniciales\n    df_master = df_chem_initial.copy()\n    logger.info(f\"  Base (quÃ­mica inicial): {df_master.shape}\")\n    \n    # Merges (left joins para preservar todos los registros base)\n    df_master = df_master.merge(grp_gas, on='heatid', how='left')\n    df_master = df_master.merge(grp_inj, on='heatid', how='left')\n    df_master = df_master.merge(grp_transformer, on='heatid', how='left')\n    df_master = df_master.merge(grp_charged, on='heatid', how='left')\n    df_master = df_master.merge(pivot_ladle, on='heatid', how='left')\n    df_master = df_master.merge(datetime_range, on='heatid', how='left')\n    \n    logger.info(f\"  DespuÃ©s de merges: {df_master.shape}\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 6: Rellenar nulos tÃ©cnicos\n    # -------------------------------------------------------------------------\n    cols_to_fix = [\n        'total_o2_lance', 'total_gas_lance', 'total_injected_carbon',\n        'total_energy', 'total_duration', 'total_charged_amount'\n    ]\n    # Solo rellenar las columnas que existen\n    cols_to_fix = [c for c in cols_to_fix if c in df_master.columns]\n    df_master[cols_to_fix] = df_master[cols_to_fix].fillna(0)\n    \n    # Rellenar columnas de materiales con 0\n    mat_cols = [c for c in df_master.columns if c.startswith('added_mat_')]\n    df_master[mat_cols] = df_master[mat_cols].fillna(0)\n    \n    logger.info(f\"\\nDataset maestro (inputs): {df_master.shape}\")\n    logger.info(f\"  - Filas: {len(df_master)}\")\n    logger.info(f\"  - Columnas: {len(df_master.columns)}\")\n    \n    return df_master\n\n\ndef add_target_temperature(df_master: pd.DataFrame, raw_data_dir: Path) -> pd.DataFrame:\n    \"\"\"\n    Agrega la variable target de temperatura al dataset maestro.\n    \n    Args:\n        df_master: DataFrame con inputs\n        raw_data_dir: Ruta al directorio con los datos raw\n    \n    Returns:\n        DataFrame final con inputs y target_temperature\n    \"\"\"\n    logger.info(\"\\nAgregando target de temperatura...\")\n    \n    # Cargar y extraer temperatura final\n    df_temp = load_standardized(raw_data_dir / \"eaf_temp.csv\")\n    df_target = get_final_temperature(df_temp)\n    \n    # Merge (inner join - solo coladas con datos completos)\n    df_final = df_master.merge(df_target, on='heatid', how='inner')\n    \n    # Limpieza: eliminar columnas innecesarias\n    cols_drop = ['datetime', 'positionrow', 'filter_key_date', 'measure_time']\n    df_final = df_final.drop(columns=[c for c in cols_drop if c in df_final.columns])\n    \n    # Eliminar filas donde el target es nulo\n    df_final = df_final.dropna(subset=['target_temperature'])\n    \n    # Rellenar nulos restantes en inputs con 0\n    df_final = df_final.fillna(0)\n    \n    logger.info(f\"Dataset con temperatura: {df_final.shape}\")\n    \n    return df_final\n\n\ndef add_target_chemical(df_master: pd.DataFrame, raw_data_dir: Path) -> pd.DataFrame:\n    \"\"\"\n    Agrega las variables target de composiciÃ³n quÃ­mica al dataset maestro.\n    \n    Args:\n        df_master: DataFrame con inputs\n        raw_data_dir: Ruta al directorio con los datos raw\n    \n    Returns:\n        DataFrame final con inputs y targets quÃ­micos\n    \"\"\"\n    logger.info(\"\\nAgregando targets de composiciÃ³n quÃ­mica...\")\n    \n    # Cargar y extraer composiciÃ³n quÃ­mica final\n    df_chem_final = load_standardized(raw_data_dir / \"eaf_final_chemical_measurements.csv\")\n    df_targets = get_final_chemical_composition(df_chem_final)\n    \n    if df_targets.empty:\n        raise ValueError(\"No se pudieron extraer los targets quÃ­micos\")\n    \n    # Merge (inner join - solo coladas con datos completos)\n    df_final = df_master.merge(df_targets, on='heatid', how='inner')\n    \n    # Limpieza: eliminar columnas innecesarias\n    cols_drop = ['datetime', 'positionrow', 'filter_key_date', 'measure_time']\n    df_final = df_final.drop(columns=[c for c in cols_drop if c in df_final.columns])\n    \n    # Eliminar filas donde TODOS los targets son nulos\n    target_cols = [c for c in df_final.columns if c.startswith('target_')]\n    df_final = df_final.dropna(subset=target_cols, how='all')\n    \n    # Rellenar nulos restantes con 0\n    df_final = df_final.fillna(0)\n    \n    logger.info(f\"Dataset con quÃ­mica: {df_final.shape}\")\n    logger.info(f\"Targets: {target_cols}\")\n    \n    return df_final\n\n\nprint(\"âœ… Funciones de construcciÃ³n definidas:\")\nprint(\"  - build_master_dataset(): Crea dataset de inputs\")\nprint(\"  - add_target_temperature(): Agrega target de temperatura\")\nprint(\"  - add_target_chemical(): Agrega targets quÃ­micos\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.6 FunciÃ³n Orquestadora: build_features()\n\nEsta funciÃ³n principal orquesta todo el pipeline de feature engineering y genera los dos datasets finales listos para Machine Learning.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIÃ“N ORQUESTADORA: BUILD_FEATURES\n# =============================================================================\n\ndef build_features(force: bool = False) -> Tuple[Optional[Path], Optional[Path]]:\n    \"\"\"\n    Pipeline completo para construir los datasets finales.\n    \n    Orquesta todo el proceso de feature engineering:\n    1. Verifica que existan los datos raw\n    2. Construye el dataset maestro de inputs\n    3. Genera dataset_final_temp.csv (para predicciÃ³n de temperatura)\n    4. Genera dataset_final_chemical.csv (para predicciÃ³n de composiciÃ³n quÃ­mica)\n    \n    Args:\n        force: Si es True, reconstruye aunque existan los archivos.\n               Si es False (default), salta si ya existen.\n    \n    Returns:\n        Tuple con (path_temp, path_chemical):\n        - path_temp: Ruta al dataset de temperatura\n        - path_chemical: Ruta al dataset quÃ­mico\n    \n    Example:\n        >>> path_temp, path_chem = build_features()\n        >>> path_temp, path_chem = build_features(force=True)  # Reconstruir\n    \"\"\"\n    # Definir rutas\n    raw_data_dir = DATA_RAW\n    processed_data_dir = DATA_PROCESSED\n    \n    output_temp = processed_data_dir / \"dataset_final_temp.csv\"\n    output_chem = processed_data_dir / \"dataset_final_chemical.csv\"\n    \n    # -------------------------------------------------------------------------\n    # Verificar datos raw\n    # -------------------------------------------------------------------------\n    if not raw_data_dir.exists():\n        raise FileNotFoundError(\n            f\"No existe el directorio {raw_data_dir}. \"\n            \"Ejecuta primero download_data() para descargar los datos.\"\n        )\n    \n    # Crear directorio de salida\n    processed_data_dir.mkdir(parents=True, exist_ok=True)\n    \n    # -------------------------------------------------------------------------\n    # Verificar si ya existen los datasets\n    # -------------------------------------------------------------------------\n    if output_temp.exists() and output_chem.exists() and not force:\n        logger.info(\"Los datasets ya existen. Usa force=True para reconstruir.\")\n        print(f\"\\n{'='*60}\")\n        print(\"DATASETS YA DISPONIBLES\")\n        print(f\"{'='*60}\")\n        print(f\"  - Temperatura: {output_temp}\")\n        print(f\"  - QuÃ­mico: {output_chem}\")\n        print(\"\\nUsa build_features(force=True) para reconstruir.\")\n        return (output_temp, output_chem)\n    \n    # -------------------------------------------------------------------------\n    # Construir dataset maestro (solo una vez, se reutiliza)\n    # -------------------------------------------------------------------------\n    df_master = build_master_dataset(raw_data_dir)\n    \n    # -------------------------------------------------------------------------\n    # Dataset de TEMPERATURA\n    # -------------------------------------------------------------------------\n    print(f\"\\n{'='*60}\")\n    print(\"CONSTRUYENDO DATASET DE TEMPERATURA\")\n    print(f\"{'='*60}\")\n    \n    df_temp = add_target_temperature(df_master.copy(), raw_data_dir)\n    \n    # Guardar\n    df_temp.to_csv(output_temp, index=False)\n    logger.info(f\"Dataset de temperatura guardado: {output_temp}\")\n    \n    # Mostrar resumen\n    print(f\"\\nðŸ“Š Dataset de Temperatura:\")\n    print(f\"   Filas: {len(df_temp):,}\")\n    print(f\"   Columnas: {len(df_temp.columns)}\")\n    print(f\"   Target: target_temperature\")\n    print(f\"   Rango temperatura: [{df_temp['target_temperature'].min():.0f}, {df_temp['target_temperature'].max():.0f}] Â°C\")\n    \n    # -------------------------------------------------------------------------\n    # Dataset QUÃMICO\n    # -------------------------------------------------------------------------\n    print(f\"\\n{'='*60}\")\n    print(\"CONSTRUYENDO DATASET DE COMPOSICIÃ“N QUÃMICA\")\n    print(f\"{'='*60}\")\n    \n    df_chem = add_target_chemical(df_master.copy(), raw_data_dir)\n    \n    # Guardar\n    df_chem.to_csv(output_chem, index=False)\n    logger.info(f\"Dataset quÃ­mico guardado: {output_chem}\")\n    \n    # Mostrar resumen\n    target_cols = [c for c in df_chem.columns if c.startswith('target_')]\n    print(f\"\\nðŸ“Š Dataset QuÃ­mico:\")\n    print(f\"   Filas: {len(df_chem):,}\")\n    print(f\"   Columnas: {len(df_chem.columns)}\")\n    print(f\"   Targets ({len(target_cols)}): {target_cols}\")\n    \n    # -------------------------------------------------------------------------\n    # Resumen final\n    # -------------------------------------------------------------------------\n    print(f\"\\n{'='*60}\")\n    print(\"CONSTRUCCIÃ“N COMPLETADA\")\n    print(f\"{'='*60}\")\n    print(f\"âœ… Temperatura: {output_temp}\")\n    print(f\"   Size: {output_temp.stat().st_size / 1024:.1f} KB\")\n    print(f\"âœ… QuÃ­mico: {output_chem}\")\n    print(f\"   Size: {output_chem.stat().st_size / 1024:.1f} KB\")\n    print(f\"{'='*60}\")\n    \n    logger.info(\"Feature engineering completado exitosamente\")\n    \n    return (output_temp, output_chem)\n\n\nprint(\"âœ… FunciÃ³n orquestadora definida: build_features(force=False)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.7 EjecuciÃ³n del Feature Engineering\n\nEjecutamos el pipeline completo para generar los datasets procesados.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# EJECUCIÃ“N DEL FEATURE ENGINEERING\n# =============================================================================\n\n# Ejecutar pipeline de construcciÃ³n de features\n# Usa force=True para reconstruir aunque ya existan\npath_temp, path_chem = build_features(force=False)\n\n# Cargar datasets para verificaciÃ³n\nprint(f\"\\n{'='*60}\")\nprint(\"VERIFICACIÃ“N DE DATASETS GENERADOS\")\nprint(f\"{'='*60}\")\n\n# Dataset de temperatura\ndf_temp_check = pd.read_csv(path_temp)\nprint(f\"\\nðŸ“Š Dataset de Temperatura ({path_temp.name}):\")\nprint(f\"   Shape: {df_temp_check.shape}\")\nprint(f\"   Columnas: {list(df_temp_check.columns[:10])}...\")\nprint(f\"\\n   EstadÃ­sticas del target:\")\nprint(df_temp_check['target_temperature'].describe())\n\n# Dataset quÃ­mico\ndf_chem_check = pd.read_csv(path_chem)\ntarget_cols = [c for c in df_chem_check.columns if c.startswith('target_')]\nprint(f\"\\nðŸ“Š Dataset QuÃ­mico ({path_chem.name}):\")\nprint(f\"   Shape: {df_chem_check.shape}\")\nprint(f\"   Targets: {target_cols}\")\nprint(f\"\\n   EstadÃ­sticas de targets quÃ­micos:\")\nprint(df_chem_check[target_cols].describe().T[['mean', 'std', 'min', 'max']])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Fin de PARTE 3\n\nEl Feature Engineering estÃ¡ completo. Se han generado:\n\n| Dataset | Archivo | DescripciÃ³n |\n|---------|---------|-------------|\n| Temperatura | `dataset_final_temp.csv` | Para predicciÃ³n de temperatura de vaciado |\n| QuÃ­mico | `dataset_final_chemical.csv` | Para predicciÃ³n de composiciÃ³n quÃ­mica final |\n\n**Transformaciones aplicadas:**\n- Series temporales agregadas por colada (heatid)\n- Top 10 materiales pivotados como columnas\n- Targets extraÃ­dos de mediciones finales\n- Nulos tÃ©cnicos rellenados con 0\n\nSiguiente fase:\n- **PARTE 4**: Entrenamiento de Modelos",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## PARTE 4: Modelado de Temperatura\n\nEn esta secciÃ³n implementamos el entrenamiento de modelos para predecir la **temperatura final de vaciado** del horno de arco elÃ©ctrico.\n\n**Modelos disponibles:**\n- **XGBoost Regressor**: Gradient boosting optimizado\n- **Random Forest Regressor**: Ensemble de Ã¡rboles de decisiÃ³n\n- **Linear Regression**: Modelo base lineal\n\n**Pipeline de entrenamiento:**\n1. Carga y limpieza de datos procesados\n2. Filtrado de nulos y preparaciÃ³n de features\n3. Split Train/Test (80/20)\n4. Entrenamiento del modelo seleccionado\n5. EvaluaciÃ³n con mÃ©tricas (RMSE, RÂ², MAE)\n6. VisualizaciÃ³n de resultados",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 4.1 Funciones Auxiliares de Carga y MÃ©tricas\n\nFunciones para cargar datos procesados y calcular mÃ©tricas de evaluaciÃ³n.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIONES AUXILIARES PARA MODELADO\n# =============================================================================\n\ndef load_and_clean_data(filename: str = \"dataset_final_temp.csv\") -> pd.DataFrame:\n    \"\"\"\n    Carga y limpia el dataset procesado.\n    \n    Realiza un DOBLE CHECK para asegurar que las columnas numÃ©ricas\n    son realmente floats, convirtiendo formatos europeos si es necesario.\n    \n    Args:\n        filename: Nombre del archivo CSV en data/processed/\n    \n    Returns:\n        DataFrame limpio con tipos de datos correctos\n    \n    Raises:\n        FileNotFoundError: Si no existe el archivo\n    \n    Example:\n        >>> df = load_and_clean_data(\"dataset_final_temp.csv\")\n        >>> df = load_and_clean_data(\"dataset_final_chemical.csv\")\n    \"\"\"\n    # Construir ruta\n    data_path = DATA_PROCESSED / filename\n    \n    if not data_path.exists():\n        raise FileNotFoundError(\n            f\"No se encuentra el dataset en: {data_path}\\n\"\n            \"Ejecuta build_features() primero para generar los datos procesados.\"\n        )\n    \n    # Cargar CSV\n    df = pd.read_csv(data_path)\n    logger.info(f\"Cargado: {filename} - Shape: {df.shape}\")\n    \n    # -------------------------------------------------------------------------\n    # DOBLE CHECK: Asegurar que columnas quÃ­micas son float\n    # Convierte formato europeo (coma) si es necesario\n    # -------------------------------------------------------------------------\n    for col in CHEMICAL_COLUMNS:\n        if col in df.columns:\n            # Si es object (string), convertir comas a puntos\n            if df[col].dtype == 'object':\n                df[col] = df[col].astype(str).str.replace(',', '.', regex=False)\n            # Convertir a numÃ©rico\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n    \n    # TambiÃ©n verificar columnas target_val*\n    target_cols = [c for c in df.columns if c.startswith('target_val')]\n    for col in target_cols:\n        if df[col].dtype == 'object':\n            df[col] = df[col].astype(str).str.replace(',', '.', regex=False)\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    \n    # Verificar columnas numÃ©ricas principales\n    numeric_cols = [\n        'total_o2_lance', 'total_gas_lance', 'total_injected_carbon',\n        'total_energy', 'total_duration', 'total_charged_amount',\n        'target_temperature'\n    ]\n    for col in numeric_cols:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors='coerce')\n    \n    return df\n\n\ndef calculate_metrics(\n    y_true: np.ndarray,\n    y_pred: np.ndarray\n) -> Dict[str, float]:\n    \"\"\"\n    Calcula mÃ©tricas de evaluaciÃ³n para regresiÃ³n.\n    \n    Args:\n        y_true: Valores reales (ground truth)\n        y_pred: Valores predichos por el modelo\n    \n    Returns:\n        Diccionario con mÃ©tricas:\n        - RMSE: Root Mean Squared Error\n        - R2: Coeficiente de determinaciÃ³n\n        - MAE: Mean Absolute Error\n    \n    Example:\n        >>> metrics = calculate_metrics(y_test, y_pred)\n        >>> print(f\"RMSE: {metrics['RMSE']:.2f}\")\n    \"\"\"\n    y_true_arr = np.asarray(y_true)\n    y_pred_arr = np.asarray(y_pred)\n    \n    # RMSE: RaÃ­z del error cuadrÃ¡tico medio\n    rmse = float(np.sqrt(mean_squared_error(y_true_arr, y_pred_arr)))\n    \n    # RÂ²: Coeficiente de determinaciÃ³n (1.0 = perfecto)\n    r2 = float(r2_score(y_true_arr, y_pred_arr))\n    \n    # MAE: Error absoluto medio\n    mae = float(np.mean(np.abs(y_true_arr - y_pred_arr)))\n    \n    return {\n        'RMSE': rmse,\n        'R2': r2,\n        'MAE': mae\n    }\n\n\ndef get_feature_importance(\n    model: Any,\n    feature_names: List[str],\n    model_type: str\n) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Obtiene la importancia de caracterÃ­sticas del modelo.\n    \n    Funciona con diferentes tipos de modelos:\n    - Linear: Usa valores absolutos de coeficientes\n    - Tree-based (RF, XGBoost): Usa feature_importances_\n    \n    Args:\n        model: Modelo entrenado (sklearn o xgboost)\n        feature_names: Lista de nombres de features\n        model_type: Tipo de modelo ('xgboost', 'random_forest', 'linear')\n    \n    Returns:\n        DataFrame con columnas 'Feature' e 'Importance',\n        ordenado ascendente por importancia (para grÃ¡ficos horizontales).\n        Retorna None si el modelo no es soportado.\n    \"\"\"\n    if model_type == 'linear':\n        # Para regresiÃ³n lineal, usar valor absoluto de coeficientes\n        importance = np.abs(model.coef_)\n    elif model_type in ['random_forest', 'xgboost']:\n        # Para modelos de Ã¡rbol, usar feature_importances_\n        importance = model.feature_importances_\n    else:\n        logger.warning(f\"Tipo de modelo no soportado para importancia: {model_type}\")\n        return None\n    \n    # Crear DataFrame ordenado\n    importance_df = pd.DataFrame({\n        'Feature': feature_names,\n        'Importance': importance\n    }).sort_values('Importance', ascending=True)\n    \n    return importance_df\n\n\nprint(\"âœ… Funciones auxiliares definidas:\")\nprint(\"  - load_and_clean_data(): Carga datos procesados con limpieza\")\nprint(\"  - calculate_metrics(): Calcula RMSE, RÂ², MAE\")\nprint(\"  - get_feature_importance(): Extrae importancia de features\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 FunciÃ³n Principal: train_temperature_model()\n\nEsta funciÃ³n entrena un modelo de predicciÃ³n de temperatura con el tipo y hiperparÃ¡metros especificados.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIÃ“N PRINCIPAL: ENTRENAMIENTO DE MODELO DE TEMPERATURA\n# =============================================================================\n\ndef train_temperature_model(\n    model_type: str = 'xgboost',\n    n_estimators: int = None,\n    max_depth: int = None,\n    learning_rate: float = None,\n    test_size: float = None,\n    random_state: int = None,\n    save_model: bool = True,\n    feature_list: List[str] = None\n) -> Tuple[Any, Dict[str, float], List[str], pd.DataFrame, pd.Series, np.ndarray, Optional[Path]]:\n    \"\"\"\n    Entrena un modelo de predicciÃ³n de temperatura.\n    \n    Args:\n        model_type: Tipo de modelo ('xgboost', 'random_forest', 'linear')\n        n_estimators: NÃºmero de estimadores para tree models (default: 100)\n        max_depth: Profundidad mÃ¡xima de Ã¡rboles (default: 6)\n        learning_rate: Learning rate para XGBoost (default: 0.1)\n        test_size: ProporciÃ³n de datos para test (default: 0.2)\n        random_state: Semilla para reproducibilidad (default: 42)\n        save_model: Si True, guarda el modelo en disco\n        feature_list: Lista de features a usar. Si None, usa INPUT_FEATURES\n    \n    Returns:\n        Tuple con:\n        - model: Modelo entrenado\n        - metrics: Dict con RMSE, RÂ², MAE\n        - feature_names: Lista de features usadas\n        - X_test: DataFrame de features de test\n        - y_test: Series con valores reales de test\n        - y_pred: Array con predicciones\n        - model_path: Path al modelo guardado (None si save_model=False)\n    \n    Example:\n        >>> model, metrics, features, X_test, y_test, y_pred, path = train_temperature_model()\n        >>> model, metrics, *_ = train_temperature_model(model_type='random_forest')\n    \"\"\"\n    # -------------------------------------------------------------------------\n    # Usar hiperparÃ¡metros por defecto si no se especifican\n    # -------------------------------------------------------------------------\n    n_estimators = n_estimators or DEFAULT_HYPERPARAMS['n_estimators']\n    max_depth = max_depth or DEFAULT_HYPERPARAMS['max_depth']\n    learning_rate = learning_rate or DEFAULT_HYPERPARAMS['learning_rate']\n    test_size = test_size or DEFAULT_HYPERPARAMS['test_size']\n    random_state = random_state or DEFAULT_HYPERPARAMS['random_state']\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"ENTRENAMIENTO DE MODELO DE TEMPERATURA\")\n    print(f\"{'='*60}\")\n    print(f\"Modelo: {MODEL_DISPLAY_NAMES.get(model_type, model_type)}\")\n    print(f\"HiperparÃ¡metros:\")\n    print(f\"  - n_estimators: {n_estimators}\")\n    print(f\"  - max_depth: {max_depth}\")\n    print(f\"  - learning_rate: {learning_rate}\")\n    print(f\"  - test_size: {test_size}\")\n    print(f\"  - random_state: {random_state}\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 1: Cargar datos\n    # -------------------------------------------------------------------------\n    logger.info(\"Cargando datos...\")\n    df = load_and_clean_data(\"dataset_final_temp.csv\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 2: Preparar features y target\n    # -------------------------------------------------------------------------\n    # Usar feature_list personalizada o INPUT_FEATURES por defecto\n    if feature_list is not None:\n        feature_cols = [f for f in feature_list if f in df.columns]\n    else:\n        feature_cols = [f for f in INPUT_FEATURES if f in df.columns]\n    \n    X = df[feature_cols].copy()\n    y = df['target_temperature'].copy()\n    \n    print(f\"\\nDataset original: {len(df)} filas\")\n    print(f\"Features disponibles: {len(feature_cols)}\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 3: Eliminar filas con valores nulos\n    # -------------------------------------------------------------------------\n    mask = ~(X.isnull().any(axis=1) | y.isnull())\n    X = X[mask]\n    y = y[mask]\n    \n    print(f\"DespuÃ©s de filtrar nulos: {len(X)} filas\")\n    logger.info(f\"Dataset limpio: {len(X)} muestras, {len(feature_cols)} features\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 4: Split Train/Test (80/20)\n    # -------------------------------------------------------------------------\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    \n    print(f\"\\nSplit Train/Test:\")\n    print(f\"  - Train: {len(X_train)} muestras ({100*(1-test_size):.0f}%)\")\n    print(f\"  - Test: {len(X_test)} muestras ({100*test_size:.0f}%)\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 5: Crear y entrenar modelo\n    # -------------------------------------------------------------------------\n    logger.info(f\"Entrenando modelo: {MODEL_DISPLAY_NAMES.get(model_type, model_type)}\")\n    \n    if model_type == 'linear':\n        model = LinearRegression()\n    elif model_type == 'random_forest':\n        model = RandomForestRegressor(\n            n_estimators=n_estimators,\n            max_depth=max_depth,\n            random_state=random_state,\n            n_jobs=-1\n        )\n    elif model_type == 'xgboost':\n        model = XGBRegressor(\n            n_estimators=n_estimators,\n            max_depth=max_depth,\n            learning_rate=learning_rate,\n            random_state=random_state,\n            n_jobs=-1\n        )\n    else:\n        raise ValueError(f\"Modelo no reconocido: {model_type}. Use: 'xgboost', 'random_forest', 'linear'\")\n    \n    # Entrenar\n    print(f\"\\nEntrenando {MODEL_DISPLAY_NAMES.get(model_type, model_type)}...\")\n    model.fit(X_train, y_train)\n    print(\"âœ… Entrenamiento completado\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 6: Predecir y evaluar\n    # -------------------------------------------------------------------------\n    y_pred = model.predict(X_test)\n    metrics = calculate_metrics(y_test, y_pred)\n    \n    print(f\"\\n{'='*60}\")\n    print(\"MÃ‰TRICAS DE EVALUACIÃ“N\")\n    print(f\"{'='*60}\")\n    print(f\"  RMSE: {metrics['RMSE']:.2f} Â°C\")\n    print(f\"  RÂ²:   {metrics['R2']:.4f}\")\n    print(f\"  MAE:  {metrics['MAE']:.2f} Â°C\")\n    print(f\"{'='*60}\")\n    \n    logger.info(f\"RMSE: {metrics['RMSE']:.2f}, R2: {metrics['R2']:.4f}, MAE: {metrics['MAE']:.2f}\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 7: Guardar modelo (opcional)\n    # -------------------------------------------------------------------------\n    model_path = None\n    if save_model:\n        from datetime import datetime\n        \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        model_name = f\"temp_{model_type}_{timestamp}\"\n        \n        # Crear subdirectorio para este modelo\n        model_subdir = MODELS_DIR / model_name\n        model_subdir.mkdir(exist_ok=True)\n        \n        model_path = model_subdir / \"model.joblib\"\n        metadata_path = model_subdir / \"metadata.json\"\n        \n        # Guardar modelo\n        joblib.dump(model, model_path)\n        logger.info(f\"Modelo guardado en: {model_path}\")\n        \n        # Guardar metadatos\n        metadata = {\n            \"model_type\": model_type,\n            \"model_display_name\": MODEL_DISPLAY_NAMES.get(model_type, model_type),\n            \"features\": feature_cols,\n            \"hyperparameters\": {\n                \"n_estimators\": n_estimators,\n                \"max_depth\": max_depth,\n                \"learning_rate\": learning_rate,\n                \"test_size\": test_size,\n                \"random_state\": random_state\n            },\n            \"metrics\": metrics,\n            \"timestamp\": timestamp,\n            \"target\": \"target_temperature\",\n            \"n_samples_train\": len(X_train),\n            \"n_samples_test\": len(X_test)\n        }\n        with open(metadata_path, 'w') as f:\n            json.dump(metadata, f, indent=4)\n        \n        print(f\"\\nðŸ’¾ Modelo guardado en: {model_subdir}\")\n    \n    return model, metrics, feature_cols, X_test, y_test, y_pred, model_path\n\n\nprint(\"âœ… FunciÃ³n principal definida: train_temperature_model()\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.3 Funciones de VisualizaciÃ³n\n\nFunciones para generar grÃ¡ficos de evaluaciÃ³n del modelo.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIONES DE VISUALIZACIÃ“N\n# =============================================================================\n\ndef plot_predictions_vs_real(\n    y_test: np.ndarray,\n    y_pred: np.ndarray,\n    metrics: Dict[str, float],\n    title: str = \"PredicciÃ³n vs Real - Temperatura\"\n) -> plt.Figure:\n    \"\"\"\n    Genera un scatter plot de predicciones vs valores reales.\n    \n    Args:\n        y_test: Valores reales\n        y_pred: Valores predichos\n        metrics: Diccionario con mÃ©tricas (RMSE, RÂ², MAE)\n        title: TÃ­tulo del grÃ¡fico\n    \n    Returns:\n        Figure de matplotlib\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Scatter plot\n    ax.scatter(y_test, y_pred, alpha=0.5, color='steelblue', edgecolors='white', linewidth=0.5)\n    \n    # LÃ­nea de predicciÃ³n perfecta (diagonal)\n    min_val = min(np.min(y_test), np.min(y_pred))\n    max_val = max(np.max(y_test), np.max(y_pred))\n    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='PredicciÃ³n Perfecta')\n    \n    # Etiquetas y tÃ­tulo\n    ax.set_xlabel('Valor Real (Â°C)', fontsize=12)\n    ax.set_ylabel('Valor Predicho (Â°C)', fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    \n    # AÃ±adir mÃ©tricas como texto\n    textstr = f\"RMSE: {metrics['RMSE']:.2f} Â°C\\nRÂ²: {metrics['R2']:.4f}\\nMAE: {metrics['MAE']:.2f} Â°C\"\n    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=11,\n            verticalalignment='top', bbox=props)\n    \n    ax.legend(loc='lower right')\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    return fig\n\n\ndef plot_feature_importance(\n    model: Any,\n    feature_names: List[str],\n    model_type: str,\n    top_n: int = 15,\n    title: str = \"Importancia de Variables - Temperatura\"\n) -> plt.Figure:\n    \"\"\"\n    Genera un bar plot horizontal de importancia de features.\n    \n    Args:\n        model: Modelo entrenado\n        feature_names: Lista de nombres de features\n        model_type: Tipo de modelo ('xgboost', 'random_forest', 'linear')\n        top_n: NÃºmero de features top a mostrar\n        title: TÃ­tulo del grÃ¡fico\n    \n    Returns:\n        Figure de matplotlib\n    \"\"\"\n    importance_df = get_feature_importance(model, feature_names, model_type)\n    \n    if importance_df is None:\n        print(f\"âš ï¸ No se puede calcular importancia para modelo tipo: {model_type}\")\n        return None\n    \n    # Tomar top N features\n    top_features = importance_df.tail(top_n)\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Bar plot horizontal\n    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n    bars = ax.barh(top_features['Feature'], top_features['Importance'], color=colors)\n    \n    # Etiquetas y tÃ­tulo\n    ax.set_xlabel('Importancia', fontsize=12)\n    ax.set_ylabel('Feature', fontsize=12)\n    ax.set_title(f'{title}\\n(Top {top_n} Variables)', fontsize=14, fontweight='bold')\n    \n    # AÃ±adir valores en las barras\n    for bar, val in zip(bars, top_features['Importance']):\n        ax.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n                f'{val:.3f}', va='center', fontsize=9)\n    \n    ax.grid(True, axis='x', alpha=0.3)\n    \n    plt.tight_layout()\n    return fig\n\n\ndef plot_residuals(\n    y_test: np.ndarray,\n    y_pred: np.ndarray,\n    title: str = \"AnÃ¡lisis de Residuos - Temperatura\"\n) -> plt.Figure:\n    \"\"\"\n    Genera grÃ¡ficos de anÃ¡lisis de residuos.\n    \n    Args:\n        y_test: Valores reales\n        y_pred: Valores predichos\n        title: TÃ­tulo del grÃ¡fico\n    \n    Returns:\n        Figure de matplotlib\n    \"\"\"\n    residuals = np.asarray(y_test) - np.asarray(y_pred)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # Subplot 1: Residuos vs Predichos\n    ax1 = axes[0]\n    ax1.scatter(y_pred, residuals, alpha=0.5, color='steelblue')\n    ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)\n    ax1.set_xlabel('Valor Predicho (Â°C)', fontsize=11)\n    ax1.set_ylabel('Residuo (Â°C)', fontsize=11)\n    ax1.set_title('Residuos vs Predicciones', fontsize=12)\n    ax1.grid(True, alpha=0.3)\n    \n    # Subplot 2: Histograma de residuos\n    ax2 = axes[1]\n    ax2.hist(residuals, bins=30, color='steelblue', edgecolor='white', alpha=0.7)\n    ax2.axvline(x=0, color='red', linestyle='--', linewidth=2)\n    ax2.set_xlabel('Residuo (Â°C)', fontsize=11)\n    ax2.set_ylabel('Frecuencia', fontsize=11)\n    ax2.set_title('DistribuciÃ³n de Residuos', fontsize=12)\n    \n    # AÃ±adir estadÃ­sticas\n    mean_res = np.mean(residuals)\n    std_res = np.std(residuals)\n    textstr = f'Media: {mean_res:.2f}\\nStd: {std_res:.2f}'\n    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n    ax2.text(0.95, 0.95, textstr, transform=ax2.transAxes, fontsize=10,\n             verticalalignment='top', horizontalalignment='right', bbox=props)\n    \n    fig.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    return fig\n\n\nprint(\"âœ… Funciones de visualizaciÃ³n definidas:\")\nprint(\"  - plot_predictions_vs_real(): Scatter plot predicciÃ³n vs real\")\nprint(\"  - plot_feature_importance(): Bar plot de importancia de features\")\nprint(\"  - plot_residuals(): AnÃ¡lisis de residuos\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.4 EjecuciÃ³n del Entrenamiento\n\nEntrenamos un modelo XGBoost para predicciÃ³n de temperatura y visualizamos los resultados.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# ENTRENAMIENTO DEL MODELO DE TEMPERATURA\n# =============================================================================\n\n# Entrenar modelo XGBoost (por defecto)\n# Puedes cambiar model_type a 'random_forest' o 'linear'\ntemp_model, temp_metrics, temp_features, X_test_temp, y_test_temp, y_pred_temp, temp_model_path = train_temperature_model(\n    model_type='xgboost',      # Opciones: 'xgboost', 'random_forest', 'linear'\n    n_estimators=100,          # NÃºmero de Ã¡rboles\n    max_depth=6,               # Profundidad mÃ¡xima\n    learning_rate=0.1,         # Learning rate\n    test_size=0.2,             # 20% para test\n    random_state=42,           # Semilla para reproducibilidad\n    save_model=True            # Guardar modelo en disco\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# VISUALIZACIÃ“N DE RESULTADOS - TEMPERATURA\n# =============================================================================\n\n# GrÃ¡fico 1: PredicciÃ³n vs Real\nfig1 = plot_predictions_vs_real(\n    y_test_temp, \n    y_pred_temp, \n    temp_metrics,\n    title=\"PredicciÃ³n vs Real - Temperatura (XGBoost)\"\n)\nplt.show()\n\n# GrÃ¡fico 2: Importancia de Features (Top 15)\nfig2 = plot_feature_importance(\n    temp_model,\n    temp_features,\n    model_type='xgboost',\n    top_n=15,\n    title=\"Importancia de Variables - Temperatura\"\n)\nplt.show()\n\n# GrÃ¡fico 3: AnÃ¡lisis de Residuos\nfig3 = plot_residuals(\n    y_test_temp,\n    y_pred_temp,\n    title=\"AnÃ¡lisis de Residuos - Modelo de Temperatura\"\n)\nplt.show()\n\nprint(\"\\nâœ… Visualizaciones completadas\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Fin de PARTE 4\n\nEl modelo de temperatura ha sido entrenado y evaluado:\n\n| MÃ©trica | Valor |\n|---------|-------|\n| RMSE | Error cuadrÃ¡tico medio |\n| RÂ² | Coeficiente de determinaciÃ³n |\n| MAE | Error absoluto medio |\n\n**Archivos generados:**\n- `models/temp_xgboost_YYYYMMDD_HHMMSS/model.joblib`: Modelo serializado\n- `models/temp_xgboost_YYYYMMDD_HHMMSS/metadata.json`: Metadatos y mÃ©tricas\n\nSiguiente fase:\n- **PARTE 5**: Modelado de ComposiciÃ³n QuÃ­mica",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## PARTE 5: Modelado de ComposiciÃ³n QuÃ­mica\n\nLa predicciÃ³n quÃ­mica es mÃ¡s delicada debido a:\n- **Outliers extremos**: Valores atÃ­picos que distorsionan las mÃ©tricas\n- **Data Leakage potencial**: Usar el valor inicial del mismo elemento como feature\n- **MÃºltiples targets**: 9 elementos quÃ­micos diferentes\n\n**Estrategias implementadas:**\n1. **ExclusiÃ³n inteligente de features**: Se excluye el valor inicial del mismo elemento\n2. **Capping de outliers**: Se eliminan el 1% inferior y superior (cuantiles 0.01-0.99)\n3. **ImputaciÃ³n de NaNs**: Features con valor 0, filas con target NaN eliminadas",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 5.1 Funciones Auxiliares para Modelos QuÃ­micos\n\nFunciones especÃ­ficas para el manejo de datos quÃ­micos.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIONES AUXILIARES PARA MODELOS QUÃMICOS\n# =============================================================================\n\ndef load_chemical_data() -> pd.DataFrame:\n    \"\"\"\n    Carga el dataset especÃ­fico para modelos quÃ­micos.\n    \n    Returns:\n        DataFrame con los datos quÃ­micos procesados\n    \"\"\"\n    return load_and_clean_data(\"dataset_final_chemical.csv\")\n\n\ndef get_chemical_features(df: pd.DataFrame, target: str) -> List[str]:\n    \"\"\"\n    Obtiene la lista de features disponibles para entrenamiento quÃ­mico,\n    aplicando exclusiones inteligentes para evitar data leakage.\n    \n    EXCLUSIONES:\n    1. El propio target (target_valc, etc.)\n    2. El valor inicial del mismo elemento (valc, etc.) - EVITA DATA LEAKAGE\n    3. Todos los demÃ¡s targets quÃ­micos\n    4. Columna de identificador (heatid)\n    \n    Args:\n        df: DataFrame con los datos\n        target: Target quÃ­mico a predecir (ej: 'target_valc')\n    \n    Returns:\n        Lista de features vÃ¡lidas para entrenamiento\n    \n    Example:\n        >>> features = get_chemical_features(df, 'target_valc')\n        >>> # ExcluirÃ¡ 'valc' y 'target_valc' de las features\n    \"\"\"\n    # Determinar el feature inicial correspondiente al target\n    # target_valc -> valc, target_valmn -> valmn, etc.\n    initial_feature_to_exclude = target.replace('target_', '')\n    \n    # Lista de columnas a excluir\n    exclude_cols = ['heatid', target]\n    \n    # Excluir TODOS los targets quÃ­micos (no solo el actual)\n    exclude_cols += [col for col in df.columns if col.startswith('target_')]\n    \n    # Excluir el feature inicial del mismo elemento\n    if initial_feature_to_exclude in df.columns:\n        exclude_cols.append(initial_feature_to_exclude)\n    \n    # Filtrar: usar INPUT_FEATURES que estÃ©n en df y NO estÃ©n en exclusiones\n    available = [\n        col for col in INPUT_FEATURES\n        if col in df.columns and col not in exclude_cols\n    ]\n    \n    return available\n\n\ndef cap_outliers(y: pd.Series, lower_quantile: float = 0.01, upper_quantile: float = 0.99) -> Tuple[pd.Series, int]:\n    \"\"\"\n    Aplica capping de outliers basado en cuantiles.\n    \n    Elimina valores extremos que distorsionan las mÃ©tricas (especialmente RÂ²).\n    \n    Args:\n        y: Serie con valores del target\n        lower_quantile: Cuantil inferior (default: 0.01 = 1%)\n        upper_quantile: Cuantil superior (default: 0.99 = 99%)\n    \n    Returns:\n        Tuple con:\n        - MÃ¡scara booleana de filas a mantener\n        - NÃºmero de outliers eliminados\n    \n    Example:\n        >>> mask, n_removed = cap_outliers(y, 0.01, 0.99)\n        >>> y_clean = y[mask]\n    \"\"\"\n    lower_bound = y.quantile(lower_quantile)\n    upper_bound = y.quantile(upper_quantile)\n    \n    mask = (y >= lower_bound) & (y <= upper_bound)\n    n_removed = (~mask).sum()\n    \n    return mask, n_removed\n\n\nprint(\"âœ… Funciones auxiliares quÃ­micas definidas:\")\nprint(\"  - load_chemical_data(): Carga dataset_final_chemical.csv\")\nprint(\"  - get_chemical_features(): SelecciÃ³n inteligente de features\")\nprint(\"  - cap_outliers(): Elimina outliers por cuantiles\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.2 FunciÃ³n Principal: train_chemical_model()\n\nEntrena un modelo para predecir un elemento quÃ­mico especÃ­fico con manejo robusto de outliers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FUNCIÃ“N PRINCIPAL: ENTRENAMIENTO DE MODELO QUÃMICO\n# =============================================================================\n\ndef train_chemical_model(\n    target: str,\n    model_type: str = 'xgboost',\n    n_estimators: int = None,\n    max_depth: int = None,\n    learning_rate: float = None,\n    test_size: float = None,\n    random_state: int = None,\n    save_model: bool = True,\n    feature_list: List[str] = None,\n    outlier_quantiles: Tuple[float, float] = (0.01, 0.99)\n) -> Tuple[Any, Dict[str, float], List[str], pd.DataFrame, pd.Series, np.ndarray, Optional[Path]]:\n    \"\"\"\n    Entrena un modelo de predicciÃ³n de composiciÃ³n quÃ­mica FINAL.\n    \n    IMPORTANTE: Implementa limpieza de outliers para evitar RÂ² negativos.\n    \n    Args:\n        target: Elemento quÃ­mico a predecir ('target_valc', 'target_valmn', etc.)\n        model_type: Tipo de modelo ('xgboost', 'random_forest', 'linear')\n        n_estimators: NÃºmero de estimadores para tree models\n        max_depth: Profundidad mÃ¡xima de Ã¡rboles\n        learning_rate: Learning rate para XGBoost\n        test_size: ProporciÃ³n de datos para test\n        random_state: Semilla para reproducibilidad\n        save_model: Si True, guarda el modelo en disco\n        feature_list: Lista personalizada de features (si None, usa selecciÃ³n inteligente)\n        outlier_quantiles: Tuple (lower, upper) para capping de outliers\n    \n    Returns:\n        Tuple con:\n        - model: Modelo entrenado\n        - metrics: Dict con RMSE, RÂ², MAE\n        - feature_names: Lista de features usadas\n        - X_test: DataFrame de features de test\n        - y_test: Series con valores reales de test\n        - y_pred: Array con predicciones\n        - model_path: Path al modelo guardado\n    \"\"\"\n    # -------------------------------------------------------------------------\n    # VALIDACIÃ“N DEL TARGET\n    # -------------------------------------------------------------------------\n    if target not in CHEMICAL_TARGETS:\n        raise ValueError(\n            f\"Target '{target}' no vÃ¡lido.\\n\"\n            f\"Debe ser uno de: {CHEMICAL_TARGETS}\"\n        )\n    \n    # Usar hiperparÃ¡metros por defecto si no se especifican\n    n_estimators = n_estimators or DEFAULT_HYPERPARAMS['n_estimators']\n    max_depth = max_depth or DEFAULT_HYPERPARAMS['max_depth']\n    learning_rate = learning_rate or DEFAULT_HYPERPARAMS['learning_rate']\n    test_size = test_size or DEFAULT_HYPERPARAMS['test_size']\n    random_state = random_state or DEFAULT_HYPERPARAMS['random_state']\n    \n    # Obtener nombre limpio del elemento\n    element_name = target.replace('target_', '').upper()\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"ENTRENAMIENTO DE MODELO QUÃMICO - {element_name}\")\n    print(f\"{'='*60}\")\n    print(f\"Target: {target}\")\n    print(f\"Modelo: {MODEL_DISPLAY_NAMES.get(model_type, model_type)}\")\n    \n    # Mostrar especificaciÃ³n si existe\n    if target in CHEMICAL_SPECS:\n        min_spec, max_spec = CHEMICAL_SPECS[target]\n        print(f\"EspecificaciÃ³n: [{min_spec:.3f}, {max_spec:.3f}]\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 1: Cargar datos\n    # -------------------------------------------------------------------------\n    logger.info(\"Cargando datos quÃ­micos...\")\n    df = load_chemical_data()\n    \n    # Verificar que el target existe\n    if target not in df.columns:\n        raise KeyError(\n            f\"La columna target '{target}' no existe en el dataset.\\n\"\n            f\"Columnas disponibles: {[c for c in df.columns if 'target' in c]}\"\n        )\n    \n    print(f\"\\nDataset cargado: {df.shape}\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 2: SelecciÃ³n inteligente de features\n    # -------------------------------------------------------------------------\n    if feature_list is None:\n        feature_cols = get_chemical_features(df, target)\n    else:\n        # Aplicar exclusiones a lista personalizada\n        initial_feature = target.replace('target_', '')\n        feature_cols = [\n            f for f in feature_list\n            if f in df.columns and f != target and f != initial_feature\n        ]\n    \n    X = df[feature_cols].copy()\n    y = df[target].copy()\n    \n    print(f\"Features seleccionadas: {len(feature_cols)}\")\n    print(f\"  (Excluido '{target.replace('target_', '')}' para evitar data leakage)\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 3: Eliminar filas donde el target es NaN\n    # -------------------------------------------------------------------------\n    rows_initial = len(X)\n    mask_not_null = y.notnull()\n    X = X[mask_not_null]\n    y = y[mask_not_null]\n    rows_after_null = len(X)\n    \n    if rows_after_null < rows_initial:\n        print(f\"Eliminadas {rows_initial - rows_after_null} filas con target NaN\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 4: CAPPING DE OUTLIERS (CRÃTICO)\n    # Elimina el 1% inferior y 1% superior para evitar RÂ² negativos\n    # -------------------------------------------------------------------------\n    if len(y) > 100:\n        lower_q, upper_q = outlier_quantiles\n        lower_bound = y.quantile(lower_q)\n        upper_bound = y.quantile(upper_q)\n        \n        outlier_mask = (y >= lower_bound) & (y <= upper_bound)\n        n_outliers = (~outlier_mask).sum()\n        \n        X = X[outlier_mask]\n        y = y[outlier_mask]\n        \n        print(f\"Capping de outliers ({lower_q*100:.0f}%-{upper_q*100:.0f}%): eliminados {n_outliers} outliers\")\n        print(f\"  Rango despuÃ©s de capping: [{lower_bound:.4f}, {upper_bound:.4f}]\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 5: ImputaciÃ³n de NaNs en features\n    # -------------------------------------------------------------------------\n    X = X.fillna(0)\n    \n    # Limpiar valores infinitos\n    X = X.replace([np.inf, -np.inf], 0)\n    y = y.replace([np.inf, -np.inf], np.nan).dropna()\n    X = X.loc[y.index]  # Sincronizar Ã­ndices\n    \n    if len(X) == 0:\n        raise ValueError(f\"El dataset para '{target}' quedÃ³ vacÃ­o despuÃ©s de la limpieza.\")\n    \n    print(f\"\\nDataset final: {len(X)} muestras, {len(feature_cols)} features\")\n    print(f\"EstadÃ­sticas del target: min={y.min():.4f}, max={y.max():.4f}, mean={y.mean():.4f}\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 6: Split Train/Test\n    # -------------------------------------------------------------------------\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state\n    )\n    \n    print(f\"\\nSplit Train/Test:\")\n    print(f\"  - Train: {len(X_train)} muestras\")\n    print(f\"  - Test: {len(X_test)} muestras\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 7: Crear y entrenar modelo\n    # -------------------------------------------------------------------------\n    logger.info(f\"Entrenando modelo: {MODEL_DISPLAY_NAMES.get(model_type, model_type)}\")\n    \n    if model_type == 'linear':\n        model = LinearRegression()\n    elif model_type == 'random_forest':\n        model = RandomForestRegressor(\n            n_estimators=n_estimators,\n            max_depth=max_depth,\n            random_state=random_state,\n            n_jobs=-1\n        )\n    elif model_type == 'xgboost':\n        model = XGBRegressor(\n            n_estimators=n_estimators,\n            max_depth=max_depth,\n            learning_rate=learning_rate,\n            random_state=random_state,\n            n_jobs=-1\n        )\n    else:\n        raise ValueError(f\"Modelo no reconocido: {model_type}\")\n    \n    print(f\"\\nEntrenando {MODEL_DISPLAY_NAMES.get(model_type, model_type)}...\")\n    model.fit(X_train, y_train)\n    print(\"âœ… Entrenamiento completado\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 8: Predecir y evaluar\n    # -------------------------------------------------------------------------\n    y_pred = model.predict(X_test)\n    metrics = calculate_metrics(y_test, y_pred)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"MÃ‰TRICAS DE EVALUACIÃ“N - {element_name}\")\n    print(f\"{'='*60}\")\n    print(f\"  RMSE: {metrics['RMSE']:.6f}\")\n    print(f\"  RÂ²:   {metrics['R2']:.4f}\")\n    print(f\"  MAE:  {metrics['MAE']:.6f}\")\n    \n    # VerificaciÃ³n de calidad de RÂ²\n    if metrics['R2'] < 0:\n        print(f\"  âš ï¸  ADVERTENCIA: RÂ² negativo. Modelo peor que la media.\")\n    elif metrics['R2'] < 0.3:\n        print(f\"  âš ï¸  RÂ² bajo. Considera ajustar hiperparÃ¡metros.\")\n    \n    print(f\"{'='*60}\")\n    \n    # -------------------------------------------------------------------------\n    # PASO 9: Guardar modelo\n    # -------------------------------------------------------------------------\n    model_path = None\n    if save_model:\n        import pickle\n        from datetime import datetime\n        \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        element_short = target.replace('target_', '')\n        model_name = f\"chem_{element_short}_{model_type}_{timestamp}\"\n        \n        # Crear subdirectorio\n        model_subdir = MODELS_DIR / model_name\n        model_subdir.mkdir(exist_ok=True)\n        \n        model_path = model_subdir / \"model.joblib\"\n        metadata_path = model_subdir / \"metadata.json\"\n        \n        # Guardar modelo\n        joblib.dump(model, model_path)\n        \n        # Guardar metadatos\n        metadata = {\n            \"model_type\": model_type,\n            \"model_display_name\": MODEL_DISPLAY_NAMES.get(model_type, model_type),\n            \"features\": feature_cols,\n            \"hyperparameters\": {\n                \"n_estimators\": n_estimators,\n                \"max_depth\": max_depth,\n                \"learning_rate\": learning_rate,\n                \"test_size\": test_size,\n                \"random_state\": random_state\n            },\n            \"metrics\": metrics,\n            \"timestamp\": timestamp,\n            \"target\": target,\n            \"element\": element_short,\n            \"n_samples_train\": len(X_train),\n            \"n_samples_test\": len(X_test),\n            \"outlier_quantiles\": list(outlier_quantiles)\n        }\n        \n        if target in CHEMICAL_SPECS:\n            metadata[\"specification\"] = {\n                \"min\": CHEMICAL_SPECS[target][0],\n                \"max\": CHEMICAL_SPECS[target][1]\n            }\n        \n        with open(metadata_path, 'w') as f:\n            json.dump(metadata, f, indent=4)\n        \n        # Guardar tambiÃ©n en chemical_results para compatibilidad con dashboard\n        chemical_results_dir = CHEMICAL_RESULTS_DIR\n        chemical_results_dir.mkdir(parents=True, exist_ok=True)\n        \n        importance_df = get_feature_importance(model, feature_cols, model_type)\n        if importance_df is not None:\n            results_data = {\n                'y_test': y_test,\n                'y_pred': y_pred,\n                'importance_df': importance_df,\n                'metrics': metrics\n            }\n            results_file = chemical_results_dir / f\"results_{element_short}.pkl\"\n            with open(results_file, 'wb') as f:\n                pickle.dump(results_data, f)\n        \n        print(f\"\\nðŸ’¾ Modelo guardado en: {model_subdir}\")\n    \n    return model, metrics, feature_cols, X_test, y_test, y_pred, model_path\n\n\nprint(\"âœ… FunciÃ³n principal definida: train_chemical_model()\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.3 VisualizaciÃ³n EspecÃ­fica para QuÃ­mica\n\nFunciones de visualizaciÃ³n que incluyen las especificaciones quÃ­micas de calidad.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# VISUALIZACIÃ“N ESPECÃFICA PARA QUÃMICA\n# =============================================================================\n\ndef plot_chemical_predictions(\n    y_test: np.ndarray,\n    y_pred: np.ndarray,\n    metrics: Dict[str, float],\n    target: str\n) -> plt.Figure:\n    \"\"\"\n    Genera scatter plot con especificaciones quÃ­micas marcadas.\n    \n    Args:\n        y_test: Valores reales\n        y_pred: Valores predichos\n        metrics: Diccionario con mÃ©tricas\n        target: Nombre del target (ej: 'target_valc')\n    \n    Returns:\n        Figure de matplotlib\n    \"\"\"\n    element_name = target.replace('target_', '').upper()\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Scatter plot\n    ax.scatter(y_test, y_pred, alpha=0.5, color='steelblue', edgecolors='white', linewidth=0.5)\n    \n    # LÃ­nea de predicciÃ³n perfecta\n    min_val = min(np.min(y_test), np.min(y_pred))\n    max_val = max(np.max(y_test), np.max(y_pred))\n    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='PredicciÃ³n Perfecta')\n    \n    # Marcar especificaciones si existen\n    if target in CHEMICAL_SPECS:\n        min_spec, max_spec = CHEMICAL_SPECS[target]\n        ax.axhline(y=min_spec, color='green', linestyle=':', alpha=0.7, label=f'Spec Min: {min_spec}')\n        ax.axhline(y=max_spec, color='green', linestyle=':', alpha=0.7, label=f'Spec Max: {max_spec}')\n        ax.axvline(x=min_spec, color='green', linestyle=':', alpha=0.7)\n        ax.axvline(x=max_spec, color='green', linestyle=':', alpha=0.7)\n        \n        # Zona de especificaciÃ³n\n        ax.fill_between([min_spec, max_spec], min_spec, max_spec, \n                        color='green', alpha=0.1, label='Zona Ã“ptima')\n    \n    ax.set_xlabel(f'Valor Real - {element_name} (%)', fontsize=12)\n    ax.set_ylabel(f'Valor Predicho - {element_name} (%)', fontsize=12)\n    ax.set_title(f'PredicciÃ³n vs Real - {element_name}', fontsize=14, fontweight='bold')\n    \n    # MÃ©tricas en recuadro\n    textstr = f\"RMSE: {metrics['RMSE']:.6f}\\nRÂ²: {metrics['R2']:.4f}\\nMAE: {metrics['MAE']:.6f}\"\n    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n            verticalalignment='top', bbox=props)\n    \n    ax.legend(loc='lower right', fontsize=9)\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    return fig\n\n\ndef plot_chemical_summary(results_dict: Dict[str, Dict]) -> plt.Figure:\n    \"\"\"\n    Genera un grÃ¡fico resumen de todos los modelos quÃ­micos entrenados.\n    \n    Args:\n        results_dict: Diccionario con resultados por target\n                     {target: {'metrics': {...}, 'model': ...}, ...}\n    \n    Returns:\n        Figure de matplotlib\n    \"\"\"\n    targets = list(results_dict.keys())\n    r2_values = [results_dict[t]['metrics']['R2'] for t in targets]\n    rmse_values = [results_dict[t]['metrics']['RMSE'] for t in targets]\n    \n    # Limpiar nombres\n    labels = [t.replace('target_', '').upper() for t in targets]\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n    \n    # Subplot 1: RÂ² por elemento\n    ax1 = axes[0]\n    colors = ['green' if r2 > 0.5 else 'orange' if r2 > 0 else 'red' for r2 in r2_values]\n    bars1 = ax1.bar(labels, r2_values, color=colors, edgecolor='white')\n    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n    ax1.axhline(y=0.5, color='green', linestyle='--', alpha=0.5, label='Umbral bueno (0.5)')\n    ax1.set_ylabel('RÂ²', fontsize=12)\n    ax1.set_title('Coeficiente de DeterminaciÃ³n (RÂ²) por Elemento', fontsize=12)\n    ax1.set_ylim(-0.5, 1.0)\n    ax1.legend()\n    \n    # AÃ±adir valores en barras\n    for bar, val in zip(bars1, r2_values):\n        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n                f'{val:.3f}', ha='center', fontsize=9)\n    \n    # Subplot 2: RMSE por elemento\n    ax2 = axes[1]\n    bars2 = ax2.bar(labels, rmse_values, color='steelblue', edgecolor='white')\n    ax2.set_ylabel('RMSE', fontsize=12)\n    ax2.set_title('Error CuadrÃ¡tico Medio (RMSE) por Elemento', fontsize=12)\n    \n    # AÃ±adir valores en barras\n    for bar, val in zip(bars2, rmse_values):\n        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n                f'{val:.4f}', ha='center', fontsize=9, rotation=45)\n    \n    plt.suptitle('Resumen de Modelos QuÃ­micos', fontsize=14, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    return fig\n\n\nprint(\"âœ… Funciones de visualizaciÃ³n quÃ­mica definidas:\")\nprint(\"  - plot_chemical_predictions(): Scatter con especificaciones\")\nprint(\"  - plot_chemical_summary(): Resumen de todos los modelos\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.4 EjecuciÃ³n: Entrenamiento de Modelos QuÃ­micos\n\nEntrenamos modelos para los principales elementos quÃ­micos.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# ENTRENAMIENTO DE MODELOS QUÃMICOS\n# =============================================================================\n\n# Seleccionar elementos a entrenar (principales 5 elementos)\n# Puedes cambiar esta lista para incluir mÃ¡s o menos elementos\nTARGETS_TO_TRAIN = [\n    'target_valc',    # Carbono\n    'target_valmn',   # Manganeso\n    'target_valsi',   # Silicio\n    'target_valp',    # FÃ³sforo\n    'target_vals',    # Azufre\n]\n\n# Diccionario para almacenar resultados\nchemical_results = {}\n\nprint(f\"{'='*60}\")\nprint(\"ENTRENAMIENTO DE MODELOS QUÃMICOS\")\nprint(f\"{'='*60}\")\nprint(f\"Elementos a entrenar: {len(TARGETS_TO_TRAIN)}\")\nprint(f\"Targets: {[t.replace('target_', '').upper() for t in TARGETS_TO_TRAIN]}\")\nprint(f\"{'='*60}\\n\")\n\n# Entrenar modelo para cada elemento\nfor target in TARGETS_TO_TRAIN:\n    try:\n        model, metrics, features, X_test, y_test, y_pred, path = train_chemical_model(\n            target=target,\n            model_type='xgboost',\n            n_estimators=100,\n            max_depth=6,\n            learning_rate=0.1,\n            save_model=True,\n            outlier_quantiles=(0.01, 0.99)  # Eliminar 1% extremos\n        )\n        \n        # Guardar resultados\n        chemical_results[target] = {\n            'model': model,\n            'metrics': metrics,\n            'features': features,\n            'X_test': X_test,\n            'y_test': y_test,\n            'y_pred': y_pred,\n            'path': path\n        }\n        \n    except Exception as e:\n        print(f\"\\nâŒ Error entrenando {target}: {e}\\n\")\n        continue\n\nprint(f\"\\n{'='*60}\")\nprint(f\"RESUMEN: {len(chemical_results)}/{len(TARGETS_TO_TRAIN)} modelos entrenados exitosamente\")\nprint(f\"{'='*60}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# VISUALIZACIÃ“N DE RESULTADOS QUÃMICOS\n# =============================================================================\n\n# Mostrar grÃ¡fico resumen de todos los modelos\nif chemical_results:\n    fig_summary = plot_chemical_summary(chemical_results)\n    plt.show()\n    \n    # Tabla resumen de mÃ©tricas\n    print(f\"\\n{'='*70}\")\n    print(\"TABLA RESUMEN DE MÃ‰TRICAS POR ELEMENTO\")\n    print(f\"{'='*70}\")\n    print(f\"{'Elemento':<12} {'RMSE':>12} {'RÂ²':>12} {'MAE':>12} {'Spec Min':>10} {'Spec Max':>10}\")\n    print(\"-\" * 70)\n    \n    for target, result in chemical_results.items():\n        element = target.replace('target_', '').upper()\n        metrics = result['metrics']\n        \n        if target in CHEMICAL_SPECS:\n            min_spec, max_spec = CHEMICAL_SPECS[target]\n            print(f\"{element:<12} {metrics['RMSE']:>12.6f} {metrics['R2']:>12.4f} {metrics['MAE']:>12.6f} {min_spec:>10.3f} {max_spec:>10.3f}\")\n        else:\n            print(f\"{element:<12} {metrics['RMSE']:>12.6f} {metrics['R2']:>12.4f} {metrics['MAE']:>12.6f} {'N/A':>10} {'N/A':>10}\")\n    \n    print(f\"{'='*70}\")\nelse:\n    print(\"âš ï¸ No hay resultados para mostrar\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# GRÃFICOS INDIVIDUALES POR ELEMENTO\n# =============================================================================\n\n# Mostrar grÃ¡fico de predicciÃ³n para cada elemento entrenado\nfor target, result in chemical_results.items():\n    fig = plot_chemical_predictions(\n        result['y_test'],\n        result['y_pred'],\n        result['metrics'],\n        target\n    )\n    plt.show()\n    \n    # Mostrar feature importance para este modelo\n    fig_imp = plot_feature_importance(\n        result['model'],\n        result['features'],\n        model_type='xgboost',\n        top_n=10,\n        title=f\"Importancia de Variables - {target.replace('target_', '').upper()}\"\n    )\n    if fig_imp:\n        plt.show()\n    \n    print(\"-\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Fin de PARTE 5\n\nLos modelos de composiciÃ³n quÃ­mica han sido entrenados con las siguientes estrategias:\n\n| Estrategia | DescripciÃ³n |\n|------------|-------------|\n| **ExclusiÃ³n de features** | Se excluye el valor inicial del mismo elemento para evitar data leakage |\n| **Capping de outliers** | Se eliminan el 1% inferior y 1% superior (cuantiles 0.01-0.99) |\n| **ImputaciÃ³n** | NaNs en features â†’ 0, filas con target NaN â†’ eliminadas |\n\n**Archivos generados por elemento:**\n- `models/chem_{elemento}_xgboost_TIMESTAMP/model.joblib`\n- `models/chem_{elemento}_xgboost_TIMESTAMP/metadata.json`\n- `models/chemical_results/results_{elemento}.pkl`\n\n---\n\n## Fin del Notebook\n\nEl notebook EAF completo incluye:\n- **PARTE 1**: ConfiguraciÃ³n del entorno y constantes\n- **PARTE 2**: Descarga de datos desde Kaggle\n- **PARTE 3**: Feature engineering y creaciÃ³n del dataset maestro\n- **PARTE 4**: Modelado de temperatura\n- **PARTE 5**: Modelado de composiciÃ³n quÃ­mica\n\nTodos los modelos estÃ¡n guardados en el directorio `models/` y listos para ser usados en el dashboard o para inferencia.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}