{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto EAF - Notebook Monol√≠tico\n",
    "\n",
    "## Electric Arc Furnace - Predicci√≥n de Temperatura y Composici√≥n Qu√≠mica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn xgboost joblib kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import logging\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import kagglehub\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar logging para que imprima en la salida del notebook con formato de tiempo\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    force=True  # Forzar reconfiguraci√≥n si ya existe\n",
    ")\n",
    "\n",
    "# Crear logger principal del notebook\n",
    "logger = logging.getLogger('EAF_Notebook')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Configuraci√≥n de Directorios\n",
    "\n",
    "Definimos la estructura de carpetas del proyecto y las creamos autom√°ticamente si no existen:\n",
    "- `data/raw`: Datos crudos descargados de Kaggle\n",
    "- `data/processed`: Datos procesados listos para entrenamiento\n",
    "- `models`: Modelos de temperatura entrenados\n",
    "- `models/chemical_results`: Modelos qu√≠micos y sus m√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURACI√ìN DE DIRECTORIOS\n",
    "# =============================================================================\n",
    "\n",
    "# Definir ra√≠z del proyecto usando pathlib\n",
    "# Usamos Path.cwd() para notebooks, asumiendo que se ejecuta desde la ra√≠z del proyecto\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# Si el notebook est√° en una subcarpeta, ajustar:\n",
    "# PROJECT_ROOT = Path.cwd().parent  # Descomentar si es necesario\n",
    "\n",
    "# Definir estructura de directorios\n",
    "DIRECTORIES = {\n",
    "    'DATA_RAW': PROJECT_ROOT / 'data' / 'raw',\n",
    "    'DATA_PROCESSED': PROJECT_ROOT / 'data' / 'processed',\n",
    "    'MODELS': PROJECT_ROOT / 'models',\n",
    "    'CHEMICAL_RESULTS': PROJECT_ROOT / 'models' / 'chemical_results'\n",
    "}\n",
    "\n",
    "# Crear directorios si no existen\n",
    "for dir_name, dir_path in DIRECTORIES.items():\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    logger.info(f\"Directorio verificado/creado: {dir_name} -> {dir_path}\")\n",
    "\n",
    "# Asignar a variables individuales para f√°cil acceso\n",
    "DATA_RAW = DIRECTORIES['DATA_RAW']\n",
    "DATA_PROCESSED = DIRECTORIES['DATA_PROCESSED']\n",
    "MODELS_DIR = DIRECTORIES['MODELS']\n",
    "CHEMICAL_RESULTS_DIR = DIRECTORIES['CHEMICAL_RESULTS']\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ESTRUCTURA DE DIRECTORIOS DEL PROYECTO\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"PROJECT_ROOT:      {PROJECT_ROOT}\")\n",
    "print(f\"DATA_RAW:          {DATA_RAW}\")\n",
    "print(f\"DATA_PROCESSED:    {DATA_PROCESSED}\")\n",
    "print(f\"MODELS_DIR:        {MODELS_DIR}\")\n",
    "print(f\"CHEMICAL_RESULTS:  {CHEMICAL_RESULTS_DIR}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PARTE 2: Ingesta de Datos\n",
    "\n",
    "En esta secci√≥n implementamos la descarga autom√°tica del dataset desde Kaggle.\n",
    "\n",
    "El dataset **\"Industrial Data from the Arc Furnace\"** contiene 11 archivos CSV con informaci√≥n del proceso de fundici√≥n:\n",
    "- Datos del transformador y temperatura\n",
    "- Mediciones qu√≠micas iniciales y finales\n",
    "- Materiales cargados e inyectados\n",
    "- Datos del horno de cuchara (Ladle Furnace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Configuraci√≥n de Descarga y Archivos Esperados\n",
    "\n",
    "Definimos los archivos que componen el dataset completo y la referencia al dataset de Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_DATASET = \"yuriykatser/industrial-data-from-the-arc-furnace\"\n",
    "\n",
    "# ARCHIVOS ESPERADOS DEL DATASET\n",
    "ARCHIVOS_ESPERADOS = [\n",
    "    \"eaf_transformer.csv\",              # Datos del transformador del horno\n",
    "    \"basket_charged.csv\",               # Cestas de chatarra cargadas\n",
    "    \"eaf_temp.csv\",                      # Mediciones de temperatura\n",
    "    \"eaf_final_chemical_measurements.csv\",  # Composici√≥n qu√≠mica final\n",
    "    \"eaf_added_materials.csv\",           # Materiales a√±adidos al horno\n",
    "    \"inj_mat.csv\",                       # Materiales inyectados\n",
    "    \"eaf_gaslance_mat.csv\",              # Gases inyectados por lanza\n",
    "    \"lf_initial_chemical_measurements.csv\",  # Qu√≠mica inicial (horno cuchara)\n",
    "    \"ladle_tapping.csv\",                 # Datos de colada\n",
    "    \"lf_added_materials.csv\",            # Materiales a√±adidos en LF\n",
    "    \"ferro.csv\"                          # Ferroaleaciones utilizadas\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Funci√≥n de Descarga de Datos\n",
    "\n",
    "Implementamos `download_data()` con la siguiente l√≥gica:\n",
    "1. Verifica si todos los archivos ya existen en `data/raw`\n",
    "2. Si existen y `force=False`, no descarga (evita trabajo innecesario)\n",
    "3. Si faltan archivos, descarga desde Kaggle usando `kagglehub`\n",
    "4. Copia los archivos al directorio del proyecto\n",
    "5. Reporta el estado de cada archivo con su tama√±o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faltan_datos = any(not (DATA_RAW / f).exists() for f in ARCHIVOS_ESPERADOS)\n",
    "\n",
    "print(f\"‚¨áÔ∏è Descargando {KAGGLE_DATASET}...\")\n",
    "try:\n",
    "    # Descarga a cach√© de Kaggle\n",
    "    cached_path = Path(kagglehub.dataset_download(KAGGLE_DATASET))\n",
    "    DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Mover archivos a nuestra carpeta raw\n",
    "    for archivo in ARCHIVOS_ESPERADOS:\n",
    "        shutil.copy2(cached_path / archivo, DATA_RAW / archivo)\n",
    "    print(\"‚úÖ Descarga y copia completada.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante la descarga: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"üìÇ Ruta de datos: {DATA_RAW}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones Auxiliares de Preprocesamiento\n",
    "\n",
    "Las siguientes funciones se utilizan tanto para el dataset secuencial como para extraer variables est√°ticas por colada:\n",
    "\n",
    "- `load_standardized()`: Carga CSVs con conversi√≥n autom√°tica de formato europeo\n",
    "- `aggregate_*()`: Funciones de agregaci√≥n de series temporales\n",
    "- `pivot_materials()`: Pivotado de materiales a√±adidos\n",
    "- `build_master_dataset()`: Construcci√≥n del dataset maestro de variables est√°ticas\n",
    "\n",
    "Estas funciones son **reutilizadas** por la PARTE 3 para fusionar variables est√°ticas con el dataset secuencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funci√≥n de Carga Estandarizada\n",
    "\n",
    "Esta funci√≥n es fundamental: carga CSVs y convierte autom√°ticamente formatos num√©ricos europeos (coma decimal) a formato est√°ndar (punto decimal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNCI√ìN DE CARGA ESTANDARIZADA\n",
    "# =============================================================================\n",
    "\n",
    "def load_standardized(filepath: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carga un CSV y estandariza los nombres de columnas.\n",
    "    \n",
    "    IMPORTANTE: Detecta y convierte autom√°ticamente formatos num√©ricos europeos\n",
    "    donde se usa coma como separador decimal (ej: \"12,5\" -> 12.5).\n",
    "    \n",
    "    Args:\n",
    "        filepath: Ruta al archivo CSV (Path o string)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con:\n",
    "        - Columnas en min√∫sculas y sin espacios\n",
    "        - Valores num√©ricos con formato decimal est√°ndar (punto)\n",
    "    \n",
    "    Example:\n",
    "        >>> df = load_standardized(DATA_RAW / \"eaf_temp.csv\")\n",
    "        >>> df.columns  # ['heatid', 'temp', 'datetime', ...]\n",
    "    \"\"\"\n",
    "    # Cargar CSV\n",
    "    df = pd.read_csv(filepath, low_memory=False)\n",
    "    \n",
    "    # Estandarizar nombres de columnas: min√∫sculas y sin espacios\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # CONVERSI√ìN DE COMAS DECIMALES (formato europeo -> americano)\n",
    "    # Detecta columnas tipo object que contienen patrones como \"12,5\" o \"-3,14\"\n",
    "    # ---------------------------------------------------------------------\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        # Patr√≥n: n√∫mero opcional negativo, d√≠gitos, coma, d√≠gitos\n",
    "        # Ejemplos v√°lidos: \"12,5\", \"-3,14\", \"0,001\"\n",
    "        if df[col].astype(str).str.match(r'^-?\\d+,\\d+$').any():\n",
    "            df[col] = df[col].astype(str).str.replace(',', '.', regex=False)\n",
    "            logger.debug(f\"  Convertida coma decimal en columna: {col}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Test de la funci√≥n\n",
    "print(\"‚úÖ Funci√≥n load_standardized() definida\")\n",
    "print(\"\\nCaracter√≠sticas:\")\n",
    "print(\"  - Convierte nombres de columnas a min√∫sculas\")\n",
    "print(\"  - Elimina espacios en nombres de columnas\")\n",
    "print(\"  - Detecta y convierte comas decimales europeas a puntos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones de Agregaci√≥n de Series Temporales\n",
    "\n",
    "Los datos originales son series temporales (m√∫ltiples registros por colada). Estas funciones agregan los datos para obtener **un valor por colada (heatid)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNCIONES DE AGREGACI√ìN DE SERIES TEMPORALES\n",
    "# =============================================================================\n",
    "\n",
    "def aggregate_gas_data(df_gas: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega los datos de gas lance por colada.\n",
    "    \n",
    "    Obtiene el √öLTIMO valor registrado temporalmente (por revtime) de cada colada.\n",
    "    Esto representa el estado final de O2 y gas inyectados.\n",
    "    \n",
    "    Args:\n",
    "        df_gas: DataFrame con datos de eaf_gaslance_mat.csv\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame indexado por heatid con columnas:\n",
    "        - total_o2_lance: √öltimo valor de O2 inyectado\n",
    "        - total_gas_lance: √öltimo valor de gas inyectado\n",
    "    \"\"\"\n",
    "    df = df_gas.copy()\n",
    "    \n",
    "    # Convertir columnas a num√©rico\n",
    "    cols_gas = ['o2_amount', 'gas_amount']\n",
    "    for col in cols_gas:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Convertir tiempo (formato original: \"2016-01-01 18:31:46,003\")\n",
    "    # Nota: La coma en milisegundos debe convertirse a punto\n",
    "    df['revtime'] = pd.to_datetime(\n",
    "        df['revtime'].astype(str).str.replace(',', '.', regex=False),\n",
    "        format='%Y-%m-%d %H:%M:%S.%f',\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Ordenar por tiempo y obtener el √öLTIMO registro por colada\n",
    "    df = df.sort_values('revtime')\n",
    "    grp_gas = df.groupby('heatid').last()[cols_gas].rename(columns={\n",
    "        'o2_amount': 'total_o2_lance',\n",
    "        'gas_amount': 'total_gas_lance'\n",
    "    })\n",
    "    \n",
    "    return grp_gas\n",
    "\n",
    "\n",
    "def aggregate_injection_data(df_inj: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega los datos de inyecciones de carb√≥n por colada.\n",
    "    \n",
    "    Obtiene el √öLTIMO valor registrado temporalmente (por revtime) de cada colada.\n",
    "    \n",
    "    Args:\n",
    "        df_inj: DataFrame con datos de inj_mat.csv\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame indexado por heatid con columna:\n",
    "        - total_injected_carbon: √öltimo valor de carb√≥n inyectado\n",
    "    \"\"\"\n",
    "    df = df_inj.copy()\n",
    "    \n",
    "    # Convertir a num√©rico\n",
    "    df['inj_amount_carbon'] = pd.to_numeric(df['inj_amount_carbon'], errors='coerce')\n",
    "    \n",
    "    # Convertir tiempo (formato: \"2016-01-01 18:31:46,003\")\n",
    "    df['revtime'] = pd.to_datetime(\n",
    "        df['revtime'].astype(str).str.replace(',', '.', regex=False),\n",
    "        format='%Y-%m-%d %H:%M:%S.%f',\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    # Ordenar por tiempo y obtener el √öLTIMO registro por colada\n",
    "    df = df.sort_values('revtime')\n",
    "    grp_inj = df.groupby('heatid').last()[['inj_amount_carbon']].rename(\n",
    "        columns={'inj_amount_carbon': 'total_injected_carbon'}\n",
    "    )\n",
    "    \n",
    "    return grp_inj\n",
    "\n",
    "\n",
    "def aggregate_transformer_data(df_transformer: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega los datos del transformador por colada.\n",
    "    \n",
    "    Calcula la ENERG√çA TOTAL consumida: MW * Duraci√≥n (en minutos)\n",
    "    y la duraci√≥n total del proceso.\n",
    "    \n",
    "    Args:\n",
    "        df_transformer: DataFrame con datos de eaf_transformer.csv\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame indexado por heatid con columnas:\n",
    "        - total_energy: Suma de (MW * duraci√≥n) para toda la colada\n",
    "        - total_duration: Duraci√≥n total en minutos\n",
    "    \"\"\"\n",
    "    df = df_transformer.copy()\n",
    "    \n",
    "    # Funci√≥n para parsear DURATION de formato \"MM: SS\" a minutos decimales\n",
    "    def parse_duration(duration_str):\n",
    "        \"\"\"Convierte 'MM: SS' a minutos decimales.\"\"\"\n",
    "        try:\n",
    "            duration_str = str(duration_str).strip()\n",
    "            parts = duration_str.split(':')\n",
    "            if len(parts) == 2:\n",
    "                minutes = float(parts[0].strip())\n",
    "                seconds = float(parts[1].strip())\n",
    "                return minutes + seconds / 60.0\n",
    "            return 0.0\n",
    "        except (ValueError, AttributeError):\n",
    "            return 0.0\n",
    "    \n",
    "    # Aplicar conversi√≥n de duraci√≥n\n",
    "    df['duration_minutes'] = df['duration'].apply(parse_duration)\n",
    "    \n",
    "    # Convertir MW a num√©rico\n",
    "    df['mw'] = pd.to_numeric(df['mw'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Calcular energ√≠a = MW * duraci√≥n (en minutos)\n",
    "    df['energy'] = df['mw'] * df['duration_minutes']\n",
    "    \n",
    "    # Agregar por colada: SUMA de energ√≠a y duraci√≥n\n",
    "    grp_transformer = df.groupby('heatid').agg({\n",
    "        'energy': 'sum',\n",
    "        'duration_minutes': 'sum'\n",
    "    }).rename(columns={\n",
    "        'energy': 'total_energy',\n",
    "        'duration_minutes': 'total_duration'\n",
    "    })\n",
    "    \n",
    "    return grp_transformer\n",
    "\n",
    "\n",
    "def aggregate_charged_amount(df_ladle: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega la cantidad total de material cargado por colada.\n",
    "    \n",
    "    Args:\n",
    "        df_ladle: DataFrame con datos de ladle_tapping.csv\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame indexado por heatid con columna:\n",
    "        - total_charged_amount: Suma total de carga\n",
    "    \"\"\"\n",
    "    df = df_ladle.copy()\n",
    "    \n",
    "    # Convertir a num√©rico\n",
    "    df['charge_amount'] = pd.to_numeric(df['charge_amount'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Agregar: SUMA por colada\n",
    "    grp_charged = df.groupby('heatid').agg({\n",
    "        'charge_amount': 'sum'\n",
    "    }).rename(columns={'charge_amount': 'total_charged_amount'})\n",
    "    \n",
    "    return grp_charged\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funciones de agregaci√≥n definidas:\")\n",
    "print(\"  - aggregate_gas_data(): O2 y gas inyectado (√∫ltimo valor)\")\n",
    "print(\"  - aggregate_injection_data(): Carb√≥n inyectado (√∫ltimo valor)\")\n",
    "print(\"  - aggregate_transformer_data(): Energ√≠a total (MW * tiempo)\")\n",
    "print(\"  - aggregate_charged_amount(): Carga total (suma)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funci√≥n de Pivotado de Materiales\n",
    "\n",
    "Esta funci√≥n transforma la tabla de materiales a√±adidos en columnas individuales.\n",
    "Selecciona los **top N materiales m√°s frecuentes** y crea una columna por cada uno (`added_mat_XXXXXX`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNCI√ìN DE PIVOTADO DE MATERIALES\n",
    "# =============================================================================\n",
    "\n",
    "def pivot_materials(df_ladle: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pivota los materiales agregados, seleccionando los top_n m√°s frecuentes.\n",
    "    \n",
    "    Transforma una tabla con m√∫ltiples filas por colada (una por material)\n",
    "    en una tabla con UNA fila por colada y UNA columna por material.\n",
    "    \n",
    "    Args:\n",
    "        df_ladle: DataFrame con datos de ladle_tapping.csv\n",
    "                  Debe contener columnas: heatid, mat_code, charge_amount\n",
    "        top_n: N√∫mero de materiales m√°s frecuentes a incluir (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame pivotado indexado por heatid con columnas:\n",
    "        - added_mat_XXXXXX: Cantidad de material con c√≥digo XXXXXX\n",
    "        \n",
    "    Example:\n",
    "        >>> pivot = pivot_materials(df_ladle, top_n=10)\n",
    "        >>> pivot.columns\n",
    "        Index(['added_mat_140107', 'added_mat_202007', ...])\n",
    "    \"\"\"\n",
    "    df = df_ladle.copy()\n",
    "    \n",
    "    # Convertir cantidad a num√©rico\n",
    "    df['charge_amount'] = pd.to_numeric(df['charge_amount'], errors='coerce')\n",
    "    \n",
    "    # Seleccionar los TOP N materiales por frecuencia de uso\n",
    "    top_materials = df['mat_code'].value_counts().head(top_n).index\n",
    "    logger.info(f\"Top {top_n} materiales seleccionados: {list(top_materials)}\")\n",
    "    \n",
    "    # Filtrar solo los materiales m√°s frecuentes\n",
    "    df_filtered = df[df['mat_code'].isin(top_materials)]\n",
    "    \n",
    "    # Crear pivot table\n",
    "    # - index: heatid (una fila por colada)\n",
    "    # - columns: mat_code (una columna por material)\n",
    "    # - values: charge_amount (suma de cantidades)\n",
    "    # - fill_value: 0 (si no se us√≥ el material, es 0)\n",
    "    pivot_ladle = df_filtered.pivot_table(\n",
    "        index='heatid',\n",
    "        columns='mat_code',\n",
    "        values='charge_amount',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    ).add_prefix('added_mat_')\n",
    "    \n",
    "    logger.info(f\"Pivot de materiales: {pivot_ladle.shape[0]} coladas, {pivot_ladle.shape[1]} materiales\")\n",
    "    \n",
    "    return pivot_ladle\n",
    "\n",
    "\n",
    "def get_datetime_range(df_ladle: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extrae el rango de fechas (inicio y fin) de cada colada.\n",
    "    \n",
    "    √ötil para an√°lisis temporal y filtrado por per√≠odo.\n",
    "    \n",
    "    Args:\n",
    "        df_ladle: DataFrame con datos de ladle_tapping.csv\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con columnas:\n",
    "        - heatid: Identificador de colada\n",
    "        - fecha_inicio: Primera fecha registrada\n",
    "        - fecha_fin: √öltima fecha registrada\n",
    "    \"\"\"\n",
    "    df = df_ladle.copy()\n",
    "    \n",
    "    # Convertir a datetime\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    \n",
    "    # Agregar: m√≠nimo y m√°ximo por colada\n",
    "    datetime_range = df.groupby('heatid').agg({\n",
    "        'datetime': ['min', 'max']\n",
    "    })\n",
    "    datetime_range.columns = ['fecha_inicio', 'fecha_fin']\n",
    "    datetime_range = datetime_range.reset_index()\n",
    "    \n",
    "    return datetime_range\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funciones de pivotado definidas:\")\n",
    "print(\"  - pivot_materials(): Crea columnas por material (top N)\")\n",
    "print(\"  - get_datetime_range(): Extrae rango de fechas por colada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones de Extracci√≥n de Targets\n",
    "\n",
    "Estas funciones extraen las variables objetivo (targets) que queremos predecir:\n",
    "- **Temperatura final**: La √∫ltima medici√≥n de temperatura antes del vaciado\n",
    "- **Composici√≥n qu√≠mica final**: Valores de C, Mn, Si, P, S, Cu, Cr, Mo, Ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNCIONES DE EXTRACCI√ìN DE TARGETS\n",
    "# =============================================================================\n",
    "\n",
    "def get_final_temperature(df_temp: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtiene la temperatura final (al vaciado) de cada colada.\n",
    "    \n",
    "    Toma la √öLTIMA medici√≥n de temperatura registrada temporalmente,\n",
    "    que corresponde al momento del vaciado del horno.\n",
    "    \n",
    "    Args:\n",
    "        df_temp: DataFrame con datos de eaf_temp.csv\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con columnas:\n",
    "        - heatid: Identificador de colada\n",
    "        - target_temperature: Temperatura final en ¬∞C\n",
    "    \"\"\"\n",
    "    df = df_temp.copy()\n",
    "    \n",
    "    # Detectar columnas autom√°ticamente\n",
    "    cols_temp = [c for c in df.columns if 'temp' in c and 'time' not in c]\n",
    "    cols_time = [c for c in df.columns if 'time' in c or 'date' in c]\n",
    "    \n",
    "    col_temp_name = cols_temp[0] if cols_temp else 'temp'\n",
    "    col_time_name = cols_time[0] if cols_time else 'datetime'\n",
    "    \n",
    "    logger.info(f\"Columna de temperatura detectada: {col_temp_name}\")\n",
    "    logger.info(f\"Columna de tiempo detectada: {col_time_name}\")\n",
    "    \n",
    "    # Limpiar tipos\n",
    "    df[col_temp_name] = pd.to_numeric(df[col_temp_name], errors='coerce')\n",
    "    df[col_time_name] = pd.to_datetime(df[col_time_name], errors='coerce')\n",
    "    \n",
    "    # Obtener la √öLTIMA medici√≥n (temperatura al vaciado)\n",
    "    # Ordenar por tiempo y tomar el √∫ltimo registro de cada colada\n",
    "    df_target = df.sort_values(col_time_name).groupby('heatid').tail(1)\n",
    "    \n",
    "    # Seleccionar solo ID y temperatura\n",
    "    df_target = df_target[['heatid', col_temp_name]].rename(\n",
    "        columns={col_temp_name: 'target_temperature'}\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Temperaturas extra√≠das: {len(df_target)} coladas\")\n",
    "    \n",
    "    return df_target\n",
    "\n",
    "\n",
    "def get_final_chemical_composition(df_chem_final: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtiene la composici√≥n qu√≠mica final de cada colada.\n",
    "    \n",
    "    Extrae los valores finales de los elementos qu√≠micos relevantes\n",
    "    para el control de calidad del acero.\n",
    "    \n",
    "    Args:\n",
    "        df_chem_final: DataFrame con datos de eaf_final_chemical_measurements.csv\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con columnas:\n",
    "        - heatid: Identificador de colada\n",
    "        - target_valc: Carbono final (%)\n",
    "        - target_valmn: Manganeso final (%)\n",
    "        - target_valsi: Silicio final (%)\n",
    "        - target_valp: F√≥sforo final (%)\n",
    "        - target_vals: Azufre final (%)\n",
    "        - target_valcu: Cobre final (%)\n",
    "        - target_valcr: Cromo final (%)\n",
    "        - target_valmo: Molibdeno final (%)\n",
    "        - target_valni: N√≠quel final (%)\n",
    "    \"\"\"\n",
    "    # Elementos qu√≠micos a extraer como targets (lista completa)\n",
    "    chemical_elements = [\n",
    "        'valc', 'valmn', 'valsi', 'valp', 'vals',\n",
    "        'valcu', 'valcr', 'valmo', 'valni'\n",
    "    ]\n",
    "    \n",
    "    # Verificar qu√© columnas existen realmente en el archivo\n",
    "    available_elements = [col for col in chemical_elements if col in df_chem_final.columns]\n",
    "    \n",
    "    if not available_elements:\n",
    "        logger.warning(\"No se encontraron columnas de elementos qu√≠micos en el archivo\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    logger.info(f\"Elementos qu√≠micos disponibles: {available_elements}\")\n",
    "    \n",
    "    # Seleccionar heatid y elementos qu√≠micos\n",
    "    cols_to_select = ['heatid'] + available_elements\n",
    "    df_targets = df_chem_final[cols_to_select].copy()\n",
    "    \n",
    "    # Convertir a num√©rico (maneja tanto puntos como comas)\n",
    "    for col in available_elements:\n",
    "        df_targets[col] = pd.to_numeric(df_targets[col], errors='coerce')\n",
    "    \n",
    "    # Renombrar con prefijo target_\n",
    "    rename_dict = {col: f'target_{col}' for col in available_elements}\n",
    "    df_targets = df_targets.rename(columns=rename_dict)\n",
    "    \n",
    "    # Eliminar duplicados por heatid (tomar el √∫ltimo registro)\n",
    "    df_targets = df_targets.drop_duplicates(subset=['heatid'], keep='last')\n",
    "    \n",
    "    logger.info(f\"Targets qu√≠micos extra√≠dos: {len(df_targets)} coladas, {len(available_elements)} elementos\")\n",
    "    \n",
    "    return df_targets\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funciones de targets definidas:\")\n",
    "print(\"  - get_final_temperature(): √öltima temperatura por colada\")\n",
    "print(\"  - get_final_chemical_composition(): Composici√≥n qu√≠mica final (9 elementos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construcci√≥n del Dataset Maestro\n",
    "\n",
    "Estas funciones combinan todas las fuentes de datos en un √∫nico dataset maestro, realizando los merges necesarios y la limpieza final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONSTRUCCI√ìN DEL DATASET MAESTRO\n",
    "# =============================================================================\n",
    "\n",
    "def _filter_outliers_by_target(df: pd.DataFrame, target: str, lower_q: float = 0.01, upper_q: float = 0.99) -> Tuple[pd.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Filtra outliers en el target especificado usando cuantiles (rows drop).\n",
    "    Retorna el DataFrame filtrado y el n√∫mero de filas eliminadas.\n",
    "    \"\"\"\n",
    "    if target not in df.columns or len(df) < 100:\n",
    "        return df, 0\n",
    "\n",
    "    try:\n",
    "        y = df[target].dropna()\n",
    "        if len(y) < 100:\n",
    "            return df, 0\n",
    "\n",
    "        lower_bound = y.quantile(lower_q)\n",
    "        upper_bound = y.quantile(upper_q)\n",
    "\n",
    "        # M√°scara para conservar valores dentro del rango\n",
    "        mask = (df[target] >= lower_bound) & (df[target] <= upper_bound)\n",
    "\n",
    "        # Se mantiene la fila si no es un outlier O si es NaN (ser√° imputado/eliminado despu√©s)\n",
    "        rows_to_keep = df[target].isnull() | mask\n",
    "\n",
    "        n_removed = (~rows_to_keep).sum()\n",
    "        df_filtered = df[rows_to_keep]\n",
    "\n",
    "        return df_filtered, n_removed\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error al filtrar outliers en {target}: {e}\")\n",
    "        return df, 0\n",
    "\n",
    "\n",
    "def build_master_dataset(raw_data_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construye el dataset maestro combinando todas las fuentes de datos.\n",
    "\n",
    "    Este es el N√öCLEO del feature engineering. Carga todos los archivos,\n",
    "    aplica las agregaciones y fusiona todo en un √∫nico DataFrame.\n",
    "\n",
    "    Args:\n",
    "        raw_data_dir: Ruta al directorio con los datos raw\n",
    "\n",
    "    Returns:\n",
    "        DataFrame maestro con todas las features de input (sin targets)\n",
    "\n",
    "    Pipeline:\n",
    "        1. Cargar archivos CSV estandarizados\n",
    "        2. Agregar series temporales (gas, inyecci√≥n, transformador, carga)\n",
    "        3. Pivotar materiales\n",
    "        4. Extraer rango de fechas\n",
    "        5. Fusionar todo por heatid\n",
    "        6. Rellenar nulos t√©cnicos con 0\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(\"CONSTRUYENDO DATASET MAESTRO\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 1: Cargar archivos necesarios\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"Cargando archivos...\")\n",
    "\n",
    "    df_gas = load_standardized(raw_data_dir / \"eaf_gaslance_mat.csv\")\n",
    "    logger.info(f\"  - eaf_gaslance_mat.csv: {df_gas.shape}\")\n",
    "\n",
    "    df_inj = load_standardized(raw_data_dir / \"inj_mat.csv\")\n",
    "    logger.info(f\"  - inj_mat.csv: {df_inj.shape}\")\n",
    "\n",
    "    df_ladle = load_standardized(raw_data_dir / \"ladle_tapping.csv\")\n",
    "    logger.info(f\"  - ladle_tapping.csv: {df_ladle.shape}\")\n",
    "\n",
    "    df_chem_initial = load_standardized(raw_data_dir / \"lf_initial_chemical_measurements.csv\")\n",
    "    logger.info(f\"  - lf_initial_chemical_measurements.csv: {df_chem_initial.shape}\")\n",
    "\n",
    "    df_transformer = load_standardized(raw_data_dir / \"eaf_transformer.csv\")\n",
    "    logger.info(f\"  - eaf_transformer.csv: {df_transformer.shape}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 2: Agregar series temporales\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"\\nAgregando series temporales...\")\n",
    "\n",
    "    grp_gas = aggregate_gas_data(df_gas)\n",
    "    logger.info(f\"  - Gases: {grp_gas.shape[0]} coladas\")\n",
    "\n",
    "    grp_inj = aggregate_injection_data(df_inj)\n",
    "    logger.info(f\"  - Inyecciones: {grp_inj.shape[0]} coladas\")\n",
    "\n",
    "    grp_transformer = aggregate_transformer_data(df_transformer)\n",
    "    logger.info(f\"  - Transformador: {grp_transformer.shape[0]} coladas\")\n",
    "\n",
    "    grp_charged = aggregate_charged_amount(df_ladle)\n",
    "    logger.info(f\"  - Carga: {grp_charged.shape[0]} coladas\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 3: Pivotar materiales\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"\\nPivotando materiales...\")\n",
    "    pivot_ladle = pivot_materials(df_ladle, top_n=10)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 4: Extraer rango de fechas\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"\\nExtrayendo rango de fechas...\")\n",
    "    datetime_range = get_datetime_range(df_ladle)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 5: Fusionar dataset maestro\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"\\nFusionando dataset maestro...\")\n",
    "\n",
    "    # Dataset base: mediciones qu√≠micas iniciales\n",
    "    df_master = df_chem_initial.copy()\n",
    "    logger.info(f\"  Base (qu√≠mica inicial): {df_master.shape}\")\n",
    "\n",
    "    # Merges (left joins para preservar todos los registros base)\n",
    "    df_master = df_master.merge(grp_gas, on='heatid', how='left')\n",
    "    df_master = df_master.merge(grp_inj, on='heatid', how='left')\n",
    "    df_master = df_master.merge(grp_transformer, on='heatid', how='left')\n",
    "    df_master = df_master.merge(grp_charged, on='heatid', how='left')\n",
    "    df_master = df_master.merge(pivot_ladle, on='heatid', how='left')\n",
    "    df_master = df_master.merge(datetime_range, on='heatid', how='left')\n",
    "\n",
    "    logger.info(f\"  Despu√©s de merges: {df_master.shape}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 6: Rellenar nulos t√©cnicos\n",
    "    # -------------------------------------------------------------------------\n",
    "    cols_to_fix = [\n",
    "        'total_o2_lance', 'total_gas_lance', 'total_injected_carbon',\n",
    "        'total_energy', 'total_duration', 'total_charged_amount'\n",
    "    ]\n",
    "    # Solo rellenar las columnas que existen\n",
    "    cols_to_fix = [c for c in cols_to_fix if c in df_master.columns]\n",
    "    df_master[cols_to_fix] = df_master[cols_to_fix].fillna(0)\n",
    "\n",
    "    # Rellenar columnas de materiales con 0\n",
    "    mat_cols = [c for c in df_master.columns if c.startswith('added_mat_')]\n",
    "    df_master[mat_cols] = df_master[mat_cols].fillna(0)\n",
    "\n",
    "    logger.info(f\"\\nDataset maestro (inputs): {df_master.shape}\")\n",
    "    logger.info(f\"  - Filas: {len(df_master)}\")\n",
    "    logger.info(f\"  - Columnas: {len(df_master.columns)}\")\n",
    "\n",
    "    return df_master\n",
    "\n",
    "\n",
    "def add_target_temperature(df_master: pd.DataFrame, raw_data_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega la variable target de temperatura al dataset maestro.\n",
    "\n",
    "    Args:\n",
    "        df_master: DataFrame con inputs\n",
    "        raw_data_dir: Ruta al directorio con los datos raw\n",
    "\n",
    "    Returns:\n",
    "        DataFrame final con inputs y target_temperature\n",
    "    \"\"\"\n",
    "    logger.info(\"\\nAgregando target de temperatura...\")\n",
    "\n",
    "    # Cargar y extraer temperatura final\n",
    "    df_temp = load_standardized(raw_data_dir / \"eaf_temp.csv\")\n",
    "    df_target = get_final_temperature(df_temp)\n",
    "\n",
    "    # Merge (inner join - solo coladas con datos completos)\n",
    "    df_final = df_master.merge(df_target, on='heatid', how='inner')\n",
    "\n",
    "    # Limpieza: eliminar columnas innecesarias\n",
    "    cols_drop = ['datetime', 'positionrow', 'filter_key_date', 'measure_time']\n",
    "    df_final = df_final.drop(columns=[c for c in cols_drop if c in df_final.columns])\n",
    "\n",
    "    # Eliminar filas donde el target es nulo\n",
    "    df_final = df_final.dropna(subset=['target_temperature'])\n",
    "\n",
    "    # Rellenar nulos restantes en inputs con 0\n",
    "    df_final = df_final.fillna(0)\n",
    "\n",
    "    logger.info(f\"Dataset con temperatura: {df_final.shape}\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def add_target_chemical(df_master: pd.DataFrame, raw_data_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Agrega las variables target de composici√≥n qu√≠mica al dataset maestro.\n",
    "\n",
    "    Args:\n",
    "        df_master: DataFrame con inputs\n",
    "        raw_data_dir: Ruta al directorio con los datos raw\n",
    "\n",
    "    Returns:\n",
    "        DataFrame final con inputs y targets qu√≠micos\n",
    "    \"\"\"\n",
    "    logger.info(\"\\nAgregando targets de composici√≥n qu√≠mica...\")\n",
    "\n",
    "    # Cargar y extraer composici√≥n qu√≠mica final\n",
    "    df_chem_final = load_standardized(raw_data_dir / \"eaf_final_chemical_measurements.csv\")\n",
    "    df_targets = get_final_chemical_composition(df_chem_final)\n",
    "\n",
    "    if df_targets.empty:\n",
    "        raise ValueError(\"No se pudieron extraer los targets qu√≠micos\")\n",
    "\n",
    "    # Merge (inner join - solo coladas con datos completos)\n",
    "    df_final = df_master.merge(df_targets, on='heatid', how='inner')\n",
    "\n",
    "    # Limpieza: eliminar columnas innecesarias\n",
    "    cols_drop = ['datetime', 'positionrow', 'filter_key_date', 'measure_time']\n",
    "    df_final = df_final.drop(columns=[c for c in cols_drop if c in df_final.columns])\n",
    "\n",
    "    # Eliminar filas donde TODOS los targets son nulos\n",
    "    target_cols = [c for c in df_final.columns if c.startswith('target_')]\n",
    "    df_final = df_final.dropna(subset=target_cols, how='all')\n",
    "\n",
    "    # <<<<<<<<<<<<<<<< CAMBIO: Aplicar Filtro de Outliers a Nivel de Dataset Maestro >>>>>>>>>>>>>>>>>>\n",
    "    print(\"\\n[PREPROCESAMIENTO QU√çMICO] Aplicando filtro de outliers (1% inferior/superior) a targets:\")\n",
    "    rows_initial = len(df_final)\n",
    "\n",
    "    # Iterar sobre todos los targets y eliminar la fila si el valor es un outlier en CUALQUIER target\n",
    "    for target in target_cols:\n",
    "        df_final, n_removed = _filter_outliers_by_target(\n",
    "            df_final,\n",
    "            target,\n",
    "            lower_q=0.01,\n",
    "            upper_q=0.99\n",
    "        )\n",
    "        if n_removed > 0:\n",
    "            print(f\"  - Eliminadas {n_removed} filas por outlier en {target} ({len(df_final)} restantes)\")\n",
    "\n",
    "    total_removed = rows_initial - len(df_final)\n",
    "    if total_removed > 0:\n",
    "        print(f\"Total de filas eliminadas por outliers en cualquier target: {total_removed}\")\n",
    "\n",
    "    # Rellenar nulos restantes con 0\n",
    "    df_final = df_final.fillna(0)\n",
    "\n",
    "    logger.info(f\"Dataset con qu√≠mica: {df_final.shape}\")\n",
    "    logger.info(f\"Targets: {target_cols}\")\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funciones de construcci√≥n definidas:\")\n",
    "print(\"  - build_master_dataset(): Crea dataset de inputs\")\n",
    "print(\"  - add_target_temperature(): Agrega target de temperatura\")\n",
    "print(\"  - add_target_chemical(): Agrega targets qu√≠micos (con filtrado de outliers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PARTE 3: Feature Engineering - Dataset Secuencial\n",
    "\n",
    "Esta secci√≥n genera el **dataset secuencial** para predicci√≥n paso a paso de temperatura.\n",
    "\n",
    "**Pipeline de transformaci√≥n:**\n",
    "1. **Carga de datos**: Mediciones temporales de temperatura (eaf_temp.csv)\n",
    "2. **Limpieza robusta**: Filtrado de temperaturas f√≠sicamente v√°lidas (1000-1850¬∞C) y eliminaci√≥n de outliers por cuantiles\n",
    "3. **Ordenamiento temporal**: Por HEATID y DATETIME\n",
    "4. **Features din√°micas**: Temperatura actual, oxidaci√≥n, posici√≥n en secuencia\n",
    "5. **Target secuencial**: Temperatura del siguiente registro (shift)\n",
    "6. **Fusi√≥n con variables est√°ticas**: Merge con dataset maestro (energ√≠a, materiales, etc.)\n",
    "7. **Exportaci√≥n**: `dataset_sequential_temp.csv`\n",
    "\n",
    "**Nota sobre funciones auxiliares:** Las funciones `load_standardized()`, `build_master_dataset()` y otras funciones de agregaci√≥n definidas anteriormente se reutilizan aqu√≠ para obtener las variables est√°ticas de cada colada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Carga y Limpieza de Datos de Temperatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 1: CARGA Y LIMPIEZA DE DATOS DE TEMPERATURA\n",
    "# =============================================================================\n",
    "\n",
    "# Cargar el archivo de temperaturas con todas las mediciones\n",
    "df_temp_seq = load_standardized(DATA_RAW / \"eaf_temp.csv\")\n",
    "\n",
    "print(f\"Archivo eaf_temp.csv cargado\")\n",
    "print(f\"Shape original: {df_temp_seq.shape}\")\n",
    "print(f\"\\nColumnas disponibles:\")\n",
    "print(df_temp_seq.columns.tolist())\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df_temp_seq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DETECCI√ìN AUTOM√ÅTICA DE COLUMNAS Y LIMPIEZA DE TIPOS\n",
    "# =============================================================================\n",
    "\n",
    "# Detectar columna de temperatura (excluir 'time' en el nombre)\n",
    "cols_temp = [c for c in df_temp_seq.columns if 'temp' in c.lower() and 'time' not in c.lower()]\n",
    "col_temp = cols_temp[0] if cols_temp else 'temp'\n",
    "print(f\"Columna de temperatura detectada: {col_temp}\")\n",
    "\n",
    "# Detectar columna de tiempo/fecha\n",
    "cols_time = [c for c in df_temp_seq.columns if 'time' in c.lower() or 'date' in c.lower()]\n",
    "col_datetime = cols_time[0] if cols_time else 'datetime'\n",
    "print(f\"Columna de tiempo detectada: {col_datetime}\")\n",
    "\n",
    "# Detectar columna de oxidaci√≥n si existe\n",
    "cols_ox = [c for c in df_temp_seq.columns if 'ox' in c.lower() or 'o2' in c.lower()]\n",
    "col_oxidation = cols_ox[0] if cols_ox else None\n",
    "print(f\"Columna de oxidaci√≥n detectada: {col_oxidation}\")\n",
    "\n",
    "# Convertir tipos de datos\n",
    "# Temperatura a num√©rico\n",
    "df_temp_seq[col_temp] = pd.to_numeric(df_temp_seq[col_temp], errors='coerce')\n",
    "\n",
    "# Datetime - manejar formato con coma en milisegundos\n",
    "df_temp_seq[col_datetime] = pd.to_datetime(\n",
    "    df_temp_seq[col_datetime].astype(str).str.replace(',', '.', regex=False),\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Oxidaci√≥n a num√©rico si existe\n",
    "if col_oxidation:\n",
    "    df_temp_seq[col_oxidation] = pd.to_numeric(df_temp_seq[col_oxidation], errors='coerce')\n",
    "\n",
    "# Mostrar tipos resultantes\n",
    "print(f\"\\nTipos de datos despu√©s de conversi√≥n:\")\n",
    "print(df_temp_seq.dtypes)\n",
    "\n",
    "# Estad√≠sticas b√°sicas\n",
    "print(f\"\\nEstad√≠sticas de temperatura:\")\n",
    "print(df_temp_seq[col_temp].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Limpieza Robusta de Datos de Temperatura\n",
    "\n",
    "**CR√çTICO**: Antes de generar features y targets, aplicamos filtros para eliminar ruido del sensor:\n",
    "\n",
    "1. **Filtro f√≠sico**: Solo temperaturas entre 1000¬∞C y 1850¬∞C (rango operativo del EAF)\n",
    "2. **Filtro estad√≠stico**: Eliminaci√≥n de cuantiles extremos (0.5% inferior y superior)\n",
    "\n",
    "Este paso previene que outliers del sensor contaminen el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 2B: LIMPIEZA ROBUSTA DE DATOS DE TEMPERATURA (CR√çTICO)\n",
    "# =============================================================================\n",
    "# Aplicar filtros ANTES de generar lags/shifts para evitar contaminaci√≥n\n",
    "\n",
    "filas_inicial = len(df_temp_seq)\n",
    "print(f\"Filas iniciales: {filas_inicial}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FILTRO 1: Rango f√≠sico de temperaturas v√°lidas para EAF\n",
    "# El horno de arco el√©ctrico opera t√≠picamente entre 1000¬∞C y 1850¬∞C\n",
    "# Valores fuera de este rango son errores del sensor\n",
    "# -----------------------------------------------------------------------------\n",
    "TEMP_MIN_FISICA = 1000  # ¬∞C - Por debajo de esto, el acero no est√° fundido\n",
    "TEMP_MAX_FISICA = 1850  # ¬∞C - Por encima de esto, da√±o al equipo\n",
    "\n",
    "mask_fisica = (df_temp_seq[col_temp] >= TEMP_MIN_FISICA) & (df_temp_seq[col_temp] <= TEMP_MAX_FISICA)\n",
    "df_temp_seq = df_temp_seq[mask_fisica].copy()\n",
    "\n",
    "filas_despues_fisica = len(df_temp_seq)\n",
    "eliminadas_fisica = filas_inicial - filas_despues_fisica\n",
    "print(f\"Filtro f√≠sico ({TEMP_MIN_FISICA}-{TEMP_MAX_FISICA}¬∞C): {eliminadas_fisica} filas eliminadas\")\n",
    "print(f\"  -> Filas restantes: {filas_despues_fisica}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FILTRO 2: Cuantiles para eliminar ruido extremo del sensor\n",
    "# Eliminamos el 0.5% inferior y superior de las temperaturas restantes\n",
    "# -----------------------------------------------------------------------------\n",
    "QUANTILE_LOWER = 0.005\n",
    "QUANTILE_UPPER = 0.995\n",
    "\n",
    "q_low = df_temp_seq[col_temp].quantile(QUANTILE_LOWER)\n",
    "q_high = df_temp_seq[col_temp].quantile(QUANTILE_UPPER)\n",
    "\n",
    "mask_quantile = (df_temp_seq[col_temp] >= q_low) & (df_temp_seq[col_temp] <= q_high)\n",
    "df_temp_seq = df_temp_seq[mask_quantile].copy()\n",
    "\n",
    "filas_despues_quantile = len(df_temp_seq)\n",
    "eliminadas_quantile = filas_despues_fisica - filas_despues_quantile\n",
    "print(f\"Filtro cuantil ({QUANTILE_LOWER:.1%}-{QUANTILE_UPPER:.1%}): {eliminadas_quantile} filas eliminadas\")\n",
    "print(f\"  -> Rango aceptado: [{q_low:.1f}, {q_high:.1f}] ¬∞C\")\n",
    "print(f\"  -> Filas restantes: {filas_despues_quantile}\")\n",
    "\n",
    "# Resumen de limpieza\n",
    "total_eliminadas = filas_inicial - filas_despues_quantile\n",
    "pct_eliminado = 100 * total_eliminadas / filas_inicial\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESUMEN DE LIMPIEZA DE DATOS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Filas iniciales:      {filas_inicial:,}\")\n",
    "print(f\"Filas eliminadas:     {total_eliminadas:,} ({pct_eliminado:.2f}%)\")\n",
    "print(f\"Filas finales:        {filas_despues_quantile:,}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Estad√≠sticas de temperatura despu√©s de limpieza\n",
    "print(f\"\\nEstad√≠sticas de temperatura DESPU√âS de limpieza:\")\n",
    "print(df_temp_seq[col_temp].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Ordenamiento Estricto por HEATID y DATETIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 2: ORDENAMIENTO ESTRICTO POR HEATID Y DATETIME\n",
    "# =============================================================================\n",
    "# CR√çTICO: Sin ordenamiento correcto, la secuencia temporal no tiene sentido\n",
    "\n",
    "# Eliminar filas con valores nulos en columnas cr√≠ticas\n",
    "df_temp_seq = df_temp_seq.dropna(subset=['heatid', col_datetime, col_temp])\n",
    "print(f\"Filas despu√©s de eliminar nulos cr√≠ticos: {len(df_temp_seq)}\")\n",
    "\n",
    "# Ordenar OBLIGATORIAMENTE por heatid (colada) y luego por datetime (tiempo)\n",
    "df_temp_seq = df_temp_seq.sort_values(['heatid', col_datetime]).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataset ordenado por heatid y {col_datetime}\")\n",
    "print(f\"Shape: {df_temp_seq.shape}\")\n",
    "\n",
    "# Verificar ordenamiento mostrando una colada de ejemplo\n",
    "ejemplo_heatid = df_temp_seq['heatid'].iloc[0]\n",
    "print(f\"\\nEjemplo de colada ordenada (heatid={ejemplo_heatid}):\")\n",
    "df_temp_seq[df_temp_seq['heatid'] == ejemplo_heatid][['heatid', col_datetime, col_temp]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Creaci√≥n de Features Din√°micas ($X_t$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 3: CREACI√ìN DE FEATURES DIN√ÅMICAS (X_t)\n",
    "# =============================================================================\n",
    "# Estas son las features que representan el estado actual en cada momento t\n",
    "\n",
    "# Feature 1: Temperatura actual (X_t)\n",
    "df_temp_seq['temp_actual'] = df_temp_seq[col_temp]\n",
    "\n",
    "# Feature 2: Oxidaci√≥n actual (si existe, sino rellenar con 0)\n",
    "if col_oxidation:\n",
    "    df_temp_seq['oxidacion_actual'] = df_temp_seq[col_oxidation].fillna(0)\n",
    "else:\n",
    "    df_temp_seq['oxidacion_actual'] = 0\n",
    "    print(\"Nota: No se encontr√≥ columna de oxidaci√≥n, se rellena con 0\")\n",
    "\n",
    "# Feature 3: N√∫mero de medici√≥n dentro de la colada (posici√≥n secuencial)\n",
    "df_temp_seq['num_medicion'] = df_temp_seq.groupby('heatid').cumcount() + 1\n",
    "\n",
    "# Feature 4: Tiempo transcurrido desde inicio de la colada (en minutos)\n",
    "df_temp_seq['tiempo_desde_inicio'] = df_temp_seq.groupby('heatid')[col_datetime].transform(\n",
    "    lambda x: (x - x.min()).dt.total_seconds() / 60\n",
    ")\n",
    "\n",
    "print(\"Features din√°micas creadas:\")\n",
    "print(\"  - temp_actual: Temperatura en el momento actual\")\n",
    "print(\"  - oxidacion_actual: Nivel de oxidaci√≥n actual\")\n",
    "print(\"  - num_medicion: N√∫mero de medici√≥n dentro de la colada\")\n",
    "print(\"  - tiempo_desde_inicio: Minutos desde inicio de la colada\")\n",
    "\n",
    "# Mostrar ejemplo\n",
    "print(f\"\\nEjemplo de features para colada {ejemplo_heatid}:\")\n",
    "df_temp_seq[df_temp_seq['heatid'] == ejemplo_heatid][[\n",
    "    'heatid', col_datetime, 'temp_actual', 'oxidacion_actual', 'num_medicion', 'tiempo_desde_inicio'\n",
    "]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Generaci√≥n del Target ($Y_t$) - Temperatura Siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 4: GENERACI√ìN DEL TARGET (Y_t) - TEMPERATURA SIGUIENTE\n",
    "# =============================================================================\n",
    "# El target es la temperatura del SIGUIENTE registro dentro de la misma colada\n",
    "# Usamos shift(-1) agrupado por heatid\n",
    "\n",
    "# Target: Temperatura del siguiente momento temporal (shift negativo de 1)\n",
    "df_temp_seq['target_temp_next'] = df_temp_seq.groupby('heatid')[col_temp].shift(-1)\n",
    "\n",
    "# Tambi√©n guardamos el datetime del siguiente registro (para calcular horizonte)\n",
    "df_temp_seq['datetime_next'] = df_temp_seq.groupby('heatid')[col_datetime].shift(-1)\n",
    "\n",
    "print(\"Target generado: target_temp_next\")\n",
    "print(\"  - Representa la temperatura del siguiente registro de la misma colada\")\n",
    "print(\"  - El √∫ltimo registro de cada colada tendr√° NaN (ser√° eliminado)\")\n",
    "\n",
    "# Verificar: mostrar ejemplo con target\n",
    "print(f\"\\nEjemplo de target para colada {ejemplo_heatid}:\")\n",
    "df_temp_seq[df_temp_seq['heatid'] == ejemplo_heatid][[\n",
    "    'heatid', col_datetime, 'temp_actual', 'target_temp_next', 'datetime_next'\n",
    "]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 C√°lculo del Horizonte Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 5: C√ÅLCULO DEL HORIZONTE TEMPORAL\n",
    "# =============================================================================\n",
    "# Calculamos cu√°ntos minutos faltan para la siguiente medici√≥n\n",
    "# Esto es √∫til para que el modelo sepa a qu√© horizonte est√° prediciendo\n",
    "\n",
    "# Horizonte: diferencia en minutos entre datetime_next y datetime actual\n",
    "df_temp_seq['horizonte_minutos'] = (\n",
    "    df_temp_seq['datetime_next'] - df_temp_seq[col_datetime]\n",
    ").dt.total_seconds() / 60\n",
    "\n",
    "print(\"Horizonte temporal calculado: horizonte_minutos\")\n",
    "print(\"  - Minutos hasta la siguiente medici√≥n\")\n",
    "\n",
    "# Estad√≠sticas del horizonte\n",
    "print(f\"\\nEstad√≠sticas del horizonte temporal:\")\n",
    "print(df_temp_seq['horizonte_minutos'].describe())\n",
    "\n",
    "# Mostrar ejemplo completo\n",
    "print(f\"\\nEjemplo completo para colada {ejemplo_heatid}:\")\n",
    "df_temp_seq[df_temp_seq['heatid'] == ejemplo_heatid][[\n",
    "    'heatid', 'num_medicion', 'temp_actual', 'target_temp_next', 'horizonte_minutos'\n",
    "]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Limpieza de Bordes (Eliminar √öltimos Registros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 6: LIMPIEZA DE BORDES - ELIMINAR √öLTIMOS REGISTROS DE CADA COLADA\n",
    "# =============================================================================\n",
    "# El √∫ltimo registro de cada colada no tiene target (NaN por el shift)\n",
    "# Debemos eliminarlo sistem√°ticamente\n",
    "\n",
    "filas_antes = len(df_temp_seq)\n",
    "\n",
    "# Contar cu√°ntas coladas tenemos\n",
    "n_coladas = df_temp_seq['heatid'].nunique()\n",
    "print(f\"N√∫mero de coladas: {n_coladas}\")\n",
    "print(f\"Filas antes de limpieza: {filas_antes}\")\n",
    "\n",
    "# Eliminar filas donde target_temp_next es NaN (√∫ltimos registros de cada colada)\n",
    "df_temp_seq = df_temp_seq.dropna(subset=['target_temp_next'])\n",
    "\n",
    "filas_despues = len(df_temp_seq)\n",
    "filas_eliminadas = filas_antes - filas_despues\n",
    "\n",
    "print(f\"Filas eliminadas (√∫ltimos de cada colada): {filas_eliminadas}\")\n",
    "print(f\"Filas despu√©s de limpieza: {filas_despues}\")\n",
    "\n",
    "# Verificar que se eliminaron aproximadamente n_coladas filas\n",
    "print(f\"\\nVerificaci√≥n: Se esperaban ~{n_coladas} eliminaciones, se eliminaron {filas_eliminadas}\")\n",
    "\n",
    "# Eliminar tambi√©n filas con horizonte negativo o muy largo (outliers)\n",
    "print(f\"\\nFiltrando horizontes an√≥malos...\")\n",
    "mask_horizonte_valido = (df_temp_seq['horizonte_minutos'] > 0) & (df_temp_seq['horizonte_minutos'] < 120)\n",
    "df_temp_seq = df_temp_seq[mask_horizonte_valido]\n",
    "print(f\"Filas despu√©s de filtrar horizontes: {len(df_temp_seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Fusi√≥n con Dataset Maestro (Merge One-to-Many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 7: FUSI√ìN CON DATASET MAESTRO (ONE-TO-MANY)\n",
    "# =============================================================================\n",
    "# Fusionamos el dataset secuencial con el dataset maestro de variables est√°ticas\n",
    "# Las variables est√°ticas se REPETIR√ÅN en cada paso temporal (dise√±o esperado)\n",
    "\n",
    "# Primero, construir el dataset maestro si no existe\n",
    "print(\"Construyendo dataset maestro de variables est√°ticas...\")\n",
    "df_master = build_master_dataset(DATA_RAW)\n",
    "\n",
    "print(f\"\\nDataset maestro shape: {df_master.shape}\")\n",
    "print(f\"Dataset secuencial shape: {df_temp_seq.shape}\")\n",
    "\n",
    "# Fusionar: dataset secuencial es la BASE (izquierda)\n",
    "# Esto replica las variables est√°ticas para cada medici√≥n temporal\n",
    "df_sequential = df_temp_seq.merge(\n",
    "    df_master,\n",
    "    on='heatid',\n",
    "    how='left'  # Mantener todas las filas del dataset secuencial\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset despu√©s de fusi√≥n: {df_sequential.shape}\")\n",
    "print(f\"  - Cada paso temporal ahora tiene acceso a las variables est√°ticas de la colada\")\n",
    "\n",
    "# Verificar que las variables est√°ticas se replicaron\n",
    "print(f\"\\nEjemplo de replicaci√≥n para colada {ejemplo_heatid}:\")\n",
    "cols_ejemplo = ['heatid', 'num_medicion', 'temp_actual', 'target_temp_next', 'total_energy', 'total_o2_lance']\n",
    "cols_disponibles = [c for c in cols_ejemplo if c in df_sequential.columns]\n",
    "df_sequential[df_sequential['heatid'] == ejemplo_heatid][cols_disponibles].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Preparaci√≥n Final del Dataset Secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 8: PREPARACI√ìN FINAL DEL DATASET SECUENCIAL\n",
    "# =============================================================================\n",
    "\n",
    "# Rellenar nulos con 0 (valores t√©cnicos faltantes)\n",
    "df_sequential = df_sequential.fillna(0)\n",
    "\n",
    "# Eliminar columnas auxiliares que ya no necesitamos\n",
    "cols_drop = ['datetime_next', col_datetime, 'datetime', 'positionrow', 'filter_key_date', 'measure_time']\n",
    "cols_drop = [c for c in cols_drop if c in df_sequential.columns]\n",
    "df_sequential = df_sequential.drop(columns=cols_drop)\n",
    "\n",
    "# Eliminar tambi√©n la columna de temperatura original (ya tenemos temp_actual)\n",
    "if col_temp in df_sequential.columns and col_temp != 'temp_actual':\n",
    "    df_sequential = df_sequential.drop(columns=[col_temp])\n",
    "\n",
    "# Reorganizar columnas: heatid primero, luego features, luego target\n",
    "target_col = 'target_temp_next'\n",
    "cols_order = ['heatid'] + [c for c in df_sequential.columns if c not in ['heatid', target_col]] + [target_col]\n",
    "df_sequential = df_sequential[cols_order]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET SECUENCIAL FINAL PARA ENTRENAMIENTO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {df_sequential.shape}\")\n",
    "print(f\"\\nColumnas ({len(df_sequential.columns)}):\")\n",
    "print(df_sequential.columns.tolist())\n",
    "\n",
    "print(f\"\\nEstad√≠sticas del target (temperatura siguiente):\")\n",
    "print(df_sequential['target_temp_next'].describe())\n",
    "\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df_sequential.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GUARDAR DATASET SECUENCIAL\n",
    "# =============================================================================\n",
    "\n",
    "output_path = DATA_PROCESSED / \"dataset_sequential_temp.csv\"\n",
    "df_sequential.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset secuencial guardado en: {output_path}\")\n",
    "print(f\"  - Filas: {len(df_sequential)}\")\n",
    "print(f\"  - Columnas: {len(df_sequential.columns)}\")\n",
    "print(f\"  - Coladas √∫nicas: {df_sequential['heatid'].nunique()}\")\n",
    "print(f\"  - Mediciones promedio por colada: {len(df_sequential) / df_sequential['heatid'].nunique():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Resumen del Dataset Secuencial\n",
    "\n",
    "**Dataset generado:** `dataset_sequential_temp.csv`\n",
    "\n",
    "**Estructura:**\n",
    "- **Una fila por medici√≥n temporal** (m√∫ltiples filas por colada)\n",
    "- **Features din√°micas**: `temp_actual`, `oxidacion_actual`, `num_medicion`, `tiempo_desde_inicio`, `horizonte_minutos`\n",
    "- **Features est√°ticas**: Variables del dataset maestro (energ√≠a, materiales, etc.) replicadas en cada paso\n",
    "- **Target**: `target_temp_next` - temperatura de la siguiente medici√≥n\n",
    "\n",
    "**Uso para entrenamiento:**\n",
    "```python\n",
    "# Cargar dataset\n",
    "df = pd.read_csv(DATA_PROCESSED / \"dataset_sequential_temp.csv\")\n",
    "\n",
    "# Separar features y target\n",
    "X = df.drop(columns=['heatid', 'target_temp_next'])\n",
    "y = df['target_temp_next']\n",
    "\n",
    "# IMPORTANTE: Para split train/test, agrupar por heatid para evitar data leakage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VERIFICACI√ìN FINAL Y AN√ÅLISIS EXPLORATORIO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AN√ÅLISIS DEL DATASET SECUENCIAL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Distribuci√≥n de mediciones por colada\n",
    "mediciones_por_colada = df_sequential.groupby('heatid').size()\n",
    "print(f\"\\nMediciones por colada:\")\n",
    "print(mediciones_por_colada.describe())\n",
    "\n",
    "# Correlaci√≥n entre features din√°micas y target\n",
    "features_dinamicas = ['temp_actual', 'oxidacion_actual', 'num_medicion', 'tiempo_desde_inicio', 'horizonte_minutos']\n",
    "features_disponibles = [f for f in features_dinamicas if f in df_sequential.columns]\n",
    "\n",
    "print(f\"\\nCorrelaci√≥n de features din√°micas con target:\")\n",
    "for f in features_disponibles:\n",
    "    corr = df_sequential[f].corr(df_sequential['target_temp_next'])\n",
    "    print(f\"  {f}: {corr:.4f}\")\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Distribuci√≥n del target\n",
    "axes[0, 0].hist(df_sequential['target_temp_next'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Temperatura siguiente (¬∞C)')\n",
    "axes[0, 0].set_ylabel('Frecuencia')\n",
    "axes[0, 0].set_title('Distribuci√≥n del Target')\n",
    "\n",
    "# 2. Temp actual vs Temp siguiente\n",
    "axes[0, 1].scatter(df_sequential['temp_actual'], df_sequential['target_temp_next'], alpha=0.1, s=1)\n",
    "axes[0, 1].plot([1500, 1750], [1500, 1750], 'r--', label='y=x')\n",
    "axes[0, 1].set_xlabel('Temperatura actual (¬∞C)')\n",
    "axes[0, 1].set_ylabel('Temperatura siguiente (¬∞C)')\n",
    "axes[0, 1].set_title('Temperatura Actual vs Siguiente')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Distribuci√≥n del horizonte\n",
    "axes[1, 0].hist(df_sequential['horizonte_minutos'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Horizonte (minutos)')\n",
    "axes[1, 0].set_ylabel('Frecuencia')\n",
    "axes[1, 0].set_title('Distribuci√≥n del Horizonte Temporal')\n",
    "\n",
    "# 4. Mediciones por colada\n",
    "axes[1, 1].hist(mediciones_por_colada, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('N√∫mero de mediciones')\n",
    "axes[1, 1].set_ylabel('Frecuencia (coladas)')\n",
    "axes[1, 1].set_title('Mediciones por Colada')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDataset secuencial listo para entrenamiento de modelos de series temporales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PARTE 3.5: Feature Engineering - Dataset Qu√≠mico\n",
    "\n",
    "En esta secci√≥n generamos el dataset para predecir la **composici√≥n qu√≠mica final** del acero.\n",
    "\n",
    "**Objetivo:** Crear `dataset_final_chemical.csv` con:\n",
    "- **Inputs:** Variables del proceso (materiales, energ√≠a, gases, etc.)\n",
    "- **Targets:** Composici√≥n qu√≠mica final (C, Mn, Si, P, S, Cu, Cr, Mo, Ni)\n",
    "\n",
    "**Pipeline:**\n",
    "1. Cargar mediciones qu√≠micas finales (`eaf_final_chemical_measurements.csv`)\n",
    "2. Extraer y limpiar los targets qu√≠micos\n",
    "3. Reutilizar/generar el dataset maestro de inputs\n",
    "4. Fusionar inputs con targets qu√≠micos\n",
    "5. Limpieza final y guardado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.1 Carga de Mediciones Qu√≠micas Finales\n",
    "\n",
    "Cargamos el archivo con las mediciones de composici√≥n qu√≠mica realizadas **al final del proceso EAF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 1: CARGAR MEDICIONES QU√çMICAS FINALES\n",
    "# =============================================================================\n",
    "\n",
    "df_chem_final = load_standardized(DATA_RAW / \"eaf_final_chemical_measurements.csv\")\n",
    "\n",
    "print(f\"Archivo cargado: eaf_final_chemical_measurements.csv\")\n",
    "print(f\"Shape: {df_chem_final.shape}\")\n",
    "print(f\"\\nColumnas disponibles:\")\n",
    "print(df_chem_final.columns.tolist())\n",
    "print(f\"\\nPrimeras filas:\")\n",
    "df_chem_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Extracci√≥n de Targets Qu√≠micos\n",
    "\n",
    "Extraemos las columnas de composici√≥n qu√≠mica que servir√°n como variables target:\n",
    "- **valc**: Carbono\n",
    "- **valmn**: Manganeso\n",
    "- **valsi**: Silicio\n",
    "- **valp**: F√≥sforo\n",
    "- **vals**: Azufre\n",
    "- **valcu**: Cobre\n",
    "- **valcr**: Cromo\n",
    "- **valmo**: Molibdeno\n",
    "- **valni**: N√≠quel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 2: EXTRACCI√ìN Y LIMPIEZA DE TARGETS QU√çMICOS\n",
    "# =============================================================================\n",
    "\n",
    "# Elementos qu√≠micos a extraer como targets\n",
    "chemical_elements = [\n",
    "    'valc', 'valmn', 'valsi', 'valp', 'vals',\n",
    "    'valcu', 'valcr', 'valmo', 'valni'\n",
    "]\n",
    "\n",
    "# Verificar qu√© columnas existen en el archivo\n",
    "available_elements = [col for col in chemical_elements if col in df_chem_final.columns]\n",
    "missing_elements = [col for col in chemical_elements if col not in df_chem_final.columns]\n",
    "\n",
    "print(f\"Elementos disponibles: {available_elements}\")\n",
    "if missing_elements:\n",
    "    print(f\"Elementos NO encontrados: {missing_elements}\")\n",
    "\n",
    "# Seleccionar heatid y elementos qu√≠micos disponibles\n",
    "cols_to_select = ['heatid'] + available_elements\n",
    "df_targets = df_chem_final[cols_to_select].copy()\n",
    "\n",
    "# Convertir a num√©rico (manejo de valores mal formateados)\n",
    "for col in available_elements:\n",
    "    df_targets[col] = pd.to_numeric(df_targets[col], errors='coerce')\n",
    "\n",
    "# Renombrar con prefijo target_\n",
    "rename_dict = {col: f'target_{col}' for col in available_elements}\n",
    "df_targets = df_targets.rename(columns=rename_dict)\n",
    "\n",
    "# Eliminar duplicados por heatid (conservar el √∫ltimo registro)\n",
    "df_targets = df_targets.drop_duplicates(subset=['heatid'], keep='last')\n",
    "\n",
    "print(f\"\\nTargets extra√≠dos: {len(df_targets)} coladas\")\n",
    "print(f\"Columnas target: {[c for c in df_targets.columns if c.startswith('target_')]}\")\n",
    "print(f\"\\nEstad√≠sticas de los targets:\")\n",
    "df_targets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.3 Dataset Maestro de Inputs\n",
    "\n",
    "Reutilizamos la funci√≥n `build_master_dataset()` para generar el dataset de inputs.\n",
    "\n",
    "**Nota:** Si `df_master` ya existe en memoria (de la PARTE 3), lo reutilizamos para evitar rec√°lculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 3: GENERAR/REUTILIZAR DATASET MAESTRO DE INPUTS\n",
    "# =============================================================================\n",
    "\n",
    "# Verificar si df_master ya existe en memoria\n",
    "if 'df_master' not in dir() or df_master is None:\n",
    "    print(\"Generando dataset maestro desde cero...\")\n",
    "    df_master = build_master_dataset(DATA_RAW)\n",
    "else:\n",
    "    print(\"Reutilizando df_master existente en memoria.\")\n",
    "\n",
    "print(f\"\\nDataset maestro (inputs): {df_master.shape}\")\n",
    "print(f\"Columnas: {df_master.columns.tolist()[:10]}... (y {len(df_master.columns)-10} m√°s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 Fusi√≥n de Inputs con Targets Qu√≠micos\n",
    "\n",
    "Realizamos un **inner join** entre el dataset maestro y los targets qu√≠micos.\n",
    "\n",
    "Solo conservamos las coladas que tienen **tanto** datos de proceso **como** mediciones qu√≠micas finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 4: FUSI√ìN DE INPUTS CON TARGETS QU√çMICOS\n",
    "# =============================================================================\n",
    "\n",
    "# Merge (inner join - solo coladas con datos completos)\n",
    "df_chemical = df_master.merge(df_targets, on='heatid', how='inner')\n",
    "\n",
    "print(f\"Dataset maestro: {len(df_master)} coladas\")\n",
    "print(f\"Targets qu√≠micos: {len(df_targets)} coladas\")\n",
    "print(f\"Dataset fusionado: {len(df_chemical)} coladas\")\n",
    "print(f\"\\nP√©rdida por merge: {len(df_master) - len(df_chemical)} coladas sin mediciones qu√≠micas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.5 Limpieza Final y Guardado\n",
    "\n",
    "Aplicamos la limpieza final:\n",
    "1. Eliminar columnas innecesarias (datetime, positionrow, etc.)\n",
    "2. Eliminar filas donde **todos** los targets son nulos\n",
    "3. Rellenar nulos en inputs con 0\n",
    "4. Guardar el dataset en `data/processed/dataset_final_chemical.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 5: LIMPIEZA FINAL Y GUARDADO\n",
    "# =============================================================================\n",
    "\n",
    "# Columnas a eliminar (metadatos innecesarios para el modelo)\n",
    "cols_drop = ['datetime', 'positionrow', 'filter_key_date', 'measure_time']\n",
    "df_chemical = df_chemical.drop(columns=[c for c in cols_drop if c in df_chemical.columns])\n",
    "print(f\"Columnas eliminadas: {[c for c in cols_drop if c in df_master.columns]}\")\n",
    "\n",
    "# Identificar columnas target\n",
    "target_cols = [c for c in df_chemical.columns if c.startswith('target_')]\n",
    "print(f\"Columnas target: {target_cols}\")\n",
    "\n",
    "# Eliminar filas donde TODOS los targets son nulos\n",
    "rows_before = len(df_chemical)\n",
    "df_chemical = df_chemical.dropna(subset=target_cols, how='all')\n",
    "rows_dropped = rows_before - len(df_chemical)\n",
    "print(f\"\\nFilas eliminadas (todos targets nulos): {rows_dropped}\")\n",
    "\n",
    "# Rellenar nulos en inputs con 0\n",
    "null_count_before = df_chemical.isnull().sum().sum()\n",
    "df_chemical = df_chemical.fillna(0)\n",
    "print(f\"Valores nulos rellenados con 0: {null_count_before}\")\n",
    "\n",
    "# Guardar el dataset\n",
    "output_path_chemical = DATA_PROCESSED / \"dataset_final_chemical.csv\"\n",
    "df_chemical.to_csv(output_path_chemical, index=False)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"DATASET QU√çMICO GUARDADO EXITOSAMENTE\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"Ruta: {output_path_chemical}\")\n",
    "print(f\"Shape final: {df_chemical.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.6 An√°lisis Exploratorio del Dataset Qu√≠mico\n",
    "\n",
    "Verificaci√≥n r√°pida del dataset generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# AN√ÅLISIS EXPLORATORIO DEL DATASET QU√çMICO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESUMEN DEL DATASET QU√çMICO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Informaci√≥n general\n",
    "print(f\"\\nüìä DIMENSIONES\")\n",
    "print(f\"  - Filas (coladas): {len(df_chemical):,}\")\n",
    "print(f\"  - Columnas totales: {len(df_chemical.columns)}\")\n",
    "print(f\"  - Features de input: {len(df_chemical.columns) - len(target_cols)}\")\n",
    "print(f\"  - Variables target: {len(target_cols)}\")\n",
    "\n",
    "# Estad√≠sticas de los targets\n",
    "print(f\"\\nüéØ ESTAD√çSTICAS DE TARGETS QU√çMICOS\")\n",
    "print(df_chemical[target_cols].describe().round(4))\n",
    "\n",
    "# Verificar nulos\n",
    "print(f\"\\nüîç VERIFICACI√ìN DE NULOS\")\n",
    "null_counts = df_chemical.isnull().sum()\n",
    "if null_counts.sum() == 0:\n",
    "    print(\"  ‚úÖ No hay valores nulos en el dataset\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Columnas con nulos: {null_counts[null_counts > 0].to_dict()}\")\n",
    "\n",
    "# Distribuci√≥n de targets (valores extremos)\n",
    "print(f\"\\nüìà RANGOS DE TARGETS\")\n",
    "for col in target_cols:\n",
    "    min_val = df_chemical[col].min()\n",
    "    max_val = df_chemical[col].max()\n",
    "    mean_val = df_chemical[col].mean()\n",
    "    print(f\"  {col}: min={min_val:.4f}, max={max_val:.4f}, mean={mean_val:.4f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset qu√≠mico listo para modelado en PARTE 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PARTE 4: Modelado de Temperatura (Dataset Secuencial)\n",
    "\n",
    "En esta secci√≥n entrenamos modelos para predecir la **temperatura del siguiente paso temporal** usando el dataset secuencial generado en la PARTE 3.\n",
    "\n",
    "**Modelos disponibles:**\n",
    "- **XGBoost Regressor**: Gradient boosting optimizado\n",
    "- **Random Forest Regressor**: Ensemble de √°rboles de decisi√≥n\n",
    "- **Linear Regression**: Modelo base lineal\n",
    "\n",
    "**Pipeline de entrenamiento:**\n",
    "1. Carga del dataset secuencial (`dataset_sequential_temp.csv`)\n",
    "2. Preparaci√≥n de features din√°micas y est√°ticas\n",
    "3. **Split Train/Test por HEATID** (GroupShuffleSplit para evitar data leakage)\n",
    "4. Entrenamiento del modelo seleccionado\n",
    "5. Evaluaci√≥n con m√©tricas (RMSE, R¬≤, MAE)\n",
    "6. Visualizaci√≥n de resultados\n",
    "\n",
    "**IMPORTANTE - Validaci√≥n por Grupos:**\n",
    "Para evitar *data leakage*, usamos `GroupShuffleSplit` que garantiza que todas las mediciones de una misma colada (heatid) est√©n completamente en train o en test, nunca mezcladas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PARTE 5: Modelado de Composici√≥n Qu√≠mica\n",
    "\n",
    "La predicci√≥n qu√≠mica es m√°s delicada debido a:\n",
    "- **Outliers extremos**: Valores at√≠picos que distorsionan las m√©tricas\n",
    "- **Data Leakage potencial**: Usar el valor inicial del mismo elemento como feature\n",
    "- **M√∫ltiples targets**: 9 elementos qu√≠micos diferentes\n",
    "\n",
    "**Estrategias implementadas:**\n",
    "1. **Exclusi√≥n inteligente de features**: Se excluye el valor inicial del mismo elemento\n",
    "2. **Capping de outliers**: Se eliminan el 1% inferior y superior (cuantiles 0.01-0.99)\n",
    "3. **Imputaci√≥n de NaNs**: Features con valor 0, filas con target NaN eliminadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Configuraci√≥n de Variables para Modelado Qu√≠mico\n",
    "\n",
    "Definimos las variables espec√≠ficas para el modelado de composici√≥n qu√≠mica:\n",
    "- **CHEMICAL_TARGETS**: Targets qu√≠micos a predecir\n",
    "- **CHEMICAL_SPECS**: Rangos de especificaci√≥n para control de calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURACI√ìN PARA MODELADO QU√çMICO\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# TARGETS QU√çMICOS\n",
    "# Valores FINALES de composici√≥n qu√≠mica a predecir\n",
    "# -----------------------------------------------------------------------------\n",
    "CHEMICAL_TARGETS = [\n",
    "    'target_valc',              # Carbono final\n",
    "    'target_valmn',             # Manganeso final\n",
    "    'target_valsi',             # Silicio final\n",
    "    'target_valp',              # F√≥sforo final\n",
    "    'target_vals',              # Azufre final\n",
    "    'target_valcu',             # Cobre final\n",
    "    'target_valcr',             # Cromo final\n",
    "    'target_valmo',             # Molibdeno final\n",
    "    'target_valni'              # N√≠quel final\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ESPECIFICACIONES QU√çMICAS\n",
    "# Rangos de especificaci√≥n (min, max) para valores FINALES - Control de calidad\n",
    "# -----------------------------------------------------------------------------\n",
    "CHEMICAL_SPECS = {\n",
    "    'target_valc':  (0.05, 0.50),    # Carbono: 0.05% - 0.50%\n",
    "    'target_valmn': (0.30, 1.50),    # Manganeso: 0.30% - 1.50%\n",
    "    'target_valsi': (0.10, 0.60),    # Silicio: 0.10% - 0.60%\n",
    "    'target_valp':  (0.001, 0.025),  # F√≥sforo: 0.001% - 0.025%\n",
    "    'target_vals':  (0.001, 0.025),  # Azufre: 0.001% - 0.025%\n",
    "    'target_valcu': (0.001, 0.030),  # Cobre: 0.001% - 0.030%\n",
    "    'target_valcr': (0.001, 0.030),  # Cromo: 0.001% - 0.030%\n",
    "    'target_valmo': (0.001, 0.010),  # Molibdeno: 0.001% - 0.010%\n",
    "    'target_valni': (0.001, 0.030)   # N√≠quel: 0.001% - 0.030%\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n qu√≠mica cargada:\")\n",
    "print(f\"  - CHEMICAL_TARGETS: {len(CHEMICAL_TARGETS)} elementos\")\n",
    "print(f\"  - CHEMICAL_SPECS: {len(CHEMICAL_SPECS)} especificaciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Funciones Auxiliares para Modelos Qu√≠micos\n",
    "\n",
    "Funciones espec√≠ficas para el manejo de datos qu√≠micos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNCIONES AUXILIARES PARA MODELOS QU√çMICOS\n",
    "# =============================================================================\n",
    "\n",
    "def load_chemical_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carga el dataset espec√≠fico para modelos qu√≠micos.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con los datos qu√≠micos procesados\n",
    "    \"\"\"\n",
    "    # El archivo 'dataset_final_chemical.csv' ahora ya fue filtrado de outliers\n",
    "    return load_and_clean_data(\"dataset_final_chemical.csv\")\n",
    "\n",
    "\n",
    "def get_chemical_features(df: pd.DataFrame, target: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Obtiene la lista de features disponibles para entrenamiento qu√≠mico,\n",
    "    aplicando exclusiones inteligentes para evitar data leakage.\n",
    "\n",
    "    EXCLUSIONES:\n",
    "    1. El propio target (target_valc, etc.)\n",
    "    2. El valor inicial del mismo elemento (valc, etc.) - EVITA DATA LEAKAGE\n",
    "    3. Todos los dem√°s targets qu√≠micos\n",
    "    4. Columna de identificador (heatid)\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame con los datos\n",
    "        target: Target qu√≠mico a predecir (ej: 'target_valc')\n",
    "\n",
    "    Returns:\n",
    "        Lista de features v√°lidas para entrenamiento\n",
    "\n",
    "    Example:\n",
    "        >>> features = get_chemical_features(df, 'target_valc')\n",
    "        >>> # Excluir√° 'valc' y 'target_valc' de las features\n",
    "    \"\"\"\n",
    "    # Determinar el feature inicial correspondiente al target\n",
    "    # target_valc -> valc, target_valmn -> valmn, etc.\n",
    "    initial_feature_to_exclude = target.replace('target_', '')\n",
    "\n",
    "    # Lista de columnas a excluir\n",
    "    exclude_cols = ['heatid', target]\n",
    "\n",
    "    # Excluir TODOS los targets qu√≠micos (no solo el actual)\n",
    "    exclude_cols += [col for col in df.columns if col.startswith('target_')]\n",
    "\n",
    "    # Excluir el feature inicial del mismo elemento\n",
    "    if initial_feature_to_exclude in df.columns:\n",
    "        exclude_cols.append(initial_feature_to_exclude)\n",
    "\n",
    "    # Filtrar: usar INPUT_FEATURES que est√©n en df y NO est√©n en exclusiones\n",
    "    available = [\n",
    "        col for col in INPUT_FEATURES\n",
    "        if col in df.columns and col not in exclude_cols\n",
    "    ]\n",
    "\n",
    "    return available\n",
    "\n",
    "\n",
    "# La funci√≥n cap_outliers ha sido ELIMINADA de esta celda,\n",
    "# ya que su l√≥gica se movi√≥ a la funci√≥n add_target_chemical en la PARTE 3.5.\n",
    "\n",
    "print(\"‚úÖ Funciones auxiliares qu√≠micas definidas:\")\n",
    "print(\"  - load_chemical_data(): Carga dataset_final_chemical.csv\")\n",
    "print(\"  - get_chemical_features(): Selecci√≥n inteligente de features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Funci√≥n Principal: train_chemical_model()\n",
    "\n",
    "Entrena un modelo para predecir un elemento qu√≠mico espec√≠fico con manejo robusto de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNCI√ìN PRINCIPAL: ENTRENAMIENTO DE MODELO QU√çMICO\n",
    "# =============================================================================\n",
    "\n",
    "def train_chemical_model(\n",
    "    target: str,\n",
    "    model_type: str = 'xgboost',\n",
    "    n_estimators: int = None,\n",
    "    max_depth: int = None,\n",
    "    learning_rate: float = None,\n",
    "    test_size: float = None,\n",
    "    random_state: int = None,\n",
    "    save_model: bool = True,\n",
    "    feature_list: List[str] = None,\n",
    "    outlier_quantiles: Tuple[float, float] = (0.01, 0.99)\n",
    ") -> Tuple[Any, Dict[str, float], List[str], pd.DataFrame, pd.Series, np.ndarray, Optional[Path]]:\n",
    "    \"\"\"\n",
    "    Entrena un modelo de predicci√≥n de composici√≥n qu√≠mica FINAL.\n",
    "    \n",
    "    IMPORTANTE: Implementa limpieza de outliers para evitar R¬≤ negativos.\n",
    "    \n",
    "    Args:\n",
    "        target: Elemento qu√≠mico a predecir ('target_valc', 'target_valmn', etc.)\n",
    "        model_type: Tipo de modelo ('xgboost', 'random_forest', 'linear')\n",
    "        n_estimators: N√∫mero de estimadores para tree models\n",
    "        max_depth: Profundidad m√°xima de √°rboles\n",
    "        learning_rate: Learning rate para XGBoost\n",
    "        test_size: Proporci√≥n de datos para test\n",
    "        random_state: Semilla para reproducibilidad\n",
    "        save_model: Si True, guarda el modelo en disco\n",
    "        feature_list: Lista personalizada de features (si None, usa selecci√≥n inteligente)\n",
    "        outlier_quantiles: Tuple (lower, upper) para capping de outliers (Aplica en la fase de FE, aqu√≠ se omite)\n",
    "\n",
    "    Returns:\n",
    "        Tuple con:\n",
    "        - model: Modelo entrenado\n",
    "        - metrics: Dict con RMSE, R¬≤, MAE\n",
    "        - feature_names: Lista de features usadas\n",
    "        - X_test: DataFrame de features de test\n",
    "        - y_test: Series con valores reales de test\n",
    "        - y_pred: Array con predicciones\n",
    "        - model_path: Path al modelo guardado\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------------\n",
    "    # VALIDACI√ìN DEL TARGET\n",
    "    # -------------------------------------------------------------------------\n",
    "    if target not in CHEMICAL_TARGETS:\n",
    "        raise ValueError(\n",
    "            f\"Target '{target}' no v√°lido.\\n\"\n",
    "            f\"Debe ser uno de: {CHEMICAL_TARGETS}\"\n",
    "        )\n",
    "\n",
    "    # Usar hiperpar√°metros por defecto si no se especifican\n",
    "    n_estimators = n_estimators or DEFAULT_HYPERPARAMS['n_estimators']\n",
    "    max_depth = max_depth or DEFAULT_HYPERPARAMS['max_depth']\n",
    "    learning_rate = learning_rate or DEFAULT_HYPERPARAMS['learning_rate']\n",
    "    test_size = test_size or DEFAULT_HYPERPARAMS['test_size']\n",
    "    random_state = random_state or DEFAULT_HYPERPARAMS['random_state']\n",
    "\n",
    "    # Obtener nombre limpio del elemento\n",
    "    element_name = target.replace('target_', '').upper()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ENTRENAMIENTO DE MODELO QU√çMICO - {element_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Modelo: {MODEL_DISPLAY_NAMES.get(model_type, model_type)}\")\n",
    "\n",
    "    # Mostrar especificaci√≥n si existe\n",
    "    if target in CHEMICAL_SPECS:\n",
    "        min_spec, max_spec = CHEMICAL_SPECS[target]\n",
    "        print(f\"Especificaci√≥n: [{min_spec:.3f}, {max_spec:.3f}]\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 1: Cargar datos\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(\"Cargando datos qu√≠micos...\")\n",
    "    df = load_chemical_data()\n",
    "\n",
    "    # Verificar que el target existe\n",
    "    if target not in df.columns:\n",
    "        raise KeyError(\n",
    "            f\"La columna target '{target}' no existe en el dataset.\\n\"\n",
    "            f\"Columnas disponibles: {[c for c in df.columns if 'target' in c]}\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nDataset cargado: {df.shape}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 2: Selecci√≥n inteligente de features\n",
    "    # -------------------------------------------------------------------------\n",
    "    if feature_list is None:\n",
    "        feature_cols = get_chemical_features(df, target)\n",
    "    else:\n",
    "        # Aplicar exclusiones a lista personalizada\n",
    "        initial_feature = target.replace('target_', '')\n",
    "        feature_cols = [\n",
    "            f for f in feature_list\n",
    "            if f in df.columns and f != target and f != initial_feature\n",
    "        ]\n",
    "\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target].copy()\n",
    "\n",
    "    print(f\"Features seleccionadas: {len(feature_cols)}\")\n",
    "    print(f\"  (Excluido '{target.replace('target_', '')}' para evitar data leakage)\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 3: Eliminar filas donde el target es NaN\n",
    "    # -------------------------------------------------------------------------\n",
    "    rows_initial = len(X)\n",
    "    mask_not_null = y.notnull()\n",
    "    X = X[mask_not_null]\n",
    "    y = y[mask_not_null]\n",
    "    rows_after_null = len(X)\n",
    "\n",
    "    if rows_after_null < rows_initial:\n",
    "        print(f\"Eliminadas {rows_initial - rows_after_null} filas con target NaN\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 4: CAPPING DE OUTLIERS (CR√çTICO)\n",
    "    # ELIMINADO: La l√≥gica de filtrado de outliers se movi√≥ a la funci√≥n add_target_chemical\n",
    "    # en la PARTE 3. El dataset cargado ya debe estar limpio.\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 5: Imputaci√≥n de NaNs en features\n",
    "    # -------------------------------------------------------------------------\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # Limpiar valores infinitos\n",
    "    X = X.replace([np.inf, -np.inf], 0)\n",
    "    y = y.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    X = X.loc[y.index]  # Sincronizar √≠ndices\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        raise ValueError(f\"El dataset para '{target}' qued√≥ vac√≠o despu√©s de la limpieza.\")\n",
    "    \n",
    "    print(f\"\\nDataset final: {len(X)} muestras, {len(feature_cols)} features\")\n",
    "    print(f\"Estad√≠sticas del target: min={y.min():.4f}, max={y.max():.4f}, mean={y.mean():.4f}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 6: Split Train/Test\n",
    "    # -------------------------------------------------------------------------\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSplit Train/Test:\")\n",
    "    print(f\"  - Train: {len(X_train)} muestras\")\n",
    "    print(f\"  - Test: {len(X_test)} muestras\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 7: Crear y entrenar modelo\n",
    "    # -------------------------------------------------------------------------\n",
    "    logger.info(f\"Entrenando modelo: {MODEL_DISPLAY_NAMES.get(model_type, model_type)}\")\n",
    "    \n",
    "    if model_type == 'linear':\n",
    "        model = LinearRegression()\n",
    "    elif model_type == 'random_forest':\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    elif model_type == 'xgboost':\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo no reconocido: {model_type}\")\n",
    "    \n",
    "    print(f\"\\nEntrenando {MODEL_DISPLAY_NAMES.get(model_type, model_type)}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"‚úÖ Entrenamiento completado\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 8: Predecir y evaluar\n",
    "    # -------------------------------------------------------------------------\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics = calculate_metrics(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"M√âTRICAS DE EVALUACI√ìN - {element_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  RMSE: {metrics['RMSE']:.6f}\")\n",
    "    print(f\"  R¬≤:   {metrics['R2']:.4f}\")\n",
    "    print(f\"  MAE:  {metrics['MAE']:.6f}\")\n",
    "    \n",
    "    # Verificaci√≥n de calidad de R¬≤\n",
    "    if metrics['R2'] < 0:\n",
    "        print(f\"  ‚ö†Ô∏è  ADVERTENCIA: R¬≤ negativo. Modelo peor que la media.\")\n",
    "    elif metrics['R2'] < 0.3:\n",
    "        print(f\"  ‚ö†Ô∏è  R¬≤ bajo. Considera ajustar hiperpar√°metros.\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # PASO 9: Guardar modelo\n",
    "    # -------------------------------------------------------------------------\n",
    "    model_path = None\n",
    "    if save_model:\n",
    "        import pickle\n",
    "        from datetime import datetime\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        element_short = target.replace('target_', '')\n",
    "        model_name = f\"chem_{element_short}_{model_type}_{timestamp}\"\n",
    "        \n",
    "        # Crear subdirectorio\n",
    "        model_subdir = MODELS_DIR / model_name\n",
    "        model_subdir.mkdir(exist_ok=True)\n",
    "        \n",
    "        model_path = model_subdir / \"model.joblib\"\n",
    "        metadata_path = model_subdir / \"metadata.json\"\n",
    "        \n",
    "        # Guardar modelo\n",
    "        joblib.dump(model, model_path)\n",
    "        \n",
    "        # Guardar metadatos\n",
    "        metadata = {\n",
    "            \"model_type\": model_type,\n",
    "            \"model_display_name\": MODEL_DISPLAY_NAMES.get(model_type, model_type),\n",
    "            \"features\": feature_cols,\n",
    "            \"hyperparameters\": {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"max_depth\": max_depth,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"test_size\": test_size,\n",
    "                \"random_state\": random_state\n",
    "            },\n",
    "            \"metrics\": metrics,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"target\": target,\n",
    "            \"element\": element_short,\n",
    "            \"n_samples_train\": len(X_train),\n",
    "            \"n_samples_test\": len(X_test),\n",
    "            \"outlier_quantiles\": list(outlier_quantiles)\n",
    "        }\n",
    "        \n",
    "        if target in CHEMICAL_SPECS:\n",
    "            metadata[\"specification\"] = {\n",
    "                \"min\": CHEMICAL_SPECS[target][0],\n",
    "                \"max\": CHEMICAL_SPECS[target][1]\n",
    "            }\n",
    "        \n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        \n",
    "        # Guardar tambi√©n en chemical_results para compatibilidad con dashboard\n",
    "        chemical_results_dir = CHEMICAL_RESULTS_DIR\n",
    "        chemical_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        importance_df = get_feature_importance(model, feature_cols, model_type)\n",
    "        if importance_df is not None:\n",
    "            results_data = {\n",
    "                'y_test': y_test,\n",
    "                'y_pred': y_pred,\n",
    "                'importance_df': importance_df,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            results_file = chemical_results_dir / f\"results_{element_short}.pkl\"\n",
    "            with open(results_file, 'wb') as f:\n",
    "                pickle.dump(results_data, f)\n",
    "        \n",
    "        print(f\"\\nüíæ Modelo guardado en: {model_subdir}\")\n",
    "    \n",
    "    return model, metrics, feature_cols, X_test, y_test, y_pred, model_path\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funci√≥n principal definida: train_chemical_model()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualizaci√≥n Espec√≠fica para Qu√≠mica\n",
    "\n",
    "Funciones de visualizaci√≥n que incluyen las especificaciones qu√≠micas de calidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZACI√ìN ESPEC√çFICA PARA QU√çMICA\n",
    "# =============================================================================\n",
    "\n",
    "def plot_chemical_predictions(\n",
    "    y_test: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    metrics: Dict[str, float],\n",
    "    target: str\n",
    ") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Genera scatter plot con especificaciones qu√≠micas marcadas.\n",
    "    \n",
    "    Args:\n",
    "        y_test: Valores reales\n",
    "        y_pred: Valores predichos\n",
    "        metrics: Diccionario con m√©tricas\n",
    "        target: Nombre del target (ej: 'target_valc')\n",
    "    \n",
    "    Returns:\n",
    "        Figure de matplotlib\n",
    "    \"\"\"\n",
    "    element_name = target.replace('target_', '').upper()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(y_test, y_pred, alpha=0.5, color='steelblue', edgecolors='white', linewidth=0.5)\n",
    "    \n",
    "    # L√≠nea de predicci√≥n perfecta\n",
    "    min_val = min(np.min(y_test), np.min(y_pred))\n",
    "    max_val = max(np.max(y_test), np.max(y_pred))\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Predicci√≥n Perfecta')\n",
    "    \n",
    "    # Marcar especificaciones si existen\n",
    "    if target in CHEMICAL_SPECS:\n",
    "        min_spec, max_spec = CHEMICAL_SPECS[target]\n",
    "        ax.axhline(y=min_spec, color='green', linestyle=':', alpha=0.7, label=f'Spec Min: {min_spec}')\n",
    "        ax.axhline(y=max_spec, color='green', linestyle=':', alpha=0.7, label=f'Spec Max: {max_spec}')\n",
    "        ax.axvline(x=min_spec, color='green', linestyle=':', alpha=0.7)\n",
    "        ax.axvline(x=max_spec, color='green', linestyle=':', alpha=0.7)\n",
    "        \n",
    "        # Zona de especificaci√≥n\n",
    "        ax.fill_between([min_spec, max_spec], min_spec, max_spec, \n",
    "                        color='green', alpha=0.1, label='Zona √ìptima')\n",
    "    \n",
    "    ax.set_xlabel(f'Valor Real - {element_name} (%)', fontsize=12)\n",
    "    ax.set_ylabel(f'Valor Predicho - {element_name} (%)', fontsize=12)\n",
    "    ax.set_title(f'Predicci√≥n vs Real - {element_name}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # M√©tricas en recuadro\n",
    "    textstr = f\"RMSE: {metrics['RMSE']:.6f}\\nR¬≤: {metrics['R2']:.4f}\\nMAE: {metrics['MAE']:.6f}\"\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    \n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_chemical_summary(results_dict: Dict[str, Dict]) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Genera un gr√°fico resumen de todos los modelos qu√≠micos entrenados.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Diccionario con resultados por target\n",
    "                     {target: {'metrics': {...}, 'model': ...}, ...}\n",
    "    \n",
    "    Returns:\n",
    "        Figure de matplotlib\n",
    "    \"\"\"\n",
    "    targets = list(results_dict.keys())\n",
    "    r2_values = [results_dict[t]['metrics']['R2'] for t in targets]\n",
    "    rmse_values = [results_dict[t]['metrics']['RMSE'] for t in targets]\n",
    "    \n",
    "    # Limpiar nombres\n",
    "    labels = [t.replace('target_', '').upper() for t in targets]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Subplot 1: R¬≤ por elemento\n",
    "    ax1 = axes[0]\n",
    "    colors = ['green' if r2 > 0.5 else 'orange' if r2 > 0 else 'red' for r2 in r2_values]\n",
    "    bars1 = ax1.bar(labels, r2_values, color=colors, edgecolor='white')\n",
    "    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    ax1.axhline(y=0.5, color='green', linestyle='--', alpha=0.5, label='Umbral bueno (0.5)')\n",
    "    ax1.set_ylabel('R¬≤', fontsize=12)\n",
    "    ax1.set_title('Coeficiente de Determinaci√≥n (R¬≤) por Elemento', fontsize=12)\n",
    "    ax1.set_ylim(-0.5, 1.0)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # A√±adir valores en barras\n",
    "    for bar, val in zip(bars1, r2_values):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{val:.3f}', ha='center', fontsize=9)\n",
    "    \n",
    "    # Subplot 2: RMSE por elemento\n",
    "    ax2 = axes[1]\n",
    "    bars2 = ax2.bar(labels, rmse_values, color='steelblue', edgecolor='white')\n",
    "    ax2.set_ylabel('RMSE', fontsize=12)\n",
    "    ax2.set_title('Error Cuadr√°tico Medio (RMSE) por Elemento', fontsize=12)\n",
    "    \n",
    "    # A√±adir valores en barras\n",
    "    for bar, val in zip(bars2, rmse_values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "                f'{val:.4f}', ha='center', fontsize=9, rotation=45)\n",
    "    \n",
    "    plt.suptitle('Resumen de Modelos Qu√≠micos', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"‚úÖ Funciones de visualizaci√≥n qu√≠mica definidas:\")\n",
    "print(\"  - plot_chemical_predictions(): Scatter con especificaciones\")\n",
    "print(\"  - plot_chemical_summary(): Resumen de todos los modelos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Ejecuci√≥n: Entrenamiento de Modelos Qu√≠micos\n",
    "\n",
    "Entrenamos modelos para los principales elementos qu√≠micos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENTRENAMIENTO DE MODELOS QU√çMICOS\n",
    "# =============================================================================\n",
    "\n",
    "# Seleccionar elementos a entrenar (principales 5 elementos)\n",
    "# Puedes cambiar esta lista para incluir m√°s o menos elementos\n",
    "TARGETS_TO_TRAIN = [\n",
    "    'target_valc',    # Carbono\n",
    "    'target_valmn',   # Manganeso\n",
    "    'target_valsi',   # Silicio\n",
    "    'target_valp',    # F√≥sforo\n",
    "    'target_vals',    # Azufre\n",
    "]\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "chemical_results = {}\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(\"ENTRENAMIENTO DE MODELOS QU√çMICOS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Elementos a entrenar: {len(TARGETS_TO_TRAIN)}\")\n",
    "print(f\"Targets: {[t.replace('target_', '').upper() for t in TARGETS_TO_TRAIN]}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Entrenar modelo para cada elemento\n",
    "for target in TARGETS_TO_TRAIN:\n",
    "    try:\n",
    "        model, metrics, features, X_test, y_test, y_pred, path = train_chemical_model(\n",
    "            target=target,\n",
    "            model_type='xgboost',\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            save_model=True,\n",
    "            outlier_quantiles=(0.01, 0.99)  # Eliminar 1% extremos\n",
    "        )\n",
    "        \n",
    "        # Guardar resultados\n",
    "        chemical_results[target] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'features': features,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'path': path\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error entrenando {target}: {e}\\n\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESUMEN: {len(chemical_results)}/{len(TARGETS_TO_TRAIN)} modelos entrenados exitosamente\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZACI√ìN DE RESULTADOS QU√çMICOS\n",
    "# =============================================================================\n",
    "\n",
    "# Mostrar gr√°fico resumen de todos los modelos\n",
    "if chemical_results:\n",
    "    fig_summary = plot_chemical_summary(chemical_results)\n",
    "    plt.show()\n",
    "    \n",
    "    # Tabla resumen de m√©tricas\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"TABLA RESUMEN DE M√âTRICAS POR ELEMENTO\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"{'Elemento':<12} {'RMSE':>12} {'R¬≤':>12} {'MAE':>12} {'Spec Min':>10} {'Spec Max':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for target, result in chemical_results.items():\n",
    "        element = target.replace('target_', '').upper()\n",
    "        metrics = result['metrics']\n",
    "        \n",
    "        if target in CHEMICAL_SPECS:\n",
    "            min_spec, max_spec = CHEMICAL_SPECS[target]\n",
    "            print(f\"{element:<12} {metrics['RMSE']:>12.6f} {metrics['R2']:>12.4f} {metrics['MAE']:>12.6f} {min_spec:>10.3f} {max_spec:>10.3f}\")\n",
    "        else:\n",
    "            print(f\"{element:<12} {metrics['RMSE']:>12.6f} {metrics['R2']:>12.4f} {metrics['MAE']:>12.6f} {'N/A':>10} {'N/A':>10}\")\n",
    "    \n",
    "    print(f\"{'='*70}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay resultados para mostrar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GR√ÅFICOS INDIVIDUALES POR ELEMENTO\n",
    "# =============================================================================\n",
    "\n",
    "# Mostrar gr√°fico de predicci√≥n para cada elemento entrenado\n",
    "for target, result in chemical_results.items():\n",
    "    fig = plot_chemical_predictions(\n",
    "        result['y_test'],\n",
    "        result['y_pred'],\n",
    "        result['metrics'],\n",
    "        target\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar feature importance para este modelo\n",
    "    fig_imp = plot_feature_importance(\n",
    "        result['model'],\n",
    "        result['features'],\n",
    "        model_type='xgboost',\n",
    "        top_n=10,\n",
    "        title=f\"Importancia de Variables - {target.replace('target_', '').upper()}\"\n",
    "    )\n",
    "    if fig_imp:\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
